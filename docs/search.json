[
  {
    "objectID": "module136_ReproducibleResearch.html",
    "href": "module136_ReproducibleResearch.html",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "",
    "text": "Discuss reproducible research principles.\nApply reproducible research principles to data analysis using R Markdown.\n\nKey points to cover: 1. Reproducible research principles 2. What is R Markdown 3. How to create a report using R Markdown a. Customize the layout of presentations or reports b. Insert and create objects, such as tables, images, or videos, within a document",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  },
  {
    "objectID": "module136_ReproducibleResearch.html#session-objectives",
    "href": "module136_ReproducibleResearch.html#session-objectives",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "",
    "text": "Discuss reproducible research principles.\nApply reproducible research principles to data analysis using R Markdown.\n\nKey points to cover: 1. Reproducible research principles 2. What is R Markdown 3. How to create a report using R Markdown a. Customize the layout of presentations or reports b. Insert and create objects, such as tables, images, or videos, within a document",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  },
  {
    "objectID": "module136_ReproducibleResearch.html#prework---before-you-begin",
    "href": "module136_ReproducibleResearch.html#prework---before-you-begin",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  },
  {
    "objectID": "module136_ReproducibleResearch.html#discuss-reproducible-research-principles.",
    "href": "module136_ReproducibleResearch.html#discuss-reproducible-research-principles.",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "1. Discuss reproducible research principles.",
    "text": "1. Discuss reproducible research principles.",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  },
  {
    "objectID": "module136_ReproducibleResearch.html#apply-reproducible-research-principles-to-data-analysis-using-r-markdown.",
    "href": "module136_ReproducibleResearch.html#apply-reproducible-research-principles-to-data-analysis-using-r-markdown.",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "2. Apply reproducible research principles to data analysis using R Markdown.",
    "text": "2. Apply reproducible research principles to data analysis using R Markdown.",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  },
  {
    "objectID": "module136_ReproducibleResearch.html#references",
    "href": "module136_ReproducibleResearch.html#references",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "References",
    "text": "References\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  },
  {
    "objectID": "module136_ReproducibleResearch.html#other-helpful-resources",
    "href": "module136_ReproducibleResearch.html#other-helpful-resources",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html",
    "href": "module134_MissingWeight.html",
    "title": "1.3.4: Missing Data and Sampling Weights",
    "section": "",
    "text": "Identify and summarize missing data.\nLearn methods to handle missing data according to variable type.\nUse a survey sampling weight to generate more representative descriptive and inferential statistical values (brief intro)\nDiscuss potential bias when removing missing observations without careful examination.\n\nkey points 1. R packages that support missing data examination 2. Mean/median imputation for continuous variables 3. What to do with missing observations for categorical variables 4. Ways to examine potential differences between complete and missing observations in association between certain independent and dependent variables a. What to do if such association significantly differs between complete and missing observations 5. R packages for complex survey data (e.g., survey package) a. R codes to generate weighted descriptive statistics and contingency tables, as well as to develop weighted linear models",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#session-objectives",
    "href": "module134_MissingWeight.html#session-objectives",
    "title": "1.3.4: Missing Data and Sampling Weights",
    "section": "",
    "text": "Identify and summarize missing data.\nLearn methods to handle missing data according to variable type.\nUse a survey sampling weight to generate more representative descriptive and inferential statistical values (brief intro)\nDiscuss potential bias when removing missing observations without careful examination.\n\nkey points 1. R packages that support missing data examination 2. Mean/median imputation for continuous variables 3. What to do with missing observations for categorical variables 4. Ways to examine potential differences between complete and missing observations in association between certain independent and dependent variables a. What to do if such association significantly differs between complete and missing observations 5. R packages for complex survey data (e.g., survey package) a. R codes to generate weighted descriptive statistics and contingency tables, as well as to develop weighted linear models",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#prework---before-you-begin",
    "href": "module134_MissingWeight.html#prework---before-you-begin",
    "title": "1.3.4: Missing Data and Sampling Weights",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#identify-and-summarize-missing-data.",
    "href": "module134_MissingWeight.html#identify-and-summarize-missing-data.",
    "title": "1.3.4: Missing Data and Sampling Weights",
    "section": "1. Identify and summarize missing data.",
    "text": "1. Identify and summarize missing data.",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#learn-methods-to-handle-missing-data-according-to-variable-type.",
    "href": "module134_MissingWeight.html#learn-methods-to-handle-missing-data-according-to-variable-type.",
    "title": "1.3.4: Missing Data and Sampling Weights",
    "section": "2. Learn methods to handle missing data according to variable type.",
    "text": "2. Learn methods to handle missing data according to variable type.",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#use-a-survey-sampling-weight-to-generate-more-representative-descriptive-and-inferential-statistical-values-brief-intro",
    "href": "module134_MissingWeight.html#use-a-survey-sampling-weight-to-generate-more-representative-descriptive-and-inferential-statistical-values-brief-intro",
    "title": "1.3.4: Missing Data and Sampling Weights",
    "section": "3. Use a survey sampling weight to generate more representative descriptive and inferential statistical values (brief intro)",
    "text": "3. Use a survey sampling weight to generate more representative descriptive and inferential statistical values (brief intro)",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#discuss-potential-bias-when-removing-missing-observations-without-careful-examination.",
    "href": "module134_MissingWeight.html#discuss-potential-bias-when-removing-missing-observations-without-careful-examination.",
    "title": "1.3.4: Missing Data and Sampling Weights",
    "section": "4. Discuss potential bias when removing missing observations without careful examination.",
    "text": "4. Discuss potential bias when removing missing observations without careful examination.",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#references",
    "href": "module134_MissingWeight.html#references",
    "title": "1.3.4: Missing Data and Sampling Weights",
    "section": "References",
    "text": "References\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#other-helpful-resources",
    "href": "module134_MissingWeight.html#other-helpful-resources",
    "title": "1.3.4: Missing Data and Sampling Weights",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html",
    "href": "module132_DataWrangling.html",
    "title": "1.3.2: Data Wrangling",
    "section": "",
    "text": "To read in data.\nTo view the Data.\nTo subset the data - select and filter.\nTo create and modify variables.\nTo get data summary and descriptive statistics",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#session-objectives",
    "href": "module132_DataWrangling.html#session-objectives",
    "title": "1.3.2: Data Wrangling",
    "section": "",
    "text": "To read in data.\nTo view the Data.\nTo subset the data - select and filter.\nTo create and modify variables.\nTo get data summary and descriptive statistics",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#prework---before-you-begin",
    "href": "module132_DataWrangling.html#prework---before-you-begin",
    "title": "1.3.2: Data Wrangling",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\nInstall Packages\nBefore you begin, please go ahead and install the following packages - these are all on CRAN, so you can install them using the RStudio Menu Tools/Install Packages interface:\n\n\nreadr on CRAN\n\nreadr package website\n\n\n\nreadxl on CRAN\n\n`readxl`` package website\n\n\n\nhaven on CRAN\n\nhaven package website\n\n\n\ndplyr on CRAN\n\ndplyr package website\n\n\nHmisc package\npsych package\narsenal\ngtsummary\ngmodels\nsummarytools\n\nSee Module 1.3.1 on Installing Packages",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-read-in-data.",
    "href": "module132_DataWrangling.html#to-read-in-data.",
    "title": "1.3.2: Data Wrangling",
    "section": "1. To read in data.",
    "text": "1. To read in data.\nBegin with a NEW RStudio Project\nLet’s begin with a new RStudio Project.\n\nFirst click on the menu at top for File/New Project:\n\n\n\nNext choose either an “Existing Directory” or “New Directory” depending on whether you want to use a folder that already exists on your computer or you want to create a new folder.\n\n\n\nFor now, let’s choose a “New Directory” and then select “New Project”\n\n\n\nWhen the next window opens, as an example, I’m creating a new folder called myfirstRproject for my RStudio project under the parent directory, C:\\MyGithub.\n\n\n\nSo, if I look back on my computer in my file manager (I’m on a computer with Windows 11 operating system) - I can now see this new folder on my computer for C:\\MyGithub\\myfirstRproject.\n\n\n\nNow let’s put some data into this folder. Feel free to move datasets of your own into this new RStudio project directory. But here are some test datasets you can download and place into this new directory on your computer - choose at least one to try out - right click on each link and use “Save As” to save the file on your computer.\n\n\nmydata.csv - CSV (comma separated value) formatted data\nmydata.xlsx - EXCEL file\nmydata.sav - SPSS Dataset\nmydata.sas7bdat - SAS Dataset\nMydata_Codebook.pdf - Codebook Details on “mydata” dataset\n\n\nAfter putting these files into your new RStudio project folder, you should see something like this now in your RStudio Files Listing (bottom right window pane):\n\n\nImporting Data\nNow that you’ve got some data in your RStudio project folder, let’s look at options for importing these datasets into your RStudio computing session.\nClick on File/Import Dataset - and then choose the file format you want.\nImport a CSV file\n\n\n\n\n\n\nWhat is a CSV file?\n\n\n\nCSV stands for “comma separated value” format. This format is what you would think - each value for a different column (or variable) is separated by a column and each new row represents a new record in the dataset.\nCSV is widely accepted as a “universal” standard as a data format for easy exchange between different software and databases.\n\nWikipedia Page on CSV\nLibrary of Congress Page on CSV\nThere is even a conference on CSV\n\n\n\n\nHere is an example of importing the mydata.csv - CSV formatted data. Let’s use the From Text (readr) option.\n\nWhy should we use the “from text” option? Why do I not see a CSV option?\nTechnically the CSV format is TEXT. You can open a CSV file in a text editor and easily read it - even if you do not have proprietary software like Excel, Access, SPSS, SAS, etc. Here is a screen shot of what the “mydata.csv” file looks like in my text editor “Notepad” on my Windows 11 computer:\nNotice that:\n\nThe first row has text labels for the “variables” (columns) in the dataset - there are 14 column labels with each value separated by a , comma.\nThe remaining rows are the “data” for the dataset.\nAFTER the 1st row of labels, there are 21 rows of data.\nTake a minute and notice there are some odd values, and odd patterns of missing data (two commas ,, together indicate that value is missing for that column (variable)). We’ll explore these issues further below.\n\n\n\nOnce the “File/Import Data/From Text (readr)” opens, click on “Browse” and choose the mydata.csv file. Assuming all goes well, this window will read the top of the datafile and show you a quick “Data Preview” to check that the import will work.\nAnd on the bottom right, the “Code Preview” shows you the R code commands needed to import this dataset. You can then click on the little “clipboard” on the bottom right to copy this R code to your “clipboard”, (the R code option will be explained below).\nOR You can also just click “Import” and the R code will be executed for you and the dataset brought into your R computing session (but this is NOT a good practice for reproducible research!).\n\nBut the better way is to save the R code commands to import the data so you will be able to reproduce all steps in your data analysis workflow using code as opposed to non-reproducible point-and-click steps.\nOnce you copied the R code above to your clipboard, go to “File/New File/R Script” to open a script programming window:\n\nAnd then “paste” your R code into this window.\nSo as you can see importing the mydata.csv dataset, involves 2 steps:\n\nLoading the readr package into your RStudio computing session, by running library(readr)\n\nRunning the read_csv() function from the readr package and then assigning &lt;- this output into a new R data object called mydata.\n\n\nTo import the dataset, select these 2 lines of code and then click “Run” to run the R code. And be sure to click “Save” to save your first R program - for example “importdata.R”.\n\nAfter running these 2 lines of code, you should see something like this - the code messages in the bottom left “Console” window pane and a new R data object “mydata” in the top right “Global Environment” window pane.\n\nImport an EXCEL file\nLet’s try another format. While you will probably encounter CSV (comma separated value) data files often (since nearly all data collection platforms, databases and software will be able to export this simple non-proprietary format), many people natively open/read CSV files in the EXCEL software. So you will probably also encounter EXCEL (*.XLS or *.XLSX) formatted data files.\nIn addition to an EXCEL file using a Microsoft proprietary format, EXCEL files can have formatting (font sizes, colors, borders) and can have multiple TABs (or SHEETs). Here are some screen shots of the mydata.xlsx - EXCEL file file.\nThe first “Data” TAB:\n\nThe second “Codebook” TAB:\n\nTo import an EXCEL file into R, we will use the same process as above, but this time we will select “File/Import Dataset/From Excel”:\n\nThis process uses the read_excel() function from the readxl package.\nWith the read_excel() function, we can specify several options including:\n\nWhich TAB do you want to import (for now we are only importing one data TAB at a time). We are selecting the “Data” TAB.\nI’m leaving all of the rest as their defaults which include:\n\nnot changing the “Range”,\nleaving “Max Rows” blank,\nand leaving rows to “Skip” as 0, which can be useful if you receive files with a lot of “header” information at the top,,\nleaving the “NA” box blank - but you could put in a value like “99” if oyu want all 99’s treated as missing - but this is applied to the ENTIRE dataset. We will look at these issues for individual variables below.\n\n\nAlso notice that the checkboxes are selected for “First Row as Names” (which is the usual convention) and “Open Data Viewer”, which creates the View(mydata) in the “Code Preview” window to the right. You can skip this if you like.\n\nSo in the “Code Preview” window to the right, we have specified the name of the data file \"mydata.xlsx\" and the “Data” TAB using the option sheet = \"Data\". Remember to copy this code to the clipboard and save it in a *.R program script.\n\nHere is the importdata.R program script we have so far for reading in the \"mydata.csv\" and \"mydata.xlsx\" data files. At the moment, the second time we “create” the mydata R data object we are overwriting the previous one.\nAlso notice I have added some comments which start with a # hashtag. Any text following a # will be ignored by R and not executed.\n\nImport SPSS data\nFor data files from other “common” statistics software like SPSS, SAS and Stata, we can use the “File/Import Dataset/From SPSS (or From SAS or From Stata)”. All of these use read_xxx() functions from the haven package.\n\nHere is the code generated to import a SPSS datafile:\n\nImport SAS data\nImporting a *.sas7bdat SAS datafile, is similar to SPSS - here is that code.\nNotice that in addition to the datafile \"mydata.sas7bdat\", the read_sas() function also shows NULL. When reading in a SAS file, you can also add options for the catalog file and encoding specifics. You can read more on the HELP pages for the haven::read_sas() function.\n\n\nHere is a quick summary of all of the data import codes shown above importdata.R:\n\n\n\n\n\n\nUsing = equals for parameter options inside a function\n\n\n\nNotice that we used sheet = \"Data\" inside the readxl::read_excel() function. The single = equals sign is used to assign a value to a parameter or option inside a function.\n\n\n\n# Import the CSV file\nlibrary(readr)\nmydata &lt;- read_csv(\"mydata.csv\")\n\n# Import the EXCEL file\n# Choose the \"Data\" TAB\nlibrary(readxl)\nmydata &lt;- read_excel(\"mydata.xlsx\", sheet = \"Data\")\n\n# Import a SPSS file\nlibrary(haven)\nmydata &lt;- read_sav(\"mydata.sav\")\n\n# Import a SAS file\nlibrary(haven)\nmydata &lt;- read_sas(\"mydata.sas7bdat\", NULL)\n\nExploring Built-in Datasets\nIf you are looking for other datasets to test out functions or just need some data to play around with, the base R packages and other R packages (like palmerpenguins) have data built-in to them. You can use these datasets.\nWe can take a look at what datasets are available using the data() function:\n\n# take a look at the datasets available in the\n# \"datasets\" base R package\ndata()\n\nThis will open a viewer window (top left) - also notive that if you search for HELP on the pressure dataset, you get a description of the dataset and the original source and citation. Notice in the HELP window, the word pressure is followed by curly brackets indicating that the pressure dataset is in the built-in R package {datasets}.\n\nWe can see the pressure dataset is indeed in the datasets package if we keep scrolling down in the viewer window - also notice the mtcars dataset which you will often find in R tutorials and coding examples.\n\nOnce you know where to look, you can then explore lots of these datasets. For example, we can take a look at the built-in pressure dataset, which includes 19 values showing the relationship between tempeature in degrees Celcius and pressure in mm (or mercury). To “see” this builtin data object, just type the name pressure to see (or print out) the object.\n\npressure\n\n   temperature pressure\n1            0   0.0002\n2           20   0.0012\n3           40   0.0060\n4           60   0.0300\n5           80   0.0900\n6          100   0.2700\n7          120   0.7500\n8          140   1.8500\n9          160   4.2000\n10         180   8.8000\n11         200  17.3000\n12         220  32.1000\n13         240  57.0000\n14         260  96.0000\n15         280 157.0000\n16         300 247.0000\n17         320 376.0000\n18         340 558.0000\n19         360 806.0000\n\n\nNormally most datasets are much larger than this little dataset - I would not advise trying to view most datasets by printing them to the “Console” window pane. Instead you can either click on the object in your “Global Environment” to view it - or you can run the View() function to open the viewer window.\nYou can “load” the built-in pressure dataset using the data(pressure) function to load the pressure dataset to load into your “Global Environment”, which loads the dataset into your R session.\nIf we click on the little “Table icon” all the way to the right of the pressure dataset in the “Global Environment” window - or run View(pressure) - we can open the dataset in the Viewer window:\n\ndata(pressure)\nView(pressure)\n\n\n\n\n\n\n\n\nExplore Datasets in R Packages\n\n\n\nI encourage you to use the data(package = \"xxx\") function to see what, if any, datasets may be built-in to the various packages you may install and load during your R computing sessions.\n\n\nIf you are interested in seeing other datasets in other R packages, go ahead and install the palmerpenguins package and take a look at the penguins dataset included:\n\n# look at datasets included with the\n# palmerpenguins dataset\ndata(package = \"palmerpenguins\")\n\nYou can learn more about the penguins dataset, by opening up the HELP page for the dataset. You can also load the palmerpenguins package and then load the penguins dataset using this code.\n\nhelp(penguins, package = \"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(penguins)\n\n\nAnd clicking the the little data table icon after loading the penguins dataset into the “Global Environment”, you can see the dataset in the viewer window.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-view-the-data.",
    "href": "module132_DataWrangling.html#to-view-the-data.",
    "title": "1.3.2: Data Wrangling",
    "section": "2. To view The Data.",
    "text": "2. To view The Data.\nLook at small data in Console\nLet’s work with the mydata dataset that we imported above using the readr::read_csv() function.\n\n# import the mydata.csv dataset\nmydata &lt;- readr::read_csv(\"mydata.csv\")\n\nThis is not a very large dataset - mydata has 21 rows (or observations) and 14 variables (or columns). So, we could view the whole thing by printing it to the “Console” window.\nYou’ll notice that depending on the size of your current “Console” window, font size, zoom settings and more, what you see may vary. Since we read this dataset in using the readr package, the data object is now a “tibble” dataframe which only shows the columns and rows that will reasonably show up in your “Console” window.\nAnd the output below also lists what kind of column each variable is. For example,\n\n\nAge is a &lt;dbl&gt; indicating it is a numeric variable saved using double-precision, whereas\n\nGenderSTR is &lt;chr&gt; indicating this is a text or character (or “string”) type variable.\n\n\n# print the dataset into the Console\nmydata\n\n# A tibble: 21 × 14\n   SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n 1         1    45        68        145    5.6     9 m                   1     4\n 2         2    50       167        166    5.4     2 f                   2     3\n 3         3    35       143        135    5.6     2 &lt;NA&gt;               NA     3\n 4         4    44       216        201    5.6     2 m                   1     4\n 5         5    32       243        223    6       2 m                   1     5\n 6         6    48       165        145    5.2     2 f                   2     2\n 7         8    50        60        132    3.3     2 m                   1     3\n 8         9    51       110        108    5.1     3 f                   2     1\n 9        12    46       167        158    5.5     2 F                   2     1\n10        14    35       190        200    5.8     1 Male                1     4\n# ℹ 11 more rows\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n\nLook the “structure” of the dataset\nYou can also view the different kinds of variables in the dataset using the str() or “structure” function - which lists the type of variable, the number of elements in each column [1:21] indicates each column has 21 elements (or 21 rows) and the other values are a quick “peek” at the data inside the dataset. For example, the first 3 people in this dataset are ages 45, 50 and 35.\n\nstr(mydata)\n\nspc_tbl_ [21 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ SubjectID  : num [1:21] 1 2 3 4 5 6 8 9 12 14 ...\n $ Age        : num [1:21] 45 50 35 44 32 48 50 51 46 35 ...\n $ WeightPRE  : num [1:21] 68 167 143 216 243 165 60 110 167 190 ...\n $ WeightPOST : num [1:21] 145 166 135 201 223 145 132 108 158 200 ...\n $ Height     : num [1:21] 5.6 5.4 5.6 5.6 6 5.2 3.3 5.1 5.5 5.8 ...\n $ SES        : num [1:21] 9 2 2 2 2 2 2 3 2 1 ...\n $ GenderSTR  : chr [1:21] \"m\" \"f\" NA \"m\" ...\n $ GenderCoded: num [1:21] 1 2 NA 1 1 2 1 2 2 1 ...\n $ q1         : num [1:21] 4 3 3 4 5 2 3 1 1 4 ...\n $ q2         : num [1:21] NA 4 4 2 3 5 NA 4 1 44 ...\n $ q3         : num [1:21] NA 1 2 2 5 5 4 1 5 1 ...\n $ q4         : num [1:21] 4 40 3 1 2 1 3 3 5 1 ...\n $ q5         : num [1:21] 4 3 5 1 4 4 9 1 1 4 ...\n $ q6         : num [1:21] 5 2 2 9 1 5 2 4 2 5 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   SubjectID = col_double(),\n  ..   Age = col_double(),\n  ..   WeightPRE = col_double(),\n  ..   WeightPOST = col_double(),\n  ..   Height = col_double(),\n  ..   SES = col_double(),\n  ..   GenderSTR = col_character(),\n  ..   GenderCoded = col_double(),\n  ..   q1 = col_double(),\n  ..   q2 = col_double(),\n  ..   q3 = col_double(),\n  ..   q4 = col_double(),\n  ..   q5 = col_double(),\n  ..   q6 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nYou can also interactively View the data by clicking on the data icon and you can also click the little “table” icon to the far right next to the dataset in the “Global Environment”to open the data viewer window on the left.\nYou can also click on the little blue circle to the left of the mydata dataset to change the arrow from facing right  to facing down  to see the “structure” of the data in the “Global Environment”.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-subset-the-data---select-and-filter.",
    "href": "module132_DataWrangling.html#to-subset-the-data---select-and-filter.",
    "title": "1.3.2: Data Wrangling",
    "section": "3. To subset the data - select and filter.",
    "text": "3. To subset the data - select and filter.\nUsing base R packages and functions\nView parts of the dataset\nNow let’s “explore” the data by viewing sections of it.\nUsing base R commands, we can use functions like head() and tail() with eash showing either the top or bottom 6 rows of the dataset. We can add a number to the function call to see more or less rows if we wish.\n\n# look at top 6 rows of data\nhead(mydata)\n\n# A tibble: 6 × 14\n  SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1         1    45        68        145    5.6     9 m                   1     4\n2         2    50       167        166    5.4     2 f                   2     3\n3         3    35       143        135    5.6     2 &lt;NA&gt;               NA     3\n4         4    44       216        201    5.6     2 m                   1     4\n5         5    32       243        223    6       2 m                   1     5\n6         6    48       165        145    5.2     2 f                   2     2\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n# look at the bottom 10 rows of data\ntail(mydata, n=10)\n\n# A tibble: 10 × 14\n   SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n 1        19    40       200        195    6.1     1 f                   2     1\n 2        21    99       180        185    5.9     3 f                   2     2\n 3        22    52       240        220    6.5     2 m                   1     2\n 4        23    24       250        240    6.4     2 M                   1     5\n 5        24    35       175        174    5.8     2 F                   2     5\n 6        27    51       220        221    6.3     2 m                   1     4\n 7        28    43       230         98    2.6     2 m                   1    11\n 8        30    36       190        180    5.7     1 female              2     5\n 9        32    44       260        109    6.4     3 male                1     1\n10        NA    NA        NA         NA   NA      NA &lt;NA&gt;               NA    NA\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nWhat are these wierd NAs?\n\n\n\nThe NA letters that show up is how R stores missing data. If the dataset you import has a blank cell (for either numeric or character type data), then R interprets that as “not available” which is indicated by NA. NA is a reserved word in R specifically set aside for handling missing values.\nYou can learn more about NA by running:\n\nhelp(NA, package = \"base\")\n\n\n\nYou can also view different parts of the data by using square brackets [] to select specific rows and columns using [row, column] index indicators.\n\n# Select the values in rows 1-4\n# and in columns 1-3\nmydata[1:4, 1:3]\n\n# A tibble: 4 × 3\n  SubjectID   Age WeightPRE\n      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1         1    45        68\n2         2    50       167\n3         3    35       143\n4         4    44       216\n\n\nTo select all of a given row or column just leave that index blank.\n\n# show all of rows 1-2\nmydata[1:2, ]\n\n# A tibble: 2 × 14\n  SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1         1    45        68        145    5.6     9 m                   1     4\n2         2    50       167        166    5.4     2 f                   2     3\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n# show all of columns 3-4\nmydata[ ,3:4]\n\n# A tibble: 21 × 2\n   WeightPRE WeightPOST\n       &lt;dbl&gt;      &lt;dbl&gt;\n 1        68        145\n 2       167        166\n 3       143        135\n 4       216        201\n 5       243        223\n 6       165        145\n 7        60        132\n 8       110        108\n 9       167        158\n10       190        200\n# ℹ 11 more rows\n\n\nView variables in dataset by name\nWe can also select columns from a dataset using the variable (or column) name. To see the names of all of the variables in a dataset, use the names() function.\n\n# list variable names in mydata\nnames(mydata)\n\n [1] \"SubjectID\"   \"Age\"         \"WeightPRE\"   \"WeightPOST\"  \"Height\"     \n [6] \"SES\"         \"GenderSTR\"   \"GenderCoded\" \"q1\"          \"q2\"         \n[11] \"q3\"          \"q4\"          \"q5\"          \"q6\"         \n\n\nWe can use the $ “dollar sign” operator to “select” named variables out of a dataset. Let’s look at all of the ages in mydata.\n\n# look at all of the ages\n# of the 21 people in mydata\nmydata$Age\n\n [1] 45 50 35 44 32 48 50 51 46 35 36 40 99 52 24 35 51 43 36 44 NA\n\n\nWe can also use these variable names with the [] brackets in base R syntax. And we use the c() combine function to help us put a list together. Let’s look at the 2 weight columns in the dataset. Put the variable names inside \"\" double quotes.\n\n# show all rows for\n# the 2 weight variables in mydata\nmydata[ , c(\"WeightPRE\", \"WeightPOST\")]\n\n# A tibble: 21 × 2\n   WeightPRE WeightPOST\n       &lt;dbl&gt;      &lt;dbl&gt;\n 1        68        145\n 2       167        166\n 3       143        135\n 4       216        201\n 5       243        223\n 6       165        145\n 7        60        132\n 8       110        108\n 9       167        158\n10       190        200\n# ℹ 11 more rows\n\n\nUsing dplyr functions\nUsing tidyverse packages and functions\nAs you can see while base R is very powerful on it’s own, the syntax is less than intuitive. There are a whole suite of R packages that are designed to work together and use a different syntax that improves programming workflow and readability.\nLearn more about the suite of tidyverse packages. You’ve already used two of these, readr and haven are both part of tidyverse for importing datasets.\nAnother one of these tidyverse packages, dplyr is a very good package for “data wrangling”.\nPick columns using dplyr::select()\nInstead of using the base R $ selector, the dplyr package has a select() function where you simply choose variables using their name. Let’s look at Height and q1 from the mydata dataset.\n\n\n\n\n\n\nUsing package::function() syntax\n\n\n\nIt is good coding practice, especially when loading several packages at once into your computing session, the make sure you are calling the exact function you want from a specific package. So, I’m using the syntax of package::function() to help keep track of which package and which function is being used below.\n\n\n\n# load dplyr package\nlibrary(dplyr)\n\n# select Height and q1 from mydata\ndplyr::select(mydata, c(Height, q1))\n\n# A tibble: 21 × 2\n   Height    q1\n    &lt;dbl&gt; &lt;dbl&gt;\n 1    5.6     4\n 2    5.4     3\n 3    5.6     3\n 4    5.6     4\n 5    6       5\n 6    5.2     2\n 7    3.3     3\n 8    5.1     1\n 9    5.5     1\n10    5.8     4\n# ℹ 11 more rows\n\n\nWorkflow using the pipe %&gt;% operator\nAnother improvement of the tidyverse approach of R programming is to use the pipe %&gt;% operator. Basically what this syntax does is take the results from “A” and pipe it into –&gt; the next “B” function, e.g. A %&gt;% B so we can begin to “daisy-chain” a sequence of programming steps together into a logical workflow and is easy to “read” and follow.\nHere is a working exmaple to show the same variable selection process using the dplyr::select() function. The code below takes the mydata dataset and pipes %&gt;% it into the select() function. We were also able to drop using the c() function here.\n\n# select Height and q1 from mydata\nmydata %&gt;% dplyr::select(Height, q1)\n\n# A tibble: 21 × 2\n   Height    q1\n    &lt;dbl&gt; &lt;dbl&gt;\n 1    5.6     4\n 2    5.4     3\n 3    5.6     3\n 4    5.6     4\n 5    6       5\n 6    5.2     2\n 7    3.3     3\n 8    5.1     1\n 9    5.5     1\n10    5.8     4\n# ℹ 11 more rows\n\n\nWe could even add the base R head() function here. If we put each code step on a separate line, you can now see that we are [1] taking the mydata dataset “and then” [2] selecting 2 variables “and then” [3] looking at the top 6 rows of the dataset.\n\n# select Height and q1 from mydata\n# and show only the top 6 rows\nmydata %&gt;% \n  dplyr::select(Height, q1) %&gt;%\n  head()\n\n# A tibble: 6 × 2\n  Height    q1\n   &lt;dbl&gt; &lt;dbl&gt;\n1    5.6     4\n2    5.4     3\n3    5.6     3\n4    5.6     4\n5    6       5\n6    5.2     2\n\n\n\n\n\n\n\n\nTL;DR If %&gt;% is a pipe, then what is |&gt;??\n\n\n\nThe %&gt;% pipe operator is implemented within tidyverse from the magrittr package which is used by the tidyverse packages which started being used quite extensively by R programmers over the last decade.\nHowever, the rest of the R development community which is much larger than just those who use the tidyverse suite, also recently added a new base R pipe operator |&gt; (since R version 4.1.0).\nLearn more in this tidyverse blog post from 2023\n\n\nSo, you do have the option to also use the base R |&gt; pipe operator.\n\n# select Height and q1 from mydata\n# and show only the top 6 rows\nmydata |&gt; \n  dplyr::select(Height, q1) |&gt;\n  head()\n\n# A tibble: 6 × 2\n  Height    q1\n   &lt;dbl&gt; &lt;dbl&gt;\n1    5.6     4\n2    5.4     3\n3    5.6     3\n4    5.6     4\n5    6       5\n6    5.2     2\n\n\nFor now, we will stay with the %&gt;% operator for consistency. But be aware that you will see both approaches on the Internet when “Googling” for answers.\nPick rows using dplyr::filter()\nIn addition to selecting columns or variables from your dataset, you can also pull out a subset of your data by “filtering” out only the rows you want.\nFor example, suppose we only want to look at the Age, WeightPRE for the Females in the dataset indicates by GenderCoded equal to 2.\nFor reference, take a look at the mydata codebook - and here is a screenshot as well:\n\nNotice that:\n\nI changed the order of the columns - this is OK\nand to filter out and KEEP only the rows for females, I typed GenderCoded == 2 using two equal signs ==. R uses two == equal signs to perform a logicial operation to ask does the variable GenderCoded equal the value of 2, with either a TRUE or FALSE result. Only the rows with a TRUE result are shown.\n\n\n\n\n\n\n\nBe careful not to mix up = and ==\n\n\n\nOdds are you will get errors at some point due to typos or other issues, but a common error is to use a single = equals sign when trying to perform a logic operation. Remember to use 2 equals signs == if you are trying to perform a TRUE/FALSE operation.\n\n\n\n# select columns from mydata\n# and then only show rows for females\nmydata %&gt;%\n  select(GenderCoded, Age, WeightPRE) %&gt;%\n  filter(GenderCoded == 2)\n\n# A tibble: 8 × 3\n  GenderCoded   Age WeightPRE\n        &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1           2    50       167\n2           2    48       165\n3           2    51       110\n4           2    46       167\n5           2    40       200\n6           2    99       180\n7           2    35       175\n8           2    36       190\n\n\nHere is an example of the error you will get if you use a single = sign instead of == two.\n\n# select columns from mydata\n# and then only show rows for females\nmydata %&gt;%\n  select(GenderCoded, Age, WeightPRE) %&gt;%\n  filter(GenderCoded = 2)\n\nError in `filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `GenderCoded == 2`?\n\n\nsort/Arrange rows using dplyr::arrange()\nHere is another helpful function from dplyr. Suppose we want to find the 5 oldest people in mydata and show their IDs.\nLet’s use the dplyr::arrange() function which will sort our data based on the variable we specify in increasing order (lowest to highest) by default. And we will add the desc() function to sort decreasing from largest to smallest.\nLearn more by running help(arrange, package = \"dplyr\")\nNote: There was someone with age 99 in this made-up dataset.\n\n# take mydata\n# select SubjectID and Age\n# sort descending by Age\n# show the top 5 IDs and Ages\nmydata %&gt;%\n  select(SubjectID, Age) %&gt;%\n  arrange(desc(Age)) %&gt;%\n  head(n=5)\n\n# A tibble: 5 × 2\n  SubjectID   Age\n      &lt;dbl&gt; &lt;dbl&gt;\n1        21    99\n2        22    52\n3         9    51\n4        27    51\n5         2    50",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-create-and-modify-variables.",
    "href": "module132_DataWrangling.html#to-create-and-modify-variables.",
    "title": "1.3.2: Data Wrangling",
    "section": "4. To create and modify variables.",
    "text": "4. To create and modify variables.\nTo create and add new variables to the dataset, we can use either a base R approach or the mutate() function from the dplyr package. Let’s take a look at both approaches. In the mydata dataset, we have Height in decimal feet and we have WeightPRE and WeightPOST in pounds.\nSo, let’s compute BMI (bodt mass index) as follows from Height (in inches) and Weight (in pounds):\n\\[BMI = \\left(\\frac{weight_{(lbs)}}{(height_{(inches)})^2}\\right) * 703\\]\nCreate New Variable - Base R Approach\nCreate a new variable using the $ selector operator. Then write out the mathematical equation. I also had to multiple the Height in decimal feet * 12 to get inches.\n\n# Compute BMI forthe PRE Weight\nmydata$bmiPRE &lt;- \n  (mydata$WeightPRE * 703) / (mydata$Height * 12)^2\n\n# look at result\nmydata$bmiPRE\n\n [1]  10.58585  27.95901  22.26142  33.62564  32.95312  29.78997  26.89777\n [8]  20.64644  26.95156  27.57341  29.21039  26.23996  25.24418  27.73176\n[15]  29.79702  25.39656  27.06041 166.10166  28.54938  30.98891        NA\n\n\nLook at the “Global Environment” or run the str() function to see if a new variable was added to mydata - which should now have 15 variables instead of only 14.\nYou can also list the variable names in the updated dataset.\n\n# look at updated data structure\nstr(mydata)\n\nspc_tbl_ [21 × 15] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ SubjectID  : num [1:21] 1 2 3 4 5 6 8 9 12 14 ...\n $ Age        : num [1:21] 45 50 35 44 32 48 50 51 46 35 ...\n $ WeightPRE  : num [1:21] 68 167 143 216 243 165 60 110 167 190 ...\n $ WeightPOST : num [1:21] 145 166 135 201 223 145 132 108 158 200 ...\n $ Height     : num [1:21] 5.6 5.4 5.6 5.6 6 5.2 3.3 5.1 5.5 5.8 ...\n $ SES        : num [1:21] 9 2 2 2 2 2 2 3 2 1 ...\n $ GenderSTR  : chr [1:21] \"m\" \"f\" NA \"m\" ...\n $ GenderCoded: num [1:21] 1 2 NA 1 1 2 1 2 2 1 ...\n $ q1         : num [1:21] 4 3 3 4 5 2 3 1 1 4 ...\n $ q2         : num [1:21] NA 4 4 2 3 5 NA 4 1 44 ...\n $ q3         : num [1:21] NA 1 2 2 5 5 4 1 5 1 ...\n $ q4         : num [1:21] 4 40 3 1 2 1 3 3 5 1 ...\n $ q5         : num [1:21] 4 3 5 1 4 4 9 1 1 4 ...\n $ q6         : num [1:21] 5 2 2 9 1 5 2 4 2 5 ...\n $ bmiPRE     : num [1:21] 10.6 28 22.3 33.6 33 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   SubjectID = col_double(),\n  ..   Age = col_double(),\n  ..   WeightPRE = col_double(),\n  ..   WeightPOST = col_double(),\n  ..   Height = col_double(),\n  ..   SES = col_double(),\n  ..   GenderSTR = col_character(),\n  ..   GenderCoded = col_double(),\n  ..   q1 = col_double(),\n  ..   q2 = col_double(),\n  ..   q3 = col_double(),\n  ..   q4 = col_double(),\n  ..   q5 = col_double(),\n  ..   q6 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# list the variable names in the\n# updated dataset\nnames(mydata)\n\n [1] \"SubjectID\"   \"Age\"         \"WeightPRE\"   \"WeightPOST\"  \"Height\"     \n [6] \"SES\"         \"GenderSTR\"   \"GenderCoded\" \"q1\"          \"q2\"         \n[11] \"q3\"          \"q4\"          \"q5\"          \"q6\"          \"bmiPRE\"     \n\n\nCreate New Variable - dplyr::mutate() Approach\nIn the dplyr package, you can create or modify variables using the mutate() function.\n\n# Compute BMI forthe POST Weight\n# use the dplyr::mutate() function\nmydata &lt;- mydata %&gt;%\n  mutate(\n    bmiPOST = (WeightPOST * 703) / (Height * 12)^2\n    )\n\n# check updates\nstr(mydata)\n\ntibble [21 × 16] (S3: tbl_df/tbl/data.frame)\n $ SubjectID  : num [1:21] 1 2 3 4 5 6 8 9 12 14 ...\n $ Age        : num [1:21] 45 50 35 44 32 48 50 51 46 35 ...\n $ WeightPRE  : num [1:21] 68 167 143 216 243 165 60 110 167 190 ...\n $ WeightPOST : num [1:21] 145 166 135 201 223 145 132 108 158 200 ...\n $ Height     : num [1:21] 5.6 5.4 5.6 5.6 6 5.2 3.3 5.1 5.5 5.8 ...\n $ SES        : num [1:21] 9 2 2 2 2 2 2 3 2 1 ...\n $ GenderSTR  : chr [1:21] \"m\" \"f\" NA \"m\" ...\n $ GenderCoded: num [1:21] 1 2 NA 1 1 2 1 2 2 1 ...\n $ q1         : num [1:21] 4 3 3 4 5 2 3 1 1 4 ...\n $ q2         : num [1:21] NA 4 4 2 3 5 NA 4 1 44 ...\n $ q3         : num [1:21] NA 1 2 2 5 5 4 1 5 1 ...\n $ q4         : num [1:21] 4 40 3 1 2 1 3 3 5 1 ...\n $ q5         : num [1:21] 4 3 5 1 4 4 9 1 1 4 ...\n $ q6         : num [1:21] 5 2 2 9 1 5 2 4 2 5 ...\n $ bmiPRE     : num [1:21] 10.6 28 22.3 33.6 33 ...\n $ bmiPOST    : num [1:21] 22.6 27.8 21 31.3 30.2 ...\n\nnames(mydata)\n\n [1] \"SubjectID\"   \"Age\"         \"WeightPRE\"   \"WeightPOST\"  \"Height\"     \n [6] \"SES\"         \"GenderSTR\"   \"GenderCoded\" \"q1\"          \"q2\"         \n[11] \"q3\"          \"q4\"          \"q5\"          \"q6\"          \"bmiPRE\"     \n[16] \"bmiPOST\"    \n\n\nCreate New Variable - add labels to codes\nAs you probably noticed in the views of the mydata dataset above, there was originally a variable where people were allowed to enter their gender using free text (the GenderSTR variable). So, there were entries like “f”, “F”, “female”, “male”, “Male” and other variations. So another variable GenderCoded was included where 1=male and 2=female, but when we look at mydata$GenderCoded all we see are 1’s and 2’s and NAs.\n\nmydata$GenderCoded\n\n [1]  1  2 NA  1  1  2  1  2  2  1  1  2  2  1  1  2  1  1  2  1 NA\n\n\nIt would be nice if we could add some labels. One way to do this is to convert GenderCoded from being a simple “numeric” variable to a new class called a “factor” which included both numeric values and text labels.\nHere is the base R approach to create a new factor type variable. Learn more by looking at the help page for factor(), run help(factor, package = \"base\").\n\n# create a new factor with labels\nmydata$GenderCoded.f &lt;-\n  factor(mydata$GenderCoded,\n         levels = c(1, 2),\n         labels = c(\"Male\", \"Female\"))\n\n# look at new variable\nmydata$GenderCoded.f\n\n [1] Male   Female &lt;NA&gt;   Male   Male   Female Male   Female Female Male  \n[11] Male   Female Female Male   Male   Female Male   Male   Female Male  \n[21] &lt;NA&gt;  \nLevels: Male Female\n\n\nWe can check the type each variable using the class() function.\n\nclass(mydata$GenderCoded)\n\n[1] \"numeric\"\n\nclass(mydata$GenderCoded.f)\n\n[1] \"factor\"\n\n\nAnother quick way to see these class type differences is to use the table() function to get the frequencies of each distinct value. I’m also adding the useNA = \"ifany\" option to also get a count of any missing values. Learn more by running help(table, package = \"base\").\n\n# table of frequencies of GenderCoded - numeric class\ntable(mydata$GenderCoded, useNA = \"ifany\")\n\n\n   1    2 &lt;NA&gt; \n  11    8    2 \n\n# table of GenderCoded.f - factor class\ntable(mydata$GenderCoded.f, useNA = \"ifany\")\n\n\n  Male Female   &lt;NA&gt; \n    11      8      2",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-get-data-summary-and-descriptive-statistics.",
    "href": "module132_DataWrangling.html#to-get-data-summary-and-descriptive-statistics.",
    "title": "1.3.2: Data Wrangling",
    "section": "5. To get data summary and descriptive statistics.",
    "text": "5. To get data summary and descriptive statistics.\nGetting summary statistics\nsummary() function\nOne of the best functions that is part of base R is the summary() function. Let’s see what this gives us for the mydata dataset.\nAs you can see for all of the numeric class variables, the summary() function gives us the min, max, median, mean, 1st quartileand 3rd quartile and a count and of the the number of missing NAs. So you can see the mean Age is 44.8 and the median Age is 44.0.\nFor the character variable GenderSTR all we know is it has a length of 21.\nBut for the factor type variable GenderCoded.f we get the number of Males, Females and NAs.\n\nsummary(mydata)\n\n   SubjectID          Age          WeightPRE       WeightPOST   \n Min.   : 1.00   Min.   :24.00   Min.   : 60.0   Min.   : 98.0  \n 1st Qu.: 5.75   1st Qu.:35.75   1st Qu.:166.5   1st Qu.:142.5  \n Median :15.00   Median :44.00   Median :190.0   Median :177.0  \n Mean   :15.30   Mean   :44.80   Mean   :185.2   Mean   :172.2  \n 3rd Qu.:23.25   3rd Qu.:50.00   3rd Qu.:230.0   3rd Qu.:203.2  \n Max.   :32.00   Max.   :99.00   Max.   :260.0   Max.   :240.0  \n NA's   :1       NA's   :1       NA's   :1       NA's   :1      \n     Height           SES       GenderSTR          GenderCoded   \n Min.   :2.600   Min.   :1.0   Length:21          Min.   :1.000  \n 1st Qu.:5.475   1st Qu.:2.0   Class :character   1st Qu.:1.000  \n Median :5.750   Median :2.0   Mode  :character   Median :1.000  \n Mean   :5.550   Mean   :2.3                      Mean   :1.421  \n 3rd Qu.:6.125   3rd Qu.:2.0                      3rd Qu.:2.000  \n Max.   :6.500   Max.   :9.0                      Max.   :2.000  \n NA's   :1       NA's   :1                        NA's   :2      \n       q1              q2               q3             q4        \n Min.   : 1.00   Min.   : 1.000   Min.   :1.00   Min.   : 1.000  \n 1st Qu.: 1.75   1st Qu.: 2.000   1st Qu.:1.00   1st Qu.: 2.000  \n Median : 3.00   Median : 4.000   Median :3.00   Median : 3.000  \n Mean   : 3.35   Mean   : 5.526   Mean   :3.15   Mean   : 5.062  \n 3rd Qu.: 4.25   3rd Qu.: 4.500   3rd Qu.:4.25   3rd Qu.: 4.000  \n Max.   :11.00   Max.   :44.000   Max.   :9.00   Max.   :40.000  \n NA's   :1       NA's   :2        NA's   :1      NA's   :5       \n       q5               q6            bmiPRE          bmiPOST     \n Min.   : 1.000   Min.   :1.000   Min.   : 10.59   Min.   :12.99  \n 1st Qu.: 2.000   1st Qu.:2.000   1st Qu.: 26.03   1st Qu.:25.38  \n Median : 4.000   Median :4.000   Median : 27.65   Median :26.42  \n Mean   : 9.176   Mean   :3.706   Mean   : 33.78   Mean   :29.43  \n 3rd Qu.: 5.000   3rd Qu.:5.000   3rd Qu.: 29.79   3rd Qu.:28.71  \n Max.   :99.000   Max.   :9.000   Max.   :166.10   Max.   :70.77  \n NA's   :4        NA's   :4       NA's   :1        NA's   :1      \n GenderCoded.f\n Male  :11    \n Female: 8    \n NA's  : 2    \n              \n              \n              \n              \n\n\nSo, the summary() function is helpful, but you’ll notice we do not get the standard deviation. For some reason that was left out of the original summary() statistics function.\nThere are a few other descriptive statistics functions that can be useful. There is a describe() function in both the Hmisc package and the psych packages.\nHmisc::describe() function\nLet’s look at Hmisc::describe() for a couple of the variables.\nYou’ll notice that this still doesn’t give us the standard deviation, but we get the min,max, mean, median, as well as the .05 5th percentile and others, and the output includes a summary ofthe frequency of the distinct values.\n\nmydata %&gt;%\n  select(Age, GenderCoded.f, bmiPRE) %&gt;%\n  Hmisc::describe()\n\n. \n\n 3  Variables      21  Observations\n--------------------------------------------------------------------------------\nAge \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n      20        1       14    0.994     44.8       43    13.81    31.60 \n     .10      .25      .50      .75      .90      .95 \n   34.70    35.75    44.00    50.00    51.10    54.35 \n                                                                           \nValue        24   32   35   36   40   43   44   45   46   48   50   51   52\nFrequency     1    1    3    2    1    1    2    1    1    1    2    2    1\nProportion 0.05 0.05 0.15 0.10 0.05 0.05 0.10 0.05 0.05 0.05 0.10 0.10 0.05\n               \nValue        99\nFrequency     1\nProportion 0.05\n\nFor the frequency table, variable is rounded to the nearest 0\n--------------------------------------------------------------------------------\nGenderCoded.f \n       n  missing distinct \n      19        2        2 \n                        \nValue        Male Female\nFrequency      11      8\nProportion  0.579  0.421\n--------------------------------------------------------------------------------\nbmiPRE \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n      20        1       20        1    33.78    27.73    18.52    20.14 \n     .10      .25      .50      .75      .90      .95 \n   22.10    26.03    27.65    29.79    33.02    40.25 \n\n10.5858489229025 (1, 0.05), 20.6464394036482 (1, 0.05), 22.2614175878685 (1,\n0.05), 25.2441827061189 (1, 0.05), 25.3965599815035 (1, 0.05), 26.2399593896503\n(1, 0.05), 26.8977655341292 (1, 0.05), 26.9515610651974 (1, 0.05),\n27.0604126424232 (1, 0.05), 27.5734079799181 (1, 0.05), 27.7317554240631 (1,\n0.05), 27.9590096784027 (1, 0.05), 28.5493827160494 (1, 0.05), 29.2103855937103\n(1, 0.05), 29.7899716469428 (1, 0.05), 29.7970241970486 (1, 0.05),\n30.9889051649305 (1, 0.05), 32.953125 (1, 0.05), 33.6256377551021 (1, 0.05),\n166.101660092045 (1, 0.05)\n\nFor the frequency table, variable is rounded to the nearest 0\n--------------------------------------------------------------------------------\n\n\npsych::describe() function\nThe psych::describe() function only works on numeric data. So, let’s look at Age and bmiPRE. This function now gives us the standard deviation sd and even the mad which is the mean absolute deviation.\n\nmydata %&gt;%\n  select(Age, bmiPRE) %&gt;%\n  psych::describe()\n\n       vars  n  mean    sd median trimmed   mad   min   max  range skew\nAge       1 20 44.80 14.87  44.00   43.06 10.38 24.00  99.0  75.00 2.21\nbmiPRE    2 20 33.78 31.53  27.65   27.79  3.17 10.59 166.1 155.52 3.66\n       kurtosis   se\nAge        6.10 3.32\nbmiPRE    12.53 7.05\n\n\nBase R specific statistics functions\nThere are many built-in functions in base R for computing specific statistics like mean(), sd() for standard deviation, median(), min(), max() and quantile() to get specific percentiles.\nLet get some summary statistics for different variablesin mydata.\n\n# get min, max for Age\nmin(mydata$Age)\n\n[1] NA\n\nmax(mydata$Age)\n\n[1] NA\n\n\nWAIT!? - why did I get NA? Since there is missing data in this dataset, we need to tell these R functions how to handle the missing data. We need to add na.rm=TRUE to remove the NAs and then compute the min() and max() for the non-missing values.\n\nmin(mydata$Age, na.rm = TRUE)\n\n[1] 24\n\nmax(mydata$Age, na.rm = TRUE)\n\n[1] 99\n\n\nIf we want, we could get the non-parametric statistics of median, 25th and 75th percentiles for the interquartile range.Let’s get these statistics for bmiPRE.\n\n# get median bmiPRE\n# and 25th and 75th percentiles for bmiPRE\nmedian(mydata$bmiPRE,\n       na.rm = TRUE)\n\n[1] 27.65258\n\nquantile(mydata$bmiPRE, \n         probs = 0.25,\n         na.rm = TRUE)\n\n     25% \n26.02911 \n\nquantile(mydata$bmiPRE, \n         probs = 0.75,\n         na.rm = TRUE)\n\n     75% \n29.79173 \n\n\nWe can also get the mean() and sd() for Height.\n\nmean(mydata$Height, na.rm = TRUE)\n\n[1] 5.55\n\nsd(mydata$Height, na.rm = TRUE)\n\n[1] 0.9795273\n\n\ndplyr::summarize() function\nThe dplyr package also has a summarize() function you can use to get specific statistics of your choosing. For example, let’s get the mean() and sd() for Age.\n\nmydata %&gt;%\n  dplyr::summarise(\n    mean_age = mean(Age, na.rm = TRUE),\n    sd_age = sd(Age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 2\n  mean_age sd_age\n     &lt;dbl&gt;  &lt;dbl&gt;\n1     44.8   14.9\n\n\nWe can do this same code again but add the dplyr::group_by() function to add a grouping variable to get the statistics by.\nLet’s get the summary stats (mean and sd) for Age by GenderCoded.f.\n\nmydata %&gt;%\n  dplyr::group_by(GenderCoded.f) %&gt;%\n  dplyr::summarise(\n    mean_age = mean(Age, na.rm = TRUE),\n    sd_age = sd(Age, na.rm = TRUE)\n  )\n\n# A tibble: 3 × 3\n  GenderCoded.f mean_age sd_age\n  &lt;fct&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n1 Male              41.5   8.77\n2 Female            50.6  20.5 \n3 &lt;NA&gt;              35    NA   \n\n\nMake summary tables\nCreating nicely formatted summary tables is an active area of development in the R community. So, I’m sure there are new functions and packages that I may not have shown here. But here are a few packages I use often for making tables of summary statistics.\narsenal package for tables\nThe arsenal package is useful for making tables - especially with Rmarkdown - to be explained further in a later session Module 1.3.6. Learn more at tableby() vignette.\nHere is a quick example of some summary statistics for Age, bmiPRE, and SES by GenderCoded.f using the tableby() function.\nFirst let’s add labels for SES and create a factor variable.\n\nmydata$SES.f &lt;- \n  factor(mydata$SES,\n         levels = c(1, 2, 3),\n         labels = c(\"low income\",\n                    \"average income\",\n                    \"high income\"))\n\nlibrary(arsenal)\ntab1 &lt;- tableby(GenderCoded.f ~ Age + bmiPRE +SES.f, \n                data = mydata)\nsummary(tab1)\n\n\n\n\n\n\n\n\n\n\nMale (N=11)\nFemale (N=8)\nTotal (N=19)\np value\n\n\n\nAge\n\n\n\n0.199\n\n\n   Mean (SD)\n41.455 (8.768)\n50.625 (20.493)\n45.316 (15.089)\n\n\n\n   Range\n24.000 - 52.000\n35.000 - 99.000\n24.000 - 99.000\n\n\n\nbmiPRE\n\n\n\n0.370\n\n\n   Mean (SD)\n40.230 (42.193)\n26.347 (2.785)\n34.384 (32.274)\n\n\n\n   Range\n10.586 - 166.102\n20.646 - 29.790\n10.586 - 166.102\n\n\n\nSES.f\n\n\n\n0.625\n\n\n   N-Miss\n1\n0\n1\n\n\n\n   low income\n2 (20.0%)\n2 (25.0%)\n4 (22.2%)\n\n\n\n   average income\n7 (70.0%)\n4 (50.0%)\n11 (61.1%)\n\n\n\n   high income\n1 (10.0%)\n2 (25.0%)\n3 (16.7%)\n\n\n\ngtsummary package for tables\nThe gtsummary package is also useful for making tables. We can even use it to make nicely formatted tables in the “Viewer” window pane or in Rmarkdown. Learn more at tbl_summary() vignette.\nHere is a quick example of some summary statistics for Age and bmiPRE by GenderCoded.f using the tbl_summary() function.\n\nlibrary(gtsummary)\n\nmydata %&gt;%\n  select(Age, bmiPRE, SES.f, GenderCoded.f) %&gt;%\n  tbl_summary(by = GenderCoded.f)\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nMale N = 111\n\n\nFemale N = 81\n\n\n\n\nAge\n44 (35, 50)\n47 (38, 51)\n\n\nbmiPRE\n29 (27, 33)\n27 (25, 28)\n\n\nSES.f\n\n\n\n\n    low income\n2 (20%)\n2 (25%)\n\n\n    average income\n7 (70%)\n4 (50%)\n\n\n    high income\n1 (10%)\n2 (25%)\n\n\n    Unknown\n1\n0\n\n\n\n\n1 Median (Q1, Q3); n (%)\n\n\n\n\n\ngmodels package for R-x-C tables\nIf we want to look at a “cross-table” similar to output from SPSS or SAS, the gmodels package has the CrossTable() function that creates text-based tables similar to these other statistics software packages.\nLet’s get the frequencies and columns percentages for gender by SES. The first variable is the row variable, the second is the column variable.\n\nlibrary(gmodels)\n\nCrossTable(mydata$GenderCoded.f,  # row variable\n           mydata$SES.f,          # column variable\n           prop.t = FALSE,        # turn off percent of total\n           prop.r = FALSE,        # turn off percent of row\n           prop.c = TRUE,         # turn off percent of column\n           prop.chisq = FALSE,     # turn off percent for chisq test\n           format = \"SPSS\"        # format like SPSS\n           )\n\n\n   Cell Contents\n|-------------------------|\n|                   Count |\n|          Column Percent |\n|-------------------------|\n\nTotal Observations in Table:  18 \n\n                     | mydata$SES.f \nmydata$GenderCoded.f |     low income  | average income  |    high income  |      Row Total | \n---------------------|----------------|----------------|----------------|----------------|\n                Male |             2  |             7  |             1  |            10  | \n                     |        50.000% |        63.636% |        33.333% |                | \n---------------------|----------------|----------------|----------------|----------------|\n              Female |             2  |             4  |             2  |             8  | \n                     |        50.000% |        36.364% |        66.667% |                | \n---------------------|----------------|----------------|----------------|----------------|\n        Column Total |             4  |            11  |             3  |            18  | \n                     |        22.222% |        61.111% |        16.667% |                | \n---------------------|----------------|----------------|----------------|----------------|",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#references",
    "href": "module132_DataWrangling.html#references",
    "title": "1.3.2: Data Wrangling",
    "section": "References",
    "text": "References\n\n\nBolker, Gregory R. Warnes andBen, Thomas Lumley, Randall C Johnson, and Randall C. Johnson. 2022. Gmodels: Various r Programming Tools for Model Fitting. https://CRAN.R-project.org/package=gmodels.\n\n\nHeinzen, Ethan, Jason Sinnwell, Elizabeth Atkinson, Tina Gunderson, and Gregory Dougherty. 2021. Arsenal: An Arsenal of r Functions for Large-Scale Statistical Summaries. https://github.com/mayoverse/arsenal.\n\n\nIannone, Richard. 2023. Fontawesome: Easily Work with Font Awesome Icons. https://github.com/rstudio/fontawesome.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSjoberg, Daniel D., Joseph Larmarange, Michael Curry, Jessica Lavery, Karissa Whiting, and Emily C. Zabor. 2024. Gtsummary: Presentation-Ready Data Summary and Analytic Result Tables. https://github.com/ddsjoberg/gtsummary.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery, and Joseph Larmarange. 2021. “Reproducible Summary Tables with the Gtsummary Package.” The R Journal 13: 570–80. https://doi.org/10.32614/RJ-2021-053.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#other-helpful-resources",
    "href": "module132_DataWrangling.html#other-helpful-resources",
    "title": "1.3.2: Data Wrangling",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theme 1. Measurement and Prediction of IPV",
    "section": "",
    "text": "Presented by Melinda Higgins, PhD\nModule 1.3 will introduce the learner to writing code in the R language and utilizing the RStudio IDE (integrated development environment). This module will include 6 sessions - the first 3 will be asynchronous-online (AO) and the last 3 will be in-person (IP).\nThe 6 sessions are:\n\n1.3.1: Introduction to R and R Studio (AO)\n1.3.2: Data Wrangling (AO)\n1.3.3: Data Visualization (AO)\n1.3.4: Missing data and sampling weight (IP)\n1.3.5: Statistical tests and models (IP)\n1.3.6: Putting reproducible research principles into practice (IP)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#module-1.3-data-analytics-using-r",
    "href": "index.html#module-1.3-data-analytics-using-r",
    "title": "Theme 1. Measurement and Prediction of IPV",
    "section": "",
    "text": "Presented by Melinda Higgins, PhD\nModule 1.3 will introduce the learner to writing code in the R language and utilizing the RStudio IDE (integrated development environment). This module will include 6 sessions - the first 3 will be asynchronous-online (AO) and the last 3 will be in-person (IP).\nThe 6 sessions are:\n\n1.3.1: Introduction to R and R Studio (AO)\n1.3.2: Data Wrangling (AO)\n1.3.3: Data Visualization (AO)\n1.3.4: Missing data and sampling weight (IP)\n1.3.5: Statistical tests and models (IP)\n1.3.6: Putting reproducible research principles into practice (IP)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#project-tidal",
    "href": "index.html#project-tidal",
    "title": "Theme 1. Measurement and Prediction of IPV",
    "section": "Project TIDAL",
    "text": "Project TIDAL\nProject TIDAL is an NIH-funded R25 study (1 R25 NR021324-01 ), co-led by Drs. Sangmi Kim and Ran Xiao from Nell Hodgson Woodruff School of Nursing at Emory University.\nThe objective of the study is to develop a short course titled “Trauma-Informed Data Science and Digital Health Technologies to Prevent Intimate Partner Violence (IPV) among Pregnant/Postpartum Women” targeting early-career researchers (predoctoral, postdoctoral, and junior faculty) from diverse backgrounds across the U.S.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "PRAMS.html",
    "href": "PRAMS.html",
    "title": "Setup Project for PRAMS Data Analysis",
    "section": "",
    "text": "PRAMS is the Pregnancy Risk Assessment Monitoring System (PRAMS). According to the CDC’s website for About PRAMS:\n\n\n\n\n\n\nWhat is PRAMS?\n\n\n\nPRAMS is the Pregnancy Risk Assessment Monitoring System. It is a joint surveillance project between state, territorial, or local health departments and CDC’s Division of Reproductive Health. PRAMS was developed in 1987 to reduce infant morbidity and mortality by influencing maternal behaviors before, during, and immediately after live birth.\n\n\n\n\n\n\n\n\nWhat is the purpose of PRAMS?\n\n\n\nThe purpose of PRAMS is to find out why some infants are born healthy and others are not. The survey asks new mothers questions about their pregnancy and their new infant. The questions give us important information about the mother and the infant and help us learn more about the impacts of health and behaviors.\n\n\n\n\nYou can request the PRAMS Data from the CDC.\nOnce granted access, follow the instructions from the CDC to download the data and sign the data sharing agreement.\nFor the purposes of the TIDAL R training session, we will be working with PRAMS Phase 8 ARF (Automated Research File) dataset.\n\n\nSee the details on the PRAMS Questionnaires.\nLearn more about the PRAMS Data Methodology including details on how the samples are weighted.\n\nDownload and Read this helpful paper on PRAMS design and methodology (Shulman, D’Angelo, Harrison, Smith, and Warner, 2018).\nThere are also helpful tutorial videos on working with PRAMS data by ASSOCIATION OF STATE AND TERRITORIAL HEALTH OFFICIALS (ASTHO.org).",
    "crumbs": [
      "Setup Project for PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#prams-data",
    "href": "PRAMS.html#prams-data",
    "title": "Setup Project for PRAMS Data Analysis",
    "section": "",
    "text": "PRAMS is the Pregnancy Risk Assessment Monitoring System (PRAMS). According to the CDC’s website for About PRAMS:\n\n\n\n\n\n\nWhat is PRAMS?\n\n\n\nPRAMS is the Pregnancy Risk Assessment Monitoring System. It is a joint surveillance project between state, territorial, or local health departments and CDC’s Division of Reproductive Health. PRAMS was developed in 1987 to reduce infant morbidity and mortality by influencing maternal behaviors before, during, and immediately after live birth.\n\n\n\n\n\n\n\n\nWhat is the purpose of PRAMS?\n\n\n\nThe purpose of PRAMS is to find out why some infants are born healthy and others are not. The survey asks new mothers questions about their pregnancy and their new infant. The questions give us important information about the mother and the infant and help us learn more about the impacts of health and behaviors.\n\n\n\n\nYou can request the PRAMS Data from the CDC.\nOnce granted access, follow the instructions from the CDC to download the data and sign the data sharing agreement.\nFor the purposes of the TIDAL R training session, we will be working with PRAMS Phase 8 ARF (Automated Research File) dataset.\n\n\nSee the details on the PRAMS Questionnaires.\nLearn more about the PRAMS Data Methodology including details on how the samples are weighted.\n\nDownload and Read this helpful paper on PRAMS design and methodology (Shulman, D’Angelo, Harrison, Smith, and Warner, 2018).\nThere are also helpful tutorial videos on working with PRAMS data by ASSOCIATION OF STATE AND TERRITORIAL HEALTH OFFICIALS (ASTHO.org).",
    "crumbs": [
      "Setup Project for PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#prework---before-you-begin",
    "href": "PRAMS.html#prework---before-you-begin",
    "title": "Setup Project for PRAMS Data Analysis",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\nInstall R Packages\nBefore you begin, please go ahead and install (or make sure these are already installed) on your computer for these following packages - these are all on CRAN, so you can install them using the RStudio Menu Tools/Install Packages interface:\n\nhaven\ndplyr\nsurvey\n\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(survey)\n\nCreate a NEW RStudio Project\nBEFORE you being any new analysis project, it is ALWAYS a good idea to begin with the NEW RStudio project.\nGo to the RStudio menu “File/New Project” and create your new project (ideally in a NEW directory, but it is also ok to use an exisiting directory/folder on your computer).\nThis new directory (or folder) will be where all of your files will “live” for your current analysis project.\nSee the step-by-step instructions for creating a new RStudio project in Module 1.3.2.",
    "crumbs": [
      "Setup Project for PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#get-prams-data-and-select-subset-for-analysis",
    "href": "PRAMS.html#get-prams-data-and-select-subset-for-analysis",
    "title": "Setup Project for PRAMS Data Analysis",
    "section": "1. Get PRAMS Data and Select Subset for Analysis",
    "text": "1. Get PRAMS Data and Select Subset for Analysis\nA. Read-in the PRAMS Phase 8 2016-2021 combined dataset\nThe PRAMS data provided by the CDC will be in SAS format (*.sas7bdat). We can read the native SAS file into R using the haven package and the read_sas() function.\n\n\n\n\n\n\nMemory Warning\n\n\n\nThe size of the phase8_arf_2016_2021.sas7bdat dataset is a little over 1GB. So, make sure your computer has enough available memory to fully load this dataset. I will provide some more details below on how we can reduce the size of the dataset and improve the memory issues below.\n\n\nYou can check your available memory, by checking your “Global Environment” TAB (upper right window pane) click on the down arrow next to the icon with “XX MiB” just to the left of the little broom:\n\n\n\nClick on the “Memory Usage Report” to see a detailed breakdown. This window will show:\n\nMemory used by R objects (in your “Global Environment”)\nMemory used on your computer by your current R Session\nMemory currently in use for everything currently running on your computer (all apps running - active and in background) - you can compare this to your “task manager” memory viewer.\nFree System Memory - when this gets low the “XX MiB” graphic will change color from green - to yellow - to orange - to red. Once you get to red, your R session will most likely crash since there is not enough memory to perfom operations or run analyses.\n\nThis is a screen shot of my computer (yours will look different) BEFORE I load the PRAMS dataset.\n\n\n\nRun the following R code to load the PRAMS Phase 8 dataset into your R Session and check the “Global Environment”.\n\nlibrary(haven)\nprams &lt;- \n  read_sas(\"phase8_arf_2016_2021.sas7bdat\")\n\nHere is my memory AFTER loading the PRAMS dataset into my “Global Environment”.\n\n\n\n\n\nB. Save the data as a *.RData binary file for use in later analyses\nOne way to reduce the size of the PRAMS dataset is to save it as a native *.RData binary file format. So, let’s save the PRAMS dataset in this format on your computer.\n\n# save the whole dataset as *.RData format\nsave(prams, \n     file = \"prams.RData\")\n\nOn my computer, here is a comparison of the size of these 2 files:\n\n\nphase8_arf_2016_2021.sas7bdat is 1,095,499,776 bytes (which is 1.02 GB)\n\nprams.RData is only 34,713,319 (which is only 0.0323 GB)\n\nThis is a file size reduction of 96.83%!!\n\n\n\nNow that we’ve reduced the file size of the dataset on your computer’s hard drive (or cloud storage), let’s also clear up the “Global Environment” back in your current RStudio computing session.\nC. Clean up files to save memory\nNow that we’ve saved the data, let’s remove the PRAMS data object from the RStudio session.\n\nFor now we can simply remove everything using the rm(list=ls()).\nHowever, if you have other objects you want to keep, you can specifically only remove the PRAMS dataset using rm(prams).\n\n\n# remove all objects from Global Environment\nrm(list=ls())\n\n# confirm Global Environment is empty\n# list all objects\nls()\n\ncharacter(0)\n\n# and free any currently unused memory\ngc()\n\n          used  (Mb) gc trigger   (Mb)  max used  (Mb)\nNcells 2107853 112.6    4136027  220.9   4136027 220.9\nVcells 3850536  29.4  153274906 1169.4 112103075 855.3\n\n\nAfter we remove everything, let’s look at the session memory again.\n\n\n\nNow let’s read the PRAMS data back in, but this time read in the prams.RData binary R data formatted file. We will use the built-in load() function.\n\n# load back only the prams dataset\nload(file = \"prams.RData\")\n\nLet’s check the R session memory again:\n\n\n\nI know this didn’t make a large difference for the R session available memory, but by doing this process:\n\nThe PRAMS dataset now takes up less memory on your computer’s file storage, and\nThe load() function for the prams.RData file should run faster when beginning your R computing session instead of having to use the haven package to read in the SAS formatted file everytime.\n\nAs a quick comparison on my computer (Windows 11), the time to read in the SAS formatted file was about 14 sec:\n&gt; system.time(\n+   prams &lt;- \n+     read_sas(\"phase8_arf_2016_2021.sas7bdat\")\n+ )\n   user  system elapsed \n  13.44    0.47   13.96\nAnd the time to read in the prams.RData file was only about 1.5 sec.\n&gt; system.time(\n+   load(\"prams.RData\")\n+ )\n   user  system elapsed \n   1.45    0.08    1.54",
    "crumbs": [
      "Setup Project for PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#getting-started-with-prams-data",
    "href": "PRAMS.html#getting-started-with-prams-data",
    "title": "Setup Project for PRAMS Data Analysis",
    "section": "2. Getting started with PRAMS Data",
    "text": "2. Getting started with PRAMS Data\nBreastfeeding summary - UNWEIGHTED data\nLet’s look at whether the mother ever breastfed her baby - this is variable BF5EVER, where 1 = “NO” and 2 = “YES”.\nPRAMS Phase 8 Codebook\n\n# create a factor variable\n# and add labels\nprams$BF5EVER.f &lt;- factor(\n  prams$BF5EVER,\n  levels = c(1, 2),\n  labels = c(\"NO\", \"YES\")\n)\n\nFor the UNWEIGHTED data, let’s get a simple table of breastfeeding by STATE (variable STATE) and YEAR (variable NEST_YR).\nAs we can see below, in 2017 for the state of GA, 919 women responded to this question:\n\n919 women responded\n\n170 said NO\n749 said YES\n\n\n36 were missing a response (indicated by &lt;NA&gt;)\n\n\nprams %&gt;%\n  filter(NEST_YR == 2017) %&gt;% \n  with(., table(STATE, BF5EVER.f, \n                useNA = \"ifany\"))\n\n     BF5EVER.f\nSTATE   NO  YES &lt;NA&gt;\n   AK   71  927   47\n   AL  181  659   42\n   CO   73 1037   18\n   DE  126  728   37\n   GA  170  749   36\n   IA  136  867   30\n   IL  140 1048   36\n   KS   81  856   58\n   KY  139  536   27\n   LA  285  586   23\n   MA  115 1268   40\n   MD   97  928   35\n   ME   88  754   30\n   MI  290 1532   75\n   MO  166  908   37\n   MT   66  851   20\n   ND  102  472   17\n   NH   42  523   15\n   NJ  125 1102   31\n   NM  123 1038   19\n   NY  109  706   33\n   PA  164 1023   42\n   PR   81  928   23\n   RI  105  960   37\n   SD  150  946   35\n   UT   93 1305   49\n   VA   88  969   26\n   VT   54  780   14\n   WA   69 1138   31\n   WI  221 1051   74\n   WV  186  475   38\n   WY   49  438   16\n   YC   99 1125   69\n\n\nThis aligns with the CDC PRAMS Indicators Report for GA in 2020 - scroll to the bottom to see the RAW count of 919 women who responded to “Ever Breastfed” in GA in 2017.\nBreastfeeding summary - WEIGHTED data\nIn the CDC PRAMS Indicators Report for GA in 2020 the columns that have the 95% CI (confidence intervals) for the percentages are the population weighted percentage estimates for the Stats of GA during that year.\nTo get the estimated percentage of women in the stats of GA who had “ever breastfed” in 2017, we need to use the survey package and apply the proper sample weighting to get these estimates.\nFrom this we can see that the population estimates for 2017 are:\n\nBreastfed ever = NO: 17639.96 +/- 2045.415\nBreastfed ever = YES: 101686.10 +/- 2271.075\n\nThis leads to a percentage of YES estimate of 101686.10 * 100 / (101686.10 + 17639.96) = 85.2170096% which should match pretty closely to what is in the CDC PRAMS Indicators Report for GA in 2020.\nWe can also get the percentage of overall breastfeeding YES for the USA for the 40 “states” (technically 38 states, Puerto Rico, and New York City) that were included in the PRAMS dataset in 2020 (see the last column in the CDC report), using the following R code. Note: 2 “states” did not have data in 2020: Connecticut and Florida.\nFrom this we can see that the population estimates for the “whole USA” for 2020 were:\n\nBreastfed ever = NO: 225560.3 +/- 4884.871\nBreastfed ever = YES: 1609464 +/- 5540.240\n\nThis leads to a percentage of YES estimate of 1609464 * 100 / (1609464 + 225560.3) = 87.7080483% which is pretty close to what is in the CDC PRAMS Indicators Report for GA in 2020 - with some numerical precision variation due to software algorithms.\nCongratulations on getting started with the PRAMS Dataset",
    "crumbs": [
      "Setup Project for PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#references",
    "href": "PRAMS.html#references",
    "title": "Setup Project for PRAMS Data Analysis",
    "section": "References",
    "text": "References\n\n\nBates, Douglas, Martin Maechler, and Mikael Jagan. 2024. Matrix: Sparse and Dense Matrix Classes and Methods. https://Matrix.R-forge.R-project.org.\n\n\nBoettiger, Carl. 2021. Knitcitations: Citations for Knitr Markdown Files. https://github.com/cboettig/knitcitations.\n\n\nLumley, Thomas. 2004. “Analysis of Complex Survey Samples.” Journal of Statistical Software 9 (1): 1–19.\n\n\n———. 2010. Complex Surveys: A Guide to Analysis Using r: A Guide to Analysis Using r. John Wiley; Sons.\n\n\nLumley, Thomas, Peter Gao, and Ben Schneider. 2024. Survey: Analysis of Complex Survey Samples. http://r-survey.r-forge.r-project.org/survey/.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nShulman, Holly B., Denise V. D’Angelo, Leslie Harrison, Ruben A. Smith, and Lee Warner. 2018. “The Pregnancy Risk Assessment Monitoring System (PRAMS): Overview of Design and Methodology.” American Journal of Public Health 108 (10): 1305–13. https://doi.org/10.2105/ajph.2018.304563.\n\n\nTerry M. Therneau, and Patricia M. Grambsch. 2000. Modeling Survival Data: Extending the Cox Model. New York: Springer.\n\n\nTherneau, Terry M. 2024. Survival: Survival Analysis. https://github.com/therneau/survival.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. Haven: Import and Export SPSS, Stata and SAS Files. https://haven.tidyverse.org.",
    "crumbs": [
      "Setup Project for PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#other-helpful-resources",
    "href": "PRAMS.html#other-helpful-resources",
    "title": "Setup Project for PRAMS Data Analysis",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "Setup Project for PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "additionalResources.html",
    "href": "additionalResources.html",
    "title": "Additional Help and Resources",
    "section": "",
    "text": "Download: R from CRAN\n\nThis is where you can download the R language software for FREE for your own computer.\nChoose your operating system (Mac OS or Windows or Linux/Unix)\nNOTE: For Windows, you should also download and install Rtools - this is technically optional, but is useful to have. Make sure to download the one for your R version.\n\nDownload: RStudio IDE Desktop\n\nNote: Windows is listed at the top - just scroll down to see the installer for the Mac OS as well. There are also installers for the versions of Linux/Unix.\n\nRStudio Education\nRStudio Cloud Tutorials\n** Quick-R **\nRmarkdown Tutorial\ntidyverse",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#r-and-rstudio-helpful-resources",
    "href": "additionalResources.html#r-and-rstudio-helpful-resources",
    "title": "Additional Help and Resources",
    "section": "",
    "text": "Download: R from CRAN\n\nThis is where you can download the R language software for FREE for your own computer.\nChoose your operating system (Mac OS or Windows or Linux/Unix)\nNOTE: For Windows, you should also download and install Rtools - this is technically optional, but is useful to have. Make sure to download the one for your R version.\n\nDownload: RStudio IDE Desktop\n\nNote: Windows is listed at the top - just scroll down to see the installer for the Mac OS as well. There are also installers for the versions of Linux/Unix.\n\nRStudio Education\nRStudio Cloud Tutorials\n** Quick-R **\nRmarkdown Tutorial\ntidyverse",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#other-helpful-websites",
    "href": "additionalResources.html#other-helpful-websites",
    "title": "Additional Help and Resources",
    "section": "Other Helpful Websites",
    "text": "Other Helpful Websites\n\nDatacamp\nR for SAS Users - My Datacamp Course\nCoursera\nReproducible Templates for Analysis and Dissemination - My Coursera Course\nEmory N741\nEmory N736",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#helpful-online-books",
    "href": "additionalResources.html#helpful-online-books",
    "title": "Additional Help and Resources",
    "section": "Helpful Online Books",
    "text": "Helpful Online Books\n\nBook: Statistical Inference via Data Science\nBook: The Epidemiologist R Handbook\nBook/Course: Stat 545",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#other-places-to-get-help",
    "href": "additionalResources.html#other-places-to-get-help",
    "title": "Additional Help and Resources",
    "section": "Other places to get HELP",
    "text": "Other places to get HELP\n\nStackOverflow\n\nI encourage you to create an account so you can post questions. But even without an account you can search for and find answers to your questions and error messages.\n\nGoogle\n\nYou can often cut and paste error messages in Google to find answers - most likely will redirect you to Stack Overflow.\n\nPackage vignettes for packages on CRAN\n\nHere is one vignette for dplyr\nThese will often help you get started.\n\nGithub package issues\n\nMany packages will host their code on Github which includes an “issues” tab. This can be a good place to see what other problems people may be having with a given package.\ndplyr issues on Github\n\nCRAN package site\n\ndplyr on CRAN - spend time looking at:\n\nthe README for the package or\nbug reports or\nNEWS which will detail the changes for each version updates\n\n\nR Bloggers\n\nThis is a really good website which curates thousands of people who are R developers, users and programmers who post articles about R.",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html",
    "href": "module131_IntroRRStudio.html",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "",
    "text": "Get acquainted with R and R Studio\nWrite simple R code in Console\nCreate your first R script\nInstall and load R packages (understanding your R session)\nCreate your first R Markdown report and produce output files in different formats (HTML, PDF, or DOCX)",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#session-objectives",
    "href": "module131_IntroRRStudio.html#session-objectives",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "",
    "text": "Get acquainted with R and R Studio\nWrite simple R code in Console\nCreate your first R script\nInstall and load R packages (understanding your R session)\nCreate your first R Markdown report and produce output files in different formats (HTML, PDF, or DOCX)",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#prework---before-you-begin",
    "href": "module131_IntroRRStudio.html#prework---before-you-begin",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\n\n\n\n\n\n\nR versus RStudio\n\n\n\nNote: R is the name of the programming language itself and RStudio is an integrated development environment (IDE) which is an enhanced interface for better organization, files management and analysis workflows.\n\n\nSoftware and Applications to Download\n\nFIRST, Download and install R onto your computer from https://cran.r-project.org/.\nNEXT, After installing R, download and install RStudio Desktop onto your computer from https://posit.co/download/rstudio-desktop/.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#get-aquainted-with-r-and-r-studio",
    "href": "module131_IntroRRStudio.html#get-aquainted-with-r-and-r-studio",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "1. Get aquainted with R and R Studio",
    "text": "1. Get aquainted with R and R Studio\nBasic R\nWhen you download R from CRAN and install it on your computer, there is a R application that you can run. However, it is very bare bones. Here is a screenshot of what it looks like on my computer (Windows 11 operating system).\n\nYou can type commands in the console window at the prompt “&gt;” but this is slow and tedious. You can also write and execute scripts from inside this application and see the output back in the console window as well as creating plots. But managing large projects using this interface is not efficient.\n\n\nRStudio IDE\nThe RStudio Integrated Development Environment (IDE) application provides much better tools for managing files within a given “project”. This biggest advantage of working in an IDE is everything is contained and managed within a given project, which is linked to a specific folder (container) on your computer (or cloud drive you may have access to).\nHowever, you will still need to write and execute code using scripts and related files. An IDE is NOT a GUI (graphical user interface) which is the “point and click” workflow you may have experience with if you’ve used other analysis software applications such as SPSS, SAS Studio, Excel and similar.\n\nThe interface is usually arranged with the following 4 “window panes”:\n\nConsole\nSource\nEnvironment\nFiles\n\n\nThe typical arrangement, usually has the “Console” window pane at the bottom left. This window also usually has TABs for the “Terminal” and any “Background Jobs” that might be running.\n\n\nThe “Source” window pane is usually at the top left. This is where you will do most of your editing of your R program scripts (*.R) or Rmarkdown files (*.Rmd). This is also where the data viewer window will open. You can also open and edit other kinds of files here as well (*.tex, *.css, *.txt, and more).\n\n\nThe top right window pane should always have your “Environment”, “History” and “Tutorial” TABs but may also have TABs for “Build” and “Git” and others depending on your project type and options selected.\n\n\nThe bottom right window pane has TABs for your:\n\n“Files” directory\n“Plots” window for graphical output\n“Packages” - which lists all add-on R packages installed on your computer\n“Help” window\nas well as other TABs for “Viewer” and “Presentation” for viewing other kinds of output.\n\n\n\nCustomizing your RStudio interface\nYou also have the option to rearrange your window panes as well as change the look and feel of your programming interface and much more. To explore all of your options, click on the menu at the top for “Tools/Global Options”:\n\nTake a look at the left side for the list of all of the options. Some of the most useful options to be aware of are:\n\nGeneral\nAppearance, and\nPane Layout\n\n\nIn the “General” TAB, this is where you can see and confirm that R is installed and where the R programming language app is installed on your computer.\n\n\nYou will probably want to explore tuning these appearance parameters to customize the appears to your preferences. For example, you can change the ZOOM level to improve readability. You may also want to change the FONT sizes for the Editor and Help windows as needed.\n\n\n\n\n\n\nZOOM + FONT\n\n\n\nWhen making changes to your RStudio interface appearance, be aware that ZOOM and FONT size settings work together, so you may need to play around with the settings that work best for your monitor or device you are using.\n\n\nI also encourage you to try out different “Editor Themes” which will change the colors of the R code as well as background colors (light or dark).\n\nThe default “Editor Theme” is textmate.\n\nBut here is an example of changing the theme to “Tomorrow Night Blue”.\n\n\nI would also suggest NOT changing the layout of the window panes until you are very familiar with the default settings. But in “Pane Layout” is where you can see what the default layout settings are and what other options are available to you.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#write-simple-r-code-in-console",
    "href": "module131_IntroRRStudio.html#write-simple-r-code-in-console",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "2. Write simple R code in Console",
    "text": "2. Write simple R code in Console\nSimple math\nSo, let’s start with some simple R code using the Console window and typing commands at the &gt; prompt (which is the greater than symbol).\nYou can write simple math expressions like 5 + 5.\n\n5 + 5\n\n[1] 10\n\n\n\nNotice that the output shows the number 1 enclosed in square brackets [] followed by the answer (or output) of 10.\nThis is because R performed the addition operation using the + operator and then “saved” the output in temporary memory as a scalar object with 1 element, which is the number 10.\nYou can actually see this temporary object by typing .Last.value - which is only temporary and will be overwritten after the execution of the next R command.\n.Last.value\n[1] 10\nHowever, if we look at our current computing environment (see upper right window pane), it is still showing as empty.\n\nThis is because we have not yet “saved” the output into an object that we created. Let’s save the output from 5 + 5 into an object called ten.\nTo do this we need to do 2 things:\n\nCreate the object called ten by\nUsing the “assign” operator &lt;- to take the result of 5 + 5 and move it (save it or pipe it) into the object ten.\n\n\nten &lt;- 5 + 5\n\n\nTo “see” the output of this object - you can either see it now in your Global Environment or type the object name in the Console to view it.\n\nten\n\n[1] 10\n\n\n\n\nIt is important to remember that R is an “object-oriented” programming language - everything in R is an object.\n\n\n\n\n\n\nTL;DR What is the Assign Operator &lt;-?\n\n\n\nThe “R” language is actually a deriviative of the original “S” language which stood for the “language of statistics” - it was written by statisiticans for statisticians (and now data scientists). The original S language was written in the mid-1970’s by programmers/statisticians at Bell Labs/AT&T.\nThe &lt;- actually comes from the physical key on their “APL” keyboards, for the APL programming language they were using at Bell Labs.\nA Nice Blog Post on the History of &lt;-\n\n\nBuilt in constants\nThere are several built in “constants” in R. Try typing these in at the Console to see the results.\n\npi\n\n[1] 3.141593\n\nletters\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n\n\nFor the constants like letters you get a list of the 26 lower case letters in the alphabet. Notice that the number in [square brackets] updates for each new line printed out. This allows you to keep track of the number of elements in the output object. letters is an “character” array (or vector) with 26 elements.\nTo confirm these details, we can use the class() function to determine that the letters object has all “character” elements. The length() function will let you know that there are 26 elements.\n\nclass(letters)\n\n[1] \"character\"\n\nlength(letters)\n\n[1] 26\n\n\n\nGetting help\nIf you would like to learn more about these built-in “constants”, you can get help in one of two ways. You can either type help(pi) in the “Console” (lower left) or type pi in the “Help” window (lower right).\n\nhelp(pi)\n\n\n\nTry out a built-in R function\nThe majority of the R programming language is driven by functions. Technically the + operator is actually a function that performs a sum.\nYou can even get help on these operators, by typing help(\"+\"). We have to add the quotes \"\" so that R knows we are looking for this operator and not trying to perform an addition operation inside the function call.\n\nhelp(\"+\")\n\n\nBut let’s try a function to create a sequence of numbers - for example, let’s use the seq() function to create a sequence of numbers from 1 to 10.\n\nseq(10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nAnd let’s look at the help page for the seq() function.\n\nR allows for what is called “lazy” coding. This basically means you can provide very minimal input and R will try to figure out what you want using the default settings for a given function. In the case of seq() the function begins by default at 1 and creates the output in steps of 1 up to the value of 10.\nWhile “lazy” coding practices are common with R, it would actually be better to explicitly define each argument to make sure you get the exact output you want. To do this, inside the parentheses () assign a value to each argument.\nNotice in the “Help” page for seq() shown above that the first 3 arguments are: from, to and by. Each of these can be defined inside the () by using the syntax of the name of the argument, equals sign = and the value (or object) you want to assign:\n\\[argument = value\\]\nFor example:\n\nseq(from = 1,\n    to = 10,\n    by = 1)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nYou could easily change these values to get a sequence from 0 to 1 in increments of 0.1 as follows:\n\nseq(from = 0,\n    to = 1,\n    by = 0.1)\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#create-your-first-r-script",
    "href": "module131_IntroRRStudio.html#create-your-first-r-script",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "3. Create your first R script",
    "text": "3. Create your first R script\nSave your code in a new script\nSo, as you can tell, the R Console is useful but slow and tedious. Let’s create an R script to save all of these commands in a file so that we can easily access everything we’ve done so far and re-run these commands as needed.\n\n\n\n\n\n\nGood coding practice\n\n\n\nIt is a good coding practice to create R code for every step in your data preparation and analysis so that:\n\nyou have a record of everything you’ve done and why\nother people on your team (including yourself in a few weeks) will know what you did and why\nyou can share your code with others so they will understand what you did and why (and to publish your code and data with your research articles - YES you can get a DOI citation to add to your CV for data and code as well as for the article)!!\n\n\n\nIn RStudio go to the top menu File/New File/R Script:\n\nOnce the R Script file is created, type in some of the commands we did above in the Console and put one command on each line.\nJust select each line and click “Run”.\n\nThen you can save the file on your computer as “myscript.R”, for example.\nYou can also select all of the rows and click run to execute all of the code in sequence and see the output in the “Console” Window.\n\nHere is the code and output:\n\n4 + 4\n\n[1] 8\n\nsqrt(25)\n\n[1] 5\n\npi\n\n[1] 3.141593\n\nseq(from=1, to=10, by=0.5)\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\n\n\nCreate R objects and Use Them\nLet’s try out some more built-in R functions, save the output in objects in your “Global Environment” and then use them in other functions and subsequent analysis steps.\nCreate a sequence of numbers and save them as an object called x. I also added a comment in the R code block below. Everything after the # hashtag is a comment which R will ignore. It is a good idea to add comments in your code to make sure that you and others understand what each part of your code does (including yourself in a few weeks when you’ve forgotten why you wrote that code step).\n\n# save sequence of numbers \n# from 1 to 10 in steps of 0.5\n# in an object named x\nx &lt;- seq(from=1, to=10, by=0.5)\n\n# Type x to view the contents\nx\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\n\n\nAlso take a look at the “Global Environment” to see the new object x.\n\n\n# use x to create new object y\ny &lt;- x*x\n\n\nOnce the object y is created, we can make a simple 2-dimensional scatterplot using the built-in plot() base R function.\n\n# plot x and y\nplot(x,y)\n\n\n\n\n\n\n\n\nThe plot is shown below, but if you are running this in the RStudio desktop, check the “Plots” window pane (lower right).\n\nOn your own\nDownload Rscript_01.R open it in your RStudio and run through the code. Try out new variations on your own.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#install-and-load-r-packages-understanding-your-r-session",
    "href": "module131_IntroRRStudio.html#install-and-load-r-packages-understanding-your-r-session",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "4. Install and load R packages (understanding your R session)\n",
    "text": "4. Install and load R packages (understanding your R session)\n\nStatus of your session with sessionInfo()\n\nWhile the base installation of R is pretty powerful on it’s own, the beauty of R and the R programming community is that there are literally hundreds of thousands if not millions of people programming in R and creating new functions everyday.\nIn order to use these new functions, the developers put them together in packages that we can install to extend the functionality of R.\nBut first, let’s take a look at the packages that are part of the basic installation of R. One way to see which packages are currently installed and loaded into your current R computing session, is by running the command sessionInfo().\nYou will also notice that the sessionInfo() command also lists the version of R I’m currently running (4.4.2), my operating system (Windows 11) and and my locale (USA, East Coast). These details can sometimes be helpful for collaborating with others who may be working with different system settings and for debugging errors.\nsessionInfo()\n\n7 Base R Packages\nThe basic installation of R includes 7 packages:\n\nstats\ngraphics\ngrDevices\nutils\ndatasets\nmethods\nbase\n\nTo learn more click on the “Packages” TAB in the lower right window pane to see the list of packages installed on your computer. I have a lot of packages on my computer, but here is a screenshot of the base R packages.\nSee the packages listed under “System Library” which are the ones that were installed with base R. You’ll notice that only some of these have checkmarks next to them. The checkmark means those are also loaded into your R session. Only some of them are loaded into memory by default to minimize the use of your computer’s memory.\n\nInstall a Package and Load it into R session memory\nLet’s install a “new” R package, like ggplot2.\nGo to the RStudio menu Tools/Install Packages\n\nThis will then open up a window where you can type in the name of the package you want. As soon as we start typing ggplot2 the menu begins listing all packages with that partial spelling…\n\nYou’ll notice that there are 3 parts to the installation:\n\nWhere you want to get the package from (i.e., which repository - more on repositories below).\nThe name of the package. You can actually type more than one package name at a time separated by commas if you want to install several packages at once.\nThe file location on your computer where the new package is installed - your file location may be different than mine. But this is useful to know in case something goes wrong. I would suggest keeping the default settings.\n\n\n\nWhere to get packages - CRAN\nUsing the Tools/Install Packages menu from within RStudio automatically links to CRAN, which is the “The Comprehensive R Archive Network”. You’ve already been here once to download and install the R programming language application.\n\nHowever, you can also click on “Packages” at the left to see the full list of packages currently available. As of right now (01/10/2025 at 5:12pm EST) there are 21,872 packages. This number increases every day as people create, validate and publish their packages on CRAN. You can get a list of all of the packages or if you have no idea what package you need, you can also look at the “Task Views” to see groupings of packages.\n\nHere is what the list of Packages looks like sorted by name:\n\nHowever, you can also browse Packages by “Task View”:\n\nFor example, suppose you are interested in survival analysis, here is a screenshot of the Survival Task View.\nAs you can see each Task View has a person(s) listed who help to maintain these collections. As you scroll through the webpage, you’ll see links to packages they recommend along with a description of what the packages do. For example, see the links below to the survival and rms packages.\n\nWhere to get packages - Bioconductor\nWhile the list of R packages on CRAN is impressive, if you plan to do analyses of biological data, there is a good chance you will need a package from Bioconductor.org.\nAs of right now (01/10/2025 at 6:45pm EST) there are 2289 packages. Similar to CRAN, Bioconductor requires each package to meet certain validation criteria and code testing requirements but these criteria are even more stringent on Bioconductor than on CRAN. You’ll notice that you can search for packages under the biocViews (left side column) or you can sort them alphabetically or search for individual packages in the section on the right side.\n\nThe one disadvantage of R packages from Bioconductor is that you cannot install them directly using the RStudio menu for Tools/Install Packages - you cannot “see” the Bioconductor repository from inside RStudio. Instead you’ll have to install these using R code.\nFor example, here is what you need to do to install the phyloseq package which “… provides a set of classes and tools to facilitate the import, storage, analysis, and graphical display of microbiome census data”.\nTo install phyloseq you need to (see the black box of code in the screenshot below):\n\nInstall BiocManager from CRAN - this package you can install from the RStudio menu for Tools/Install Packages - or you can run the code shown below for install.packages().\nThen go to the Console or open an R script and run:\n\ninstall.packages(\"BiocManager\")\n\nWhere to get packages - Github, friends, teammates, …\nIn addition to the CRAN and Bioconductor repositories, you can get packages from Github (and other cloud-based repositories), friends, teammates or write your own.\nTo get an idea of how many packages may be currently on Github, we can “search” for “R package” https://github.com/search?q=R+package&type=repositories and as you can see this is well over 118,000+ packages.\n\nWhile you can find packages on Github that have not (yet) been published on CRAN or Bioconductor, the developers of packages currently on CRAN and Bioconductor also often publish their development version (think of these as in “beta” and still undergoing testing) on Github. For example, the current published version of the data wrangling R package dplyr on CRAN was last updated on 11/17/2023.\n\nBut the development version of dplyr on Github was last updated 5 months ago in August 2024. There is probably a new version of dplyr coming soon for CRAN.\n\nSo, while the developers haven’t published this on CRAN, if you want to test out new functions and updates under development for this package, you can go to the R Console or write an R script to install the development version using these commands (see below) which is explained on the dplyr on Github website.\n# install.packages(\"pak\")\npak::pak(\"tidyverse/dplyr\")\n\nFinding and vetting R packages\nSo, as you have seen there are numerous ways to find R packages and there are hundreds of thousands of them out there. Your company or team may have their own custom R package tailored for your specific research areas and data analysis workflows.\nFinding R packages is similar to finding new questionnaires, surveys or instruments for your research. For example, if you want to measure someone’s depression levels, you should use a validated instrument like the Center for Epidemiological Studies-Depression (CESD) or the Beck Depression Index (BDI). These measurement instruments have both been well published and are well established for depression research.\nFinding R packages is similar - do your research! Make sure that the R package has been published and is well established to do the analysis you want. In terms of reliability, getting packages from CRAN or Bioconductor are the best followed by Github or other individuals. The best suggestion is look to see which R packages are being used by other people in your field.\n\n\n\n\n\n\nNo oversight company or agency\n\n\n\nWhile it may seem worrisome that there is no governing company or organization that verifies and validates and certifies all R packages, the good news is that the R community is a vast Global community. The development of R is not controlled by a limited number of people hired within some single company - instead there are literally millions of R programmers across the Globe testing and providing feedback on a 24/7 basis. If there is a problem with a package or function, there will be people posting about these issues - see Additional Resources.\nThis is the power of Open Source computing!!\n\n\nTo get an idea of how long a package has been in use and if it is still being actively supported and how it relates to other similar packages, check out this interactive Shiny app website for CRAN downloads. Type in the packages you want to compare and change the dates.\nHere is an example comparing arsenal, gtsummary, and tableone packages all of which are useful for making tables of summary statistics (aka, “Table 1”) - showing the number of downloads since the beginning of Jan 1, 2024.\nAs you can see the most downloaded is gtsummary followed by tableone with arsenal having the fewest downloads. This does NOT necessarily imply quality, but it does give you some insight into the popularity of these packages. I actually prefer the arsenal table package but tableone has been around longer and gtsummary is written by members of the RStudio/Posit development community and is more well known and popular.\n\nHere is an example of 2 specific packages I like. The rggobi package which was great for visualizing multiple dimensions of data simultaneously but which is no longer supported and the newer tourr package which was written by the same developer to replace the rggobi package. You can see that in the middle of 2020, the number of downloads for rggobi dropped almost to 0 and the tourr package downloads started to rise - this is about when they switched over from maintaining one package to supporting the newer one. rggobi on CRAN moved to archived status in July 2020, but tourr on CRAN was last updated in April 2024.\n\nSo, do your homework and check to see when the package was last updated, who maintains it and how good their documentation is for the package and what it does.\nLoad the new R package into your R session\nAfter you’ve decided what package you want and have installed it onto your computer, you must load it into memory for EVERY new R session for which you want those functions available.\nFor example, suppose I want to make a plot using the ggplot2 package. Before I can use the ggplot() function, I have to load that package into my computing session. Here is an example:\n\n# show current sessionInfo\nsessionInfo()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 22000)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.1.1     cli_3.6.3        \n [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.15.0 yaml_2.3.8       \n [9] rmarkdown_2.26    knitr_1.49        jsonlite_1.8.8    xfun_0.49        \n[13] digest_0.6.35     rlang_1.1.4       evaluate_0.23    \n\n# notice that ggplot2 is not listed\n# but let's try the ggplot() function with the\n# built-in pressure dataset\nggplot(pressure, aes(temperature, pressure)) +\n  geom_point()\n\nError in ggplot(pressure, aes(temperature, pressure)): could not find function \"ggplot\"\n\n\n\nThis will generate an error since these functions are not yet available in our session. So, use the library() function to LOAD the ggplot2 functions into current working memory.\n\n# load ggplot2 package\nlibrary(ggplot2)\n\n# look at sessionInfo again\nsessionInfo()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 22000)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.5.1\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.5       cli_3.6.3         knitr_1.49        rlang_1.1.4      \n [5] xfun_0.49         generics_0.1.3    jsonlite_1.8.8    glue_1.8.0       \n [9] colorspace_2.1-0  htmltools_0.5.8.1 scales_1.3.0      fansi_1.0.6      \n[13] rmarkdown_2.26    grid_4.4.2        evaluate_0.23     munsell_0.5.0    \n[17] tibble_3.2.1      fastmap_1.1.1     yaml_2.3.8        lifecycle_1.0.4  \n[21] compiler_4.4.2    dplyr_1.1.4       htmlwidgets_1.6.4 pkgconfig_2.0.3  \n[25] rstudioapi_0.15.0 digest_0.6.35     R6_2.5.1          tidyselect_1.2.1 \n[29] utf8_1.2.4        pillar_1.9.0      magrittr_2.0.3    withr_3.0.2      \n[33] tools_4.4.2       gtable_0.3.6     \n\n\n\nNotice that under other attached packages we can now see ggplot2_3.5.1 indicating that yes ggplot2 is installed and in memory and that version 3.5.1 is the version you are currently using.\nLet’s try the plot again.\n\n# try the plot again\nggplot(pressure, aes(temperature, pressure)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReload packages for every new R session\n\n\n\nEverything you close out your R/RStudio computing session (or restart your R session) you will need to load all of your package again. I know this seems like a HUGE pain, but there is a rationale for this.\n\nYou may not need the same packages for every new computing session - so R begins with the minimum loaded to save computing memory.\nThe GOOD NEWS is you do not have to re-install the packages - these are already saved on your computer. You only have to re-load them into memory using the library() function.\nThis workflow forces you to document (in your code) which packages you need for your computing sessions and why you are using them.\n\nBUT … If you do have a core set of packages that you would like to make sure get loaded into memory every time you start R/RStudio, see these helpful posts:\n\nhttps://www.datacamp.com/doc/r/customizing\nhttps://www.r-bloggers.com/2014/09/fun-with-rprofile-and-customizing-r-startup/",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#create-your-first-r-markdown-report-and-produce-output-files-in-different-formats-html-pdf-or-docx",
    "href": "module131_IntroRRStudio.html#create-your-first-r-markdown-report-and-produce-output-files-in-different-formats-html-pdf-or-docx",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "5. Create your first R Markdown report and produce output files in different formats (HTML, PDF, or DOCX)",
    "text": "5. Create your first R Markdown report and produce output files in different formats (HTML, PDF, or DOCX)\nCreate a new Rmarkdown File\nWe will do more in the later lesson 1.3.6: Putting reproducible research principles into practice, but let’s take a look at an Rmarkdown file and how we can use it to create a report that combines together data + code + documentation to produce a seamless report.\nGo to the RStudio menu and click File/New File/R Markdown:\n\nType in a title, your name, the date and choose the format you’d like to create. For your first document I encourage you to try HTML. But you can create WORD (DOC) documents and even PDFs. In addition to documents, you can also create slide deck presentations, Shiny apps and other custom products like R packages, websites, books, dashboards and many more.\n\n\n\n\n\n\nRmarkdown ideas and inspiration\n\n\n\n\nRmarkdown Gallery\nRmarkdown Formats\nRmarkdown Cookbook\n\n\n\nTo get started, use the built-in template:\n\nType in a title\nType in your name as author\nChoose and output document format\n\nHTML is always a good place to start - only need a browser to read the output *.html file.\n\nDOC usually works OK - but you need MS Word or Open Office installed on your computer.\n\nPDF NOTE: You need a TEX compiler on your computer - Learn about installing the tinytex https://yihui.org/tinytex/ R package to create PDFs.\n\n\n\n\nRmarkdown sections\nHere is the Example RMarkdown Template provided by RStudio to help you get started with your first Rmarkdown document.\n\nThis document consists of the following 3 key sections:\n\nYAML (yet another markup language) - this is essentially the metadata for your document and defines elements like the title, author, date and type of output document to be created (HTML in this example).\n\n\n\nR code blocks - the goal is to “interweave” code and documentation so these 2 elements live together. That way the analysis output and any associated tables or figures are updated automatically without having to cut-and-paste from other applications into your document - which is time consuming and prone to human errors.\n\nNotice that the code block starts and ends with 3 backticks ``` and includes the {r} Rlanguage designation inside the curly braces.\n\n\n\n\n\n\nRmarkdown\n\n\n\nRmarkdown can be used for many different programming languages including python, sas, and more, see rmarkdown - language-engines.\n\n\n\nAnd along with the R code blocks, we can also create our document with “marked up (or marked down)” text. Rmarkdown is a version of “markdown” which is a simplified set of tags that tell the computer how you want a piece of text formatted.\nFor example putting 2 asterisks ** before and after a word will make it bold, putting one _ underscore before and after a word will make the word italics; one or more hashtags # indicate a header at certain levels, e.g. 2 hashtags ## indicate a header level 2.\n\n\n\n\n\n\nRmarkdown Tutorial\n\n\n\nI encourage you to go through the step by step tutorial at https://rmarkdown.rstudio.com/lesson-1.html.\n\n\n\nHere are all 3 sections outlined.\n\nAt the top of the page you’ll notice a little blue button that says “knit” - this will “knit” (or combine) the output from the R code chunks and format the text as “marked up” and produce the HTML file:",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#references",
    "href": "module131_IntroRRStudio.html#references",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "References",
    "text": "References\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#other-helpful-resources",
    "href": "module131_IntroRRStudio.html#other-helpful-resources",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module133_DataVis.html",
    "href": "module133_DataVis.html",
    "title": "1.3.3: Data Visualization",
    "section": "",
    "text": "To visualize data using different R packages.\n\nKey points to cover: 1. Introduce to ggplot2 and other R packages. 2. Visualize one, two, or more variables at a time. 3. Introduce other resources (e.g., books, blogs, or websites) trainees can refer to.",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#session-objectives",
    "href": "module133_DataVis.html#session-objectives",
    "title": "1.3.3: Data Visualization",
    "section": "",
    "text": "To visualize data using different R packages.\n\nKey points to cover: 1. Introduce to ggplot2 and other R packages. 2. Visualize one, two, or more variables at a time. 3. Introduce other resources (e.g., books, blogs, or websites) trainees can refer to.",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#prework---before-you-begin",
    "href": "module133_DataVis.html#prework---before-you-begin",
    "title": "1.3.3: Data Visualization",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#base-r-graphical-functions",
    "href": "module133_DataVis.html#base-r-graphical-functions",
    "title": "1.3.3: Data Visualization",
    "section": "1. Base R graphical functions",
    "text": "1. Base R graphical functions",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#ggplot2-package",
    "href": "module133_DataVis.html#ggplot2-package",
    "title": "1.3.3: Data Visualization",
    "section": "2. ggplot2 package",
    "text": "2. ggplot2 package",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#get-boilerplate-code-to-start",
    "href": "module133_DataVis.html#get-boilerplate-code-to-start",
    "title": "1.3.3: Data Visualization",
    "section": "3. Get boilerplate code to start",
    "text": "3. Get boilerplate code to start\nR Gallery\nR Graphics Cookbook",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#references",
    "href": "module133_DataVis.html#references",
    "title": "1.3.3: Data Visualization",
    "section": "References",
    "text": "References\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#other-helpful-resources",
    "href": "module133_DataVis.html#other-helpful-resources",
    "title": "1.3.3: Data Visualization",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html",
    "href": "module135_StatisticalTests.html",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "",
    "text": "Develop linear and logistic regression models.\n(Use a survey sampling weight to generate more representative descriptive and inferential statistical values.) - Currently, this objective is under the Module 1.3.4: Missing data and sampling weight.\nInterpret a model output.\n\nkey points Key points to cover: 1. Run multivariate linear regression models with R. 2. Run multivariate logistic regression models with R. 3. Include interaction terms in regression models. 4. (R packages for complex survey data (e.g., survey package) a. R codes to generate weighted descriptive statistics and contingency tables, as well as to develop weighted linear models) 5. Interpret a model output. 6. (Compare the outputs of unweighted and weighted models.)",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#session-objectives",
    "href": "module135_StatisticalTests.html#session-objectives",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "",
    "text": "Develop linear and logistic regression models.\n(Use a survey sampling weight to generate more representative descriptive and inferential statistical values.) - Currently, this objective is under the Module 1.3.4: Missing data and sampling weight.\nInterpret a model output.\n\nkey points Key points to cover: 1. Run multivariate linear regression models with R. 2. Run multivariate logistic regression models with R. 3. Include interaction terms in regression models. 4. (R packages for complex survey data (e.g., survey package) a. R codes to generate weighted descriptive statistics and contingency tables, as well as to develop weighted linear models) 5. Interpret a model output. 6. (Compare the outputs of unweighted and weighted models.)",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#prework---before-you-begin",
    "href": "module135_StatisticalTests.html#prework---before-you-begin",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#develop-linear-and-logistic-regression-models.",
    "href": "module135_StatisticalTests.html#develop-linear-and-logistic-regression-models.",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "1. Develop linear and logistic regression models.",
    "text": "1. Develop linear and logistic regression models.",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#use-a-survey-sampling-weight-to-generate-more-representative-descriptive-and-inferential-statistical-values.",
    "href": "module135_StatisticalTests.html#use-a-survey-sampling-weight-to-generate-more-representative-descriptive-and-inferential-statistical-values.",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "2. (Use a survey sampling weight to generate more representative descriptive and inferential statistical values.)",
    "text": "2. (Use a survey sampling weight to generate more representative descriptive and inferential statistical values.)\nCurrently, this objective is under the Module 1.3.4: Missing data and sampling weight.",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#interpret-a-model-output.",
    "href": "module135_StatisticalTests.html#interpret-a-model-output.",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "3. Interpret a model output.",
    "text": "3. Interpret a model output.",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#references",
    "href": "module135_StatisticalTests.html#references",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "References",
    "text": "References\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#other-helpful-resources",
    "href": "module135_StatisticalTests.html#other-helpful-resources",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  }
]