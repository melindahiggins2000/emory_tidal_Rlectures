[
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "test",
    "section": "",
    "text": "text before table\n\nlibrary(gtsummary)\n\ngt_tbl &lt;- pressure %&gt;%\n  gtsummary::tbl_summary() %&gt;%\n  as_gt()\n\ntext after table\n\nknitr::is_html_output()\n\n[1] TRUE\n\nknitr::is_latex_output()\n\n[1] FALSE\n\n\n\ngt_tbl\n\n\n\n\n\nCharacteristic\n\nN = 191\n\n\n\n\ntemperature\n180 (80, 280)\n\n\npressure\n9 (0, 157)\n\n\n\n\n1 Median (Q1, Q3)\n\n\n\n\n\nsave for future\n\nknitr::include_graphics(\"myannimategif.gif\")\n\n\n\n\n\n\n\n\nknitr::include_graphics(\"myannimategif.png\")"
  },
  {
    "objectID": "module135_StatisticalTests.html",
    "href": "module135_StatisticalTests.html",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "",
    "text": "Develop linear and logistic regression models.\n(Use a survey sampling weight to generate more representative descriptive and inferential statistical values.) - Currently, this objective is under the Module 1.3.4: Missing data and sampling weight.\nInterpret a model output.",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#session-objectives",
    "href": "module135_StatisticalTests.html#session-objectives",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "",
    "text": "Develop linear and logistic regression models.\n(Use a survey sampling weight to generate more representative descriptive and inferential statistical values.) - Currently, this objective is under the Module 1.3.4: Missing data and sampling weight.\nInterpret a model output.",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#prework---before-you-begin",
    "href": "module135_StatisticalTests.html#prework---before-you-begin",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\nA. Install packages\nIf you do not have them already, install the following packages from CRAN (using the RStudio Menu “Tools/Install” Packages interface):\n\n\nVIM and VIM package website\n\n(Optional) skimr and skimr website\n\n(Optional) modelsummary and modelsummary website\n\n(Optional) summarytools and summarytools on Github\n\n\npalmerpenguins and palmerpenguins website\n\n\nggplot2 and ggplot2 website\n\n\nnaniar and naniar website\n\n\ndplyr and dplyr website\n\n\ngtsummary and gtsummary website\n\n\nHmisc and Hmisc website\n\n\nmice and mice website\n\nB. Review these online Book Chapters:\n\nBOOK: Flexible Imputation of Missing Data, 2nd ed., by Stef van Buuren (mice package author) - Chapter 1 “Introduction”, Sections 1.1-1.4\nBOOK: The Epidemiologist R Handbook - Chapter 20 “Missing Data”\nC. Open/create an RStudio project for this lesson\nLet’s start with the myfirstRproject RStudio project you created in Module 1.3.2 - part 1. If you have not yet created this myfirstRproject RStudio project, go ahead and create a new RStudio Project for this lesson. Feel free to name your project whatever you want, it does not need to be named myfirstRproject.",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#develop-linear-and-logistic-regression-models.",
    "href": "module135_StatisticalTests.html#develop-linear-and-logistic-regression-models.",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "1. Develop linear and logistic regression models.",
    "text": "1. Develop linear and logistic regression models.\nLinear Regression\nOne aaaaaaaaaaaaaaa\nLogistic Regression\nxxxxxxxxxxxxxxxxxx",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#linear-and-logistic-regression-models-with-complex-survey-sampling-weights",
    "href": "module135_StatisticalTests.html#linear-and-logistic-regression-models-with-complex-survey-sampling-weights",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "2. Linear and Logistic Regression Models with complex survey sampling weights",
    "text": "2. Linear and Logistic Regression Models with complex survey sampling weights\naaaaaaaaaaaaa",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#interpret-a-model-output",
    "href": "module135_StatisticalTests.html#interpret-a-model-output",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "3. Interpret a model output",
    "text": "3. Interpret a model output\naaaaaaaaaaaaa",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#r-code-for-this-module",
    "href": "module135_StatisticalTests.html#r-code-for-this-module",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "R Code For This Module",
    "text": "R Code For This Module\n\nmodule_135.R",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#references",
    "href": "module135_StatisticalTests.html#references",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "References",
    "text": "References\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#other-helpful-resources",
    "href": "module135_StatisticalTests.html#other-helpful-resources",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nMissing Data Resources\n\nCRAN Task View for Missing Data\nR-miss-tastic Website\nFlexible Imputation of Missing Data (online book for 2nd edition) by Stef van Buuren\nBlog post on Missing Data Visualization in R using ggplot2\nMissing data R tutorial\nCRAN Task View on Missing Data\nA resource website on missing values\nHandling missing values with R - tutorial\nBlog post “My favourite R package for: summarising data”\n\nand\nOther Helpful Resources",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module133_DataVis.html",
    "href": "module133_DataVis.html",
    "title": "1.3.3: Data Visualization",
    "section": "",
    "text": "To visualize data using different R packages.\n\nThis lesson module will include:\n\nIntroductions to ggplot2 and other relevant R packages for graphics.\nVisualizing one, two, and more variables at a time.\nSummary Tables with Graphics\nLists of other resources: books, blogs, websites, etc.",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#session-objectives",
    "href": "module133_DataVis.html#session-objectives",
    "title": "1.3.3: Data Visualization",
    "section": "",
    "text": "To visualize data using different R packages.\n\nThis lesson module will include:\n\nIntroductions to ggplot2 and other relevant R packages for graphics.\nVisualizing one, two, and more variables at a time.\nSummary Tables with Graphics\nLists of other resources: books, blogs, websites, etc.",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#prework---before-you-begin",
    "href": "module133_DataVis.html#prework---before-you-begin",
    "title": "1.3.3: Data Visualization",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\nA. Install packages\nIf you do not have them already, install the following packages from CRAN:\n\nggplot2\ndplyr\npatchwork\nggpubr\nGGally\nvcd\nOptional gapminder\n\nOptional gganimate\n\nOptional plotly\n\nOptional gt\n\nOptional gtExtras\n\nB. Open/create your RStudio project\nLet’s start with the myfirstRproject RStudio project you created in Module 1.3.2 - part 1. If you have not yet created this myfirstRproject RStudio project, go ahead and create a new RStudio Project for this lesson. Feel free to name your project whatever you want, it does not need to be named myfirstRproject.\nC. Create a new R script and load data into your computing session\nAt the end of Module 1.3.2 - part 6 you saved the mydata dataset in the mydata.RData R binary format.\n\nGo ahead and create a new R script (*.R) for this computing session. We did this already in Module 1.3.1 - part 3 - refer to this section to remember how to create a new R script.\nPut this code into your new R script (*.R) to load mydata.RData into your current computing session.\n\n\n# load mydata\nload(file = \"mydata.RData\")\n\n\n\n\n\n\n\nData must/should be in your RStudio project\n\n\n\nREMEMBER R/RStudio automatically looks in your current RStudio project folder for all files for your current computing session. So, make sure the mydata.RData file is in your current RStudio project myfirstRproject folder on your computer.\nFor a more detailed overview of RStudio projects:\n\nread “Chapter 6: R projects” in the The Epidemiologist R Handbook and\nrefer to “Chapter 45 Directory interactions” in the The Epidemiologist R Handbook.\n\n\n\nD. Get Inspired!\n\nGet Inspired at The R Graph Gallery\n\nAlso see the Top Curated R Graphs\n\nAlso see Additional Resources - R Graphics",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#base-r-graphical-functions",
    "href": "module133_DataVis.html#base-r-graphical-functions",
    "title": "1.3.3: Data Visualization",
    "section": "1. Base R graphical functions",
    "text": "1. Base R graphical functions\nThe base R graphics package is very powerful on its own. As you saw in 1.3.1: Introduction to R and R Studio, we can make a simple 2-dimensional scatterplot with the plot() function.\nBase R - Scatterplot\nFor example, let’s make a plot of Height on the X-axis (horizontal) and WeightPRE on the Y-axis (vertical) from the mydata dataset. Since we are using base R function, we have to use the $selector to identify the variables we want inside the mydata dataset.\nLearn more about the plot() function and arguments by running help(plot, package = \"graphics\").\n\nplot(x = mydata$Height,\n     y = mydata$WeightPRE)\n\n\n\n\n\n\n\nThe plot does look a little odd - this is due to some data errors in the mydata dataset. We will fix these below. But for now, you can “see” that these data may have some issues that need to be addressed. For example:\n\nThere are 2 people with heights &lt; 5 feet tall which may be suspect\nThere are 2 people with a weight &lt; 100 pounds which may be data entry errors or incorrect units\n\n\nFor now, let’s add some additional graphical elements:\n\na better label for the x-axis\na better label for the y-axis\na title for the graph\na subtitle for the graph\n\n\nplot(x = mydata$Height,\n     y = mydata$WeightPRE,\n     xlab = \"Height (in decimal inches)\",\n     ylab = \"Weight (in pounds) - before intervention\",\n     main = \"Weight by Height in the Mydata Project\",\n     sub = \"Hypothetical Madeup mydata Dataset\")\n\n\n\n\n\n\n\n\nAnd we could also add color and change the shapes - for example, let’s color and shape the points by GenderCoded, the numeric coding for gender where 1=Male, 2=Female.\nAnd we can add a legend inside the plot as well.\n\n\n\n\n\n\nPlot code inspiration\n\n\n\nI pulled this code together from code examples at:\n\nStackoverflow post on using pch\nSTHDA post on point shapes\nSTDHA post on base R legends\nR-Graph Galler post on base R legends\n\n\n\n\nplot(x = mydata$Height,\n     y = mydata$WeightPRE,\n     col = c(\"blue\", \"green\")[mydata$GenderCoded],\n     pch = c(15, 19)[mydata$GenderCoded],\n     xlab = \"Height (in decimal inches)\",\n     ylab = \"Weight (in pounds) - before intervention\",\n     main = \"Weight by Height in the Mydata Project\",\n     sub = \"Hypothetical Madeup mydata Dataset\")\nlegend(3, 250, legend=c(\"Male\", \"Female\"),\n       col=c(\"blue\", \"green\"), pch = c(15, 19), cex=0.8)\n\n\n\n\n\n\n\nThe STHDA website on “R Base Graphs” has a nice walk through of using the base R graphics package to make really nice plots.\n\nBase R - Histogram\nBasic Histogram\nAs we noted above, let’s take a look at the distribution of the heights in the mydata dataset. There is a specific hist() function in the graphics package for making histograms, learn more by running help(hist, package = \"graphics\").\nNotice that we can use some of the same arguments as we did above for plot().\n\nhist(mydata$Height,\n     xlab = \"Height (in decimal inches)\",\n     col = \"lightblue\",\n     border = \"black\",\n     main = \"Histogram of Heights\",\n     sub = \"Hypothetical Madeup mydata Dataset\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nColors available\n\n\n\nThere are 657 names of colors immediately available to you from the built-in grDevices Base R package which works in conjunction with graphics. You can view the names of all of these colors by running colors(). You can also learn more at:\n\nhttps://www.sthda.com/english/wiki/colors-in-r#google_vignette\nhttps://r-graph-gallery.com/42-colors-names.html\n\nhttps://r-graph-gallery.com/ggplot2-color.html - which explains how colors can be specified using the built-in color names, but can also be specified using RGB (red, green, blue) indexes or even Hexcodes for which there are many online tools like https://htmlcolorcodes.com/.\n\n\n\n\n# code not run here - do in your session\n# list built-in colors\ncolors()\n\n\nHistogram with Overlaid Density Curve\nStatisticians often like seeing a histogram (for the frequencies or probability of each value for the variable in the dataset) with an overlaid density curve (which is “smoothed” line for these probabilities). Statistical software like SAS and SPSS make this really easy. However, in R, we need to think through the process to get this to work.\n\nFirst, we need to make the histogram using probabilities for the “bars” in the histogram instead of frequency counts.\nSecond, we need to add a density line curve over the histogram “bars”.\n\nSee these online examples:\n\nhttps://r-charts.com/distribution/histogram-curves/\nhttps://www.datacamp.com/doc/r/histograms-and-density\nhttps://www.r-bloggers.com/2012/09/histogram-density-plot-combo-in-r/\n\n\n# make histogram as we did above\n# add freq = FALSE\nhist(mydata$Height,\n     freq = FALSE,\n     xlab = \"Height (in decimal inches)\",\n     col = \"lightblue\",\n     border = \"black\",\n     main = \"Histogram of Heights\",\n     sub = \"Hypothetical Madeup mydata Dataset\")\n\n# add density curve line\n# add na.rm=TRUE to remove \n# the missing values in Height\nlines(density(mydata$Height, na.rm=TRUE),\n      col = \"black\")\n\n\n\n\n\n\n\n\nFix the Heights\nSo as you can see in the histogram and in the scatterplot figures above for the Height variable, there are 2 people with heights under 4 feet tall.\n\n# use dplyr::arrange()\nlibrary(dplyr)\n\nmydata %&gt;%\n  select(SubjectID, Height) %&gt;%\n  arrange(Height) %&gt;%\n  head()\n\n# A tibble: 6 × 2\n  SubjectID Height\n      &lt;dbl&gt;  &lt;dbl&gt;\n1        28    2.6\n2         8    3.3\n3         9    5.1\n4         6    5.2\n5         2    5.4\n6        12    5.5\n\n\nLet’s look at these values:\n\n\nSubjectID number 28 has a Height of 2.6 feet tall\n\nIf this wasn’t a made-up dataset, we could ask the original data collectors to see if there is a way to check this value in their records or possibly to re-measure this individual.\nFor now, let’s assume this was a simple typo where the 2 numbers were transposed where this individual should be 6.2 feet tall.\n\n\n\nSubjectID number 8 has a Height of 3.3 feet tall\n\nUnfortunately, this is probably not a simple typo. Without further details, we should maybe set this to missing as an invalidated data point.\nAs a side-note, I actually ran into this problem in a study where one of the participants was a paraplegic. So, this could be a legitimate height. But when computing BMI, adjustments need to be made or alternative body metrics are needed.\nFor now, we will set this to missing, NA_real_ which is missing for “real” numeric variables.\n\n\n\n\n# make a copy of the dataset\nmydata_corrected &lt;- mydata\n\n# compute a new corrected height\n# fix heights for these 2 IDs\nmydata_corrected &lt;- \n  mydata_corrected %&gt;%\n  mutate(Height_corrected = case_when(\n    (SubjectID == 28) ~ 6.2,\n    (SubjectID == 8) ~ NA_real_,\n    .default = Height\n  ))\n\n\nRemake the histogram with the corrected heights.\n\n# make histogram as we did above\n# add freq = FALSE\nhist(mydata_corrected$Height_corrected,\n     freq = FALSE,\n     xlab = \"Height (in decimal inches)\",\n     col = \"lightblue\",\n     border = \"black\",\n     main = \"Histogram of Heights\",\n     sub = \"Hypothetical Madeup mydata Dataset\")\n\n# add density curve line\n# add na.rm=TRUE to remove \n# the missing values in Height\nlines(density(mydata_corrected$Height_corrected, na.rm=TRUE),\n      col = \"black\")\n\n\n\n\n\n\n\n\nBase R - Barchart\nLet’s make a bar chart for the frequencies for the 3 SES categories:\n\nfill the bars with a yellow color specified by the HEX code #f7f445\n\nset the border color as darkgreen and make the border line thicker by updating the lwd, see Stack Overflow Post on bar width.\n\n\n# get table of frequencies for each category\ntab1 &lt;- table(mydata_corrected$SES.f)\n\nopar &lt;- par() # save current plotting parameters\npar(lwd = 3) # change border linewidth\n\n# make plot of the frequencies for \n# each category\nbarplot(tab1,\n        xlab = \"SES Categories\",\n        ylab = \"Frequencies\",\n        col = \"#f7f445\",\n        border = \"darkgreen\",\n        main = \"Socio Economic Status Categories\",\n        sub = \"Hypothetical Madeup mydata Dataset\")\n\n\n\n\n\n\npar(opar) # reset plotting parameters to defaults\n\n\nBase R - Boxplot\nMake side-by-side boxplots of the heights by gender.\n\nboxplot(Height_corrected ~ GenderCoded.f,\n        data = mydata_corrected,\n        xlab = \"Gender\",\n        ylab = \"Height (in decimal feet)\",\n        col = \"#f58ef1\",\n        border = \"darkmagenta\",\n        main = \"Height by Gender\",\n        sub = \"Hypothetical Madeup mydata Dataset\")",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#the-ggplot2-package",
    "href": "module133_DataVis.html#the-ggplot2-package",
    "title": "1.3.3: Data Visualization",
    "section": "2. The ggplot2 package",
    "text": "2. The ggplot2 package\nThe ggplot2 package name starts with gg which stands for the “grammar of graphics” which is explained in the “ggplot2: Elegant Graphics for Data Analysis (3e)” Book.\n\n\n\n\n\n\nWhy is the package ggplot2 and not ggplot?\n\n\n\nMany people often ask Hadley Wickham (the developer of ggplot2) what happened to the first ggplot? Technically, there was a ggplot package and you can still view the ggplot archived package versions on CRAN which date back to 2006 with the last version posted in 2008. However, in 2007, Hadley redesigned the package and published the first version of ggplot2 (version 0.5.1) was posted on CRAN. So, ggplot2 is the package that has stayed in production and actively maintained for nearly 20 years!!\n\n\nGiven that ggplot2 has been actively maintained for nearly 20 years, it has become almost the defacto graphical standard for R graphics. If you take a look at the list of packages on CRAN that start with the letter “G”, as of this morning 01/28/2025 at 8:23 am EST, USA, there are 230 packages that start with gg - nearly all of these are compatible packages that extend the functionality or work in concert with the ggplot2 package. There are also currently 14 packages on the Bioconductor repository that start with gg.\nLet’s make plots similar to the ones above but now using ggplot2. When making a ggplot2 plot, we build the plots using layers that get added to the previous layers.\n\n\nggplot2 - Scatterplot\nHere are the steps to building a scatterplot.\n\nFirst, load the ggplot2 package, designate the dataset and variables (aesthetics) to be included. This creates a plot space with nothing in it - we will add data in the next steps below.\n\n\n#load ggplot2\nlibrary(ggplot2)\n\n# create the plot space\nggplot(data = mydata_corrected,\n       aes(x = WeightPRE,\n           y = WeightPOST))\n\n\n\n\n\n\n\n\n\nNext add + a “geometric object” or “geom” to show the data as points.\n\n\nggplot(data = mydata_corrected,\n       aes(x = WeightPRE,\n           y = WeightPOST)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nWe can add color by GenderCoded.f\n\n\n\n\n\n\n\n\nAutomatic Legend\n\n\n\nNotice that by adding color = GenderCoded.f inside the aes() aesthetic that a legend for the coloring of the points is automatically added to the plot. This can be disabled if you wish. Learn more about colors and legends in the ggplot2 book - Chapter 11.\n\n\n\nggplot(data = mydata_corrected,\n       aes(x = WeightPRE,\n           y = WeightPOST,\n           color = GenderCoded.f)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nWe can also add labels, a title and better legend title\n\n\nggplot(data = mydata_corrected,\n       aes(x = WeightPRE,\n           y = WeightPOST,\n           color = GenderCoded.f)) +\n  geom_point() +\n  xlab(\"Weight (in pounds) before program\") +\n  ylab(\"Weight (in pounds) after program\") +\n  labs(\n    title = \"Weights (in pounds) before and after\",\n    subtitle = \"Hypothetical Madeup mydata Dataset\",\n    color = \"Gender\"\n  ) \n\n\n\n\n\n\n\nNotice that there are 4 weights that seem off. Also notice that the values are within a reasonable range when considering PRE or POST separately, but when you put them together in a scatterplot you can see that the values are off since we expect PRE and POST weights to be somewhat similar.\n\nTwo individuals have PRE weights that are &lt; 100 pounds (bottom left side of plot).\n\nThere is a good chance that these weights may have been accidentally recorded as kg (kilograms) instead of in pounds.\n\n\nAnd there are 2 individuals with POST weights around 100-120 lbs, but for whom their PRE weights were 225-260 lbs.\n\nThere is a good chance that these two data points may have had a typo in the first number (e.g. a weight of 110 should be 210).\n\n\nFor this made-up dataset, it also appears that all 4 of these odd data points are Males. It is a good idea to explore other “correlates” that may help identify underlying data collection issues.\n\nLet’s correct these values.\n\n# for WeightPRE &lt; 100, convert kg to lbs\nmydata_corrected &lt;- mydata_corrected %&gt;%\n  mutate(WeightPRE_corrected = case_when(\n    (WeightPRE &lt; 100) ~ WeightPRE * 2.20462,\n    .default = WeightPRE\n  ))\n\n\n# For WeightPOST, for\n# SubjectID 28, change WeightPOST=98 to 198\n# since this person's WeightPRE was 230.\n# also fix SubjectID = 32, for\n# WeightPOST from 109 to 209 since\n# their WeightPRE was 260\n\nmydata_corrected &lt;- mydata_corrected %&gt;%\n  mutate(WeightPOST_corrected = case_when(\n    (SubjectID == 28) ~ 198,\n    (SubjectID == 32) ~ 209,\n    .default = WeightPOST\n  ))\n\n\nLet’s redo the plot with these corrected values - now the PRE and POST weights look similar.\nI’ve also added a “reference line” (in “red” color) to the plot below. By adding the line “Y = X” we can also visualize which points are above or below the line for people who gained or lost weight from PRE-to-POST, respectively. It looks like most people lost weight - the majority of the points are below the line where PRE &gt; POST weights.\nI also:\n\napplied colors to each gender category,\napplied shapes to each gender category,\nchanged the size of the points,\nassigned custom colors for each gender category,\n\nthe colors are for the non-missing values\nif you want to see the person missing a gender, we have to specifically assign a color for NA using na.value=\n\n\n\nassigned custom shapes for each gender category,\n\nthe colors are for the non-missing values\nif you want to see the person missing a gender, we have to specifically assign a color for NA using na.value=\n\n\n\nalso notice that I had to provide a custom label in the labs() for the shape and color legend - the labels are the same for color and shape so they will be in the same legend box. It is possible to assign the variables for color and shape to different variables.\n\n\n\nggplot(data = mydata_corrected,\n       aes(x = WeightPRE_corrected,\n           y = WeightPOST_corrected,\n           color = GenderCoded.f,\n           shape = GenderCoded.f)) +\n  geom_point(size = 2) +\n  geom_abline(slope = 1, \n              intercept = 0,\n              color = \"red\") +\n  scale_shape_manual(values = c(16, 17),\n                     na.value = 15) +\n  scale_color_manual(values = c(\"blue\", \n                                \"magenta\"),\n                     na.value = \"grey30\") +\n  xlab(\"Weight (in pounds) before program\") +\n  ylab(\"Weight (in pounds) after program\") +\n  labs(\n    title = \"Weights (in pounds) before and after\",\n    subtitle = \"Hypothetical Madeup mydata Dataset\",\n    color = \"Gender\",\n    shape = \"Gender\"\n  ) \n\n\n\n\n\n\n\n\n\nggplot2 - Histogram\nLet’s make a histogram of Age and overlay a density curve like we did above for the heights, but this time using the ggplot2 package functions.\nThe first step:\n\nspecify the dataset mydata_corrected and “aesthetics” variable x=Age inside the ggplot() step\nthen add the geometric object geom_histogram()\n\n\n\nggplot(data = mydata_corrected, \n       aes(x = Age)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nLet’s add some color using fill= for the inside colors of the bars and color= for the border color for the bars.\n\nggplot(mydata_corrected, \n       aes(x = Age)) +\n  geom_histogram(fill = \"lightblue\",\n                 color = \"black\")\n\n\n\n\n\n\n\n\nTo add the density curve, we need to do 2 things:\n\nAdd an aesthetic aes() to change from counts (or frequencies) for the bars to probabilities. We can do this using the after_stat() function.\n\nLearn more by running help(aes_eval, package = \"ggplot2\").\n\n\nAnd then we can add the geom_density() geometric object and add color= for the overlaid line color.\n\nAnd I also added some better labels to the axes, title and subtitle.\n\nggplot(mydata_corrected, \n       aes(x = Age,\n           y = after_stat(density))) +\n  geom_histogram(fill = \"lightblue\",\n                 color = \"black\") +\n  geom_density(color = \"red\") +\n  xlab(\"Age (in years)\") +\n  ylab(\"Proportion\") +\n  labs(\n    title = \"Ages for Participants\",\n    subtitle = \"Hypothetical Madeup mydata Dataset\"\n  ) \n\n\n\n\n\n\n\n\n\nggplot2 - Boxplot (and variations)\nLet’s look at the corrected PRE weights by SES.\n\nggplot(data = mydata_corrected,\n       aes(x = SES.f,\n           y = WeightPRE_corrected)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThere is one person missing SES. So, let’s filter the dataset and remake the plot. Instead of creating another “new” dataset, we can use the dplyr pipe %&gt;% into our plotting workflow as follows to filter out the missing SES before we make the plot. Notice I can drop the data = in the ggplot() step.\nIn the filter() step below, I used the ! exclamation point to indicate that we want to keep all rows for which SES.f is NOT missing, by using !is.na().\n\nlibrary(dplyr)\n\nmydata_corrected %&gt;%\n  filter(!is.na(SES.f)) %&gt;%\n  ggplot(aes(x = SES.f, \n             y = WeightPRE_corrected)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLet’s add a fill color for the SES categories. Notice that a legend is automatically added to the plot for the SES colors.\n\nmydata_corrected %&gt;%\n  filter(!is.na(SES.f)) %&gt;%\n  ggplot(aes(x = SES.f, \n             y = WeightPRE_corrected,\n             fill = SES.f)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nAnd add better axis labels plus a title and subtitle.\n\nmydata_corrected %&gt;%\n  filter(!is.na(SES.f)) %&gt;%\n  ggplot(aes(x = SES.f, \n             y = WeightPRE_corrected,\n             fill = SES.f)) +\n  geom_boxplot() +\n  xlab(\"Socio-Economic Status Categories\") +\n  ylab(\"Weight (in pounds) before program\") +\n  labs(\n    title = \"Weights by SES Categories\",\n    subtitle = \"Hypothetical Madeup mydata Dataset\"\n  ) \n\n\n\n\n\n\n\n\nAdd Another Layer with Points\nWe can also add points on top of the boxplots using geom_jitter() AFTER using geom_boxplot. If you switch the order of these “geom’s” you can specify whether the boxplot is on top of the points or if the points are on top of the boxplots (like we did here).\nIn geom_jitter(), I also added height=0 and width=.10 to adjust the amount of jitter in the vertical and horizontal directions.\n\nmydata_corrected %&gt;%\n  filter(!is.na(SES.f)) %&gt;%\n  ggplot(aes(x = SES.f, \n             y = WeightPRE_corrected,\n             fill = SES.f)) +\n  geom_boxplot() +\n  geom_jitter(height=0, \n              width=.10) +\n  xlab(\"Socio-Economic Status Categories\") +\n  ylab(\"Weight (in pounds) before program\") +\n  labs(\n    title = \"Weights by SES Categories\",\n    subtitle = \"Hypothetical Madeup mydata Dataset\",\n    fill = \"SES Categories\"\n  ) \n\n\n\n\n\n\n\nA couple more packages to look at the distribution of data points by groups are:\n\n\nbeeswarm\n\nR Graph Gallery Example of beeswarm\nbeeswarm on CRAN\n\n\nggbeeswarm on CRAN\n\n\nTry Another Geom\nOne of the cool things about ggplot2 is the ability to easily swap out geom’s. Let’s try a violin plot which provides a better idea of the shape of the underlying distributions that you don’t get with a simple boxplot. Change geom_boxplot() to geom_violin(). I also added the bw argument to change the “bandwidth” for how much smoothing is done. Try changing this number and see what happens. Learn more by running help(geom_violin, package = \"ggplot2\")\n\nmydata_corrected %&gt;%\n  filter(!is.na(SES.f)) %&gt;%\n  ggplot(aes(x = SES.f, \n             y = WeightPRE_corrected,\n             fill = SES.f)) +\n  geom_violin(bw=10) +\n  xlab(\"Socio-Economic Status Categories\") +\n  ylab(\"Weight (in pounds) before program\") +\n  labs(\n    title = \"Weights by SES Categories\",\n    subtitle = \"Hypothetical Madeup mydata Dataset\"\n  ) \n\n\n\n\n\n\n\n\n\nggplot2 - Barchart\nLet’s make a simple barchart for SES.f after filtering out the NAs.\n\nmydata_corrected %&gt;%\n  filter(!is.na(SES.f)) %&gt;%\n  ggplot(aes(x = SES.f)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nLet’s also make a clustered barplot of SES.f by GenderCoded.f. Let’s also filter out the NAs from GenderCoded.f as well.\nTo add the 2nd grouping or clustering variable, we add fill= to the aesthetics and then add position = \"dodge\" for geom_bar() to see the colors side by side instead of stacked.\n\nmydata_corrected %&gt;%\n  filter(!is.na(SES.f)) %&gt;%\n  filter(!is.na(GenderCoded.f)) %&gt;%\n  ggplot(aes(x = SES.f,\n             fill = GenderCoded.f)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\nLet’s also add custom colors and better labels. A few notes:\n\n\nfill controls the interior filled color for the bars\n\ncolor controls the border color of the bars\n\nscale_fill_manual() is where you add custom colors\n\n\nmydata_corrected %&gt;%\n  filter(!is.na(SES.f)) %&gt;%\n  filter(!is.na(GenderCoded.f)) %&gt;%\n  ggplot(aes(x = SES.f,\n             fill = GenderCoded.f)) +\n  geom_bar(position = \"dodge\", \n           color = \"black\") +\n  scale_fill_manual(values = c(\"blue\", \n                               \"magenta\")) +\n  xlab(\"Socio-Economic Status Categories\") +\n  ylab(\"Frequency\") +\n  labs(\n    title = \"Frequencies of SES Categories by Gender\",\n    subtitle = \"Hypothetical Madeup mydata Dataset\",\n    fill = \"Gender\"\n  )\n\n\n\n\n\n\n\n\n\nggplot2 - Errorbar plots\nBarplot with Error Bars\nSuppose instead of boxplots for looking at the differences in heights by gender, let’s make a plot of the mean heights by gender with error bars added to reflect the 95% confidence intervals for these group means.\nFirst let’s compute the means using the mean() function and we’ll pull the 95% confidence interval limits from the t.test() output for a one-sample t-test (see details below).\nSave these outputs into another small dataset called dt.\nLet’s take a look at the t.test() output.\nWe can run a one-sample t-test to test whether the mean of the Heights is different from 0. This will result in a p-value for this hypothesis test. The t.test() output can also be used to evaluate to see whether or not the 95% confidence interval contains 0 or not.\n\\[H_0: \\mu_{height} = 0\\] versus\n\\[H_a: \\mu_{height} \\neq 0\\]\n\n# run one-sample t-test and save the output\n# into an object called tt1\ntt1 &lt;- t.test(mydata_corrected$Height_corrected, \n              conf.level = 0.95)\n\nIf we print the tt1 object to the console, we get the abbreviated t-test results which gives us the t-test statistic, p-value, the mean of the heights and the 95% confidence interval for that mean (which does not include 0).\n\ntt1\n\n\n    One Sample t-test\n\ndata:  mydata_corrected$Height_corrected\nt = 61.664, df = 18, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 5.658315 6.057475\nsample estimates:\nmean of x \n 5.857895 \n\n\nBut the tt1 t-test output object actually has a bunch of details stored inside it. Let’s look at the structure of the tt1 t-test object:\n\nstr(tt1)\n\nList of 10\n $ statistic  : Named num 61.7\n  ..- attr(*, \"names\")= chr \"t\"\n $ parameter  : Named num 18\n  ..- attr(*, \"names\")= chr \"df\"\n $ p.value    : num 2.13e-22\n $ conf.int   : num [1:2] 5.66 6.06\n  ..- attr(*, \"conf.level\")= num 0.95\n $ estimate   : Named num 5.86\n  ..- attr(*, \"names\")= chr \"mean of x\"\n $ null.value : Named num 0\n  ..- attr(*, \"names\")= chr \"mean\"\n $ stderr     : num 0.095\n $ alternative: chr \"two.sided\"\n $ method     : chr \"One Sample t-test\"\n $ data.name  : chr \"mydata_corrected$Height_corrected\"\n - attr(*, \"class\")= chr \"htest\"\n\n\nWe can select elements of this t-test object just like we select variables out of a dataset using the $ dollar sign selector. Let’s take a look at the conf.int part of the t-test object for the 95% confidence interval limits.\n\ntt1$conf.int\n\n[1] 5.658315 6.057475\nattr(,\"conf.level\")\n[1] 0.95\n\n\nWe can further pull out each limit separately using the [] square brackets to pull specifically the first element [1] for the lower limit of the 95% confidence interval and the second element [2] for the upper limit of the 95% confidence interval.\n\ntt1$conf.int[1]\n\n[1] 5.658315\n\ntt1$conf.int[2]\n\n[1] 6.057475\n\n\nSo, we will save these components from the t-test output inside the mutate step in the code below to make sure we capture the lower confidence interval lci and upper confidence interval uci for each gender category which we will later use for making our error bars.\n\n# capture the means of the correct heights \n# and get the 95% confidence intervals\n# upper bound and lower bound by gender\n# filter out the missing GenderCoded.f\ndt &lt;- mydata_corrected %&gt;%\n  filter(!is.na(GenderCoded.f)) %&gt;%\n  dplyr::group_by(GenderCoded.f)%&gt;%\n  dplyr::summarise(\n    mean = mean(Height_corrected, na.rm = TRUE),\n    lci = t.test(Height_corrected, \n                 conf.level = 0.95)$conf.int[1],\n    uci = t.test(Height_corrected, \n                 conf.level = 0.95)$conf.int[2])\ndt\n\n# A tibble: 2 × 4\n  GenderCoded.f  mean   lci   uci\n  &lt;fct&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Male           6.1   5.86  6.34\n2 Female         5.59  5.30  5.88\n\n\n\nUse this small dataset dt with the means and 95% confidence intervals limits to make this plot.\n\nggplot(data = dt) +\n  geom_bar(aes(x = GenderCoded.f, \n               y = mean, \n               fill = GenderCoded.f), \n           color = \"black\",\n           stat=\"identity\") +\n  scale_fill_manual(values = c(\"blue\", \n                               \"magenta\")) + \n  geom_errorbar(aes(x = GenderCoded.f, \n                    ymin = lci, \n                    ymax = uci), \n                width = 0.4, \n                color =\"black\", \n                size = 1) +\n  xlab(\"Gender\") +\n  ylab(\"Mean Height (in decimal feet)\") +\n  labs(\n    title = \"Average Heights by Gender\",\n    subtitle = \"Hypothetical Madeup mydata Dataset\",\n    caption = \"Error Bars Represent 95% Confidence Intervals\",\n    fill = \"Gender\"\n  )\n\n\n\n\n\n\n\n\nLineplot with Points and Error Bars\nWe can also remove the bars and just create a line plot connecting the points for the means with the error bars shown. I set size=1.5to make the lines a little thicker in the plot.\n\nggplot(data = dt) +\n  geom_point(aes(x = GenderCoded.f, \n                 y = mean,\n                 color = GenderCoded.f),\n             size = 3) + \n  geom_errorbar(aes(x = GenderCoded.f, \n                    ymin = lci, \n                    ymax = uci,\n                    color = GenderCoded.f), \n                width = 0.4, \n                size = 1.5) +\n  geom_line(aes(x = GenderCoded.f, \n                y = mean), \n            group = 1,\n            size = 1.5,\n            color = \"black\") +\n  scale_color_manual(values = c(\"blue\", \n                               \"magenta\")) + \n  xlab(\"Gender\") +\n  ylab(\"Mean Height (in decimal feet)\") +\n  labs(\n    title = \"Average Heights by Gender\",\n    subtitle = \"Hypothetical Madeup mydata Dataset\",\n    caption = \"Error Bars Represent 95% Confidence Intervals\",\n    color = \"Gender\"\n  )\n\n\n\n\n\n\n\n\n\nggplot2 - Lollipop plots\nAnother plot that can be useful to help visualize changes between two time points,like PRE-to-POST changes is using a “lollipop” plot which utilizes the geom_segment() to create line segments with points (the lollipops) on each end.\nThe plot below was inspired by the code example at https://r-graph-gallery.com/303-lollipop-plot-with-2-values.html.\nLet’s look at the corrected Weights PRE to POST- sorted by their starting PRE weights.\n\n# sort data by WeightPRE_corrected ascending\ndata &lt;- mydata_corrected %&gt;%\n  rowwise() %&gt;%\n  arrange(WeightPRE_corrected) %&gt;%\n  mutate(SubjectID = factor(SubjectID, SubjectID))\n\n# Plot\nggplot(data) +\n  geom_segment(aes(x = SubjectID,\n                   xend = SubjectID,\n                   y = WeightPRE_corrected,\n                   yend = WeightPOST_corrected), \n    color = \"grey30\") +\n  geom_point(aes(x = SubjectID, \n                 y = WeightPRE_corrected,\n                 color = \"WeightPRE_corrected\"),\n             size = 3) +\n  geom_point(aes(x = SubjectID, \n                 y = WeightPOST_corrected,\n                 color = \"WeightPOST_corrected\"),\n             size = 3) +\n  scale_color_manual(\n    labels = c(\"PRE\", \"POST\"),\n    values = c(\"coral\",\"darkblue\"),\n    guide  = guide_legend(), \n    name   = \"Group\") +\n  coord_flip() +\n  theme(legend.position = \"bottom\") +\n  xlab(\"Subject IDs\") +\n  ylab(\"Weight Change (in pounds) PRE to POST\")",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#other-graphics-packages-to-know",
    "href": "module133_DataVis.html#other-graphics-packages-to-know",
    "title": "1.3.3: Data Visualization",
    "section": "3. Other Graphics Packages to Know",
    "text": "3. Other Graphics Packages to Know\nSave plot objects and reuse/rearrange them\nOnce a “chunk” of ggplot code is run, technically a ggplot2 plot object is created. We can save and reuse these objects to create composite figures.\nFor example, let’s create the scatterplot, histogram and clustered barplot and save each into three separate plot objects p1, p2, and p3.\n\n# make the scatterplot, save as p1\n\np1 &lt;- ggplot(\n  data = mydata_corrected,\n  aes(\n    x = WeightPRE_corrected,\n    y = WeightPOST_corrected,\n    color = GenderCoded.f,\n    shape = GenderCoded.f\n  )) +\n  geom_point(size = 2) +\n  geom_abline(slope = 1,\n              intercept = 0,\n              color = \"red\") +\n  scale_shape_manual(values = c(16, 17), na.value = 15) +\n  scale_color_manual(values = c(\"blue\", \"magenta\"),\n                     na.value = \"grey30\") +\n  xlab(\"Weight (in pounds) before program\") +\n  ylab(\"Weight (in pounds) after program\") +\n  labs(\n    title = \"Weights (in pounds) before and after\",\n    subtitle = \"Hypothetical Madeup mydata Dataset\",\n    color = \"Gender\",\n    shape = \"Gender\"\n  )\n\n\n# make the histogram, save as p2\n\np2 &lt;- ggplot(mydata_corrected, \n             aes(x = Age, \n                 y = after_stat(density))) +\n  geom_histogram(fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  xlab(\"Age (in years)\") +\n  ylab(\"Proportion\") +\n  labs(title = \"Ages for Participants\", \n       subtitle = \"Hypothetical Madeup mydata Dataset\")\n\n\n# make the barplot, save as p3\n\np3 &lt;- mydata_corrected %&gt;%\n  filter(!is.na(SES.f)) %&gt;%\n  filter(!is.na(GenderCoded.f)) %&gt;%\n  ggplot(aes(x = SES.f,\n             fill = GenderCoded.f)) +\n  geom_bar(position = \"dodge\", \n           color = \"black\") +\n  scale_fill_manual(values = c(\"blue\", \n                               \"magenta\")) +\n  xlab(\"Socio-Economic Status Categories\") +\n  ylab(\"Frequency\") +\n  labs(\n    title = \"Frequencies of SES Categories by Gender\",\n    subtitle = \"Hypothetical Madeup mydata Dataset\",\n    fill = \"Gender\"\n  )\n\n\npatchwork package\nAfter making and saving each of the ggplot2 plot objects above, we can arrange them into a new composite plot. The patchwork package is really good for making these composite figures.\nLearn more at https://patchwork.data-imaginist.com/articles/patchwork.html\n\n# load patchwork package\nlibrary(patchwork)\n\n# put p1 and p2 side-by-side\n# and put both of these on top of p3\n(p1 + p2) / p3\n\n\n\n\n\n\n\n\nggpubr package and ggarrange() function\nAnother package that is useful for arrange plot objects into composite plots is the ggpubr package with the ggarrange() function.\n\n# load ggpubr package\nlibrary(ggpubr)\n\n# use ggarrange twice\n# put p1 and p2 side by side\n# then put on top of p3\nggarrange(\n  ggarrange(p1, p2, widths = c(1, 1)),\n  p3, nrow = 2, ncol = 1)\n\n\n\n\n\n\n\n\n\nGGally package and ggpairs() function\nIf you would like to make a scatterplot matrix to look at the associations (correlations) between multiple numeric variables at the same time, the GGally::ggpairs() function is useful.\nIn the plot below, we can see the 2-dimensional scatterplots between the heights and weights at PRE and POST. The plot also provides the Pearson’s correlations for all of the pairwise associations between each combination of 2 variables.\nI also added a “best fit” linear line by adding lower = list(continuous = \"smooth\").\n\nlibrary(GGally)\n\nggpairs(mydata_corrected,\n        columns = c(\"Height_corrected\", \n                    \"WeightPRE_corrected\", \n                    \"WeightPOST_corrected\"),\n        lower = list(continuous = \"smooth\"))\n\n\n\n\n\n\n\n\nWhat is really cool about this plotting function is the easy way to add in a 3rd variable like gender to see if these correlations (and scatterplots) change by gender (i.e. does gender moderate the associations?). Notice that we get separate lines for each gender and we get the correlations for each gender as well.\nIf you look at the correlations by gender between Height_corrected and WeightPOST_corrected the correlation for Males was 0.692 and for Females was 0.913, so it does look like the correlation is stronger for the Females than the Males. For this made-up dataset, this doesn’t matter. But this approach is a good way to start exploring your data for moderating effects.\n\nggpairs(mydata_corrected,\n        mapping = aes(color = GenderCoded.f),\n        columns = c(\"Height_corrected\", \n                    \"WeightPRE_corrected\", \n                    \"WeightPOST_corrected\"),\n        lower = list(continuous = \"smooth\"))\n\n\n\n\n\n\n\n\nAnd if we add GenderCoded.f to the list of variable columns to be included, we now also get bar charts for the frequencies of each gender, histograms of each variable for each gender, boxplots for each variable by each gender, along with the density curves by gender and the correlation matrix.\n\nggpairs(mydata_corrected,\n        mapping = aes(color = GenderCoded.f),\n        columns = c(\"GenderCoded.f\",\n                    \"Height_corrected\", \n                    \"WeightPRE_corrected\", \n                    \"WeightPOST_corrected\"),\n        lower = list(continuous = \"smooth\"))\n\n\n\n\n\n\n\n\nVisualize Categorical Data with vcd package\nThe vcd package for “visualizing categorical data” has been around for over 20 years!! It is a helpful package for visualizing multiple categorical variables at once.\nLet’s visualize the relative proportions of gender and SES using the vcd::mosaic() function.\n\nlibrary(vcd)\n\nvcd::mosaic(GenderCoded.f ~ SES.f, \n            data = mydata_corrected,\n            gp = gpar(fill = c(\"gray\",\"dark magenta\")),\n            main = \"Gender and SES\",\n            )\n\n\n\n\n\n\n\n\nExample of an animated graph with gganimate\n\nLearn how to make an animated figure with the gganimate package at https://gganimate.com/. The animation demo shown below is of the relationship between Life Expectancy and GDP (gross domestic product) per capita in 142 countries over 12 years from 1952 to 2007.\nThe animated figure is viewable in the HTML website at https://melindahiggins2000.github.io/emory_tidal_Rlectures/module133_DataVis.html. In the PDF file only a static view is shown for the single year “1976” from the gapminder dataset and R package.\n\n\n\n\n\n\n\n\n\nInteractive Graphics with plotly\n\nAnother cool package is the plotly graphics package which allows for active interaction with the plot. This works in the RStudio environment (viewing in the “Viewer” window) and from within an HTML formatted document rendered from Rmarkdown.\nLearn more at https://plotly-r.com/.\nHere is an interactive version (on this website for the HTML version) of the side-by-side boxplots below - with a horizontal orientation. This plot will NOT be interactive in the PDF document but the figure will be shown.\n\nlibrary(plotly)\nfig &lt;- plot_ly(mydata_corrected, \n               x = ~WeightPRE_corrected, \n               color = ~SES.f, \n               type = \"box\",\n               orientation = \"h\")\nfig",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#summary-tables-with-graphics",
    "href": "module133_DataVis.html#summary-tables-with-graphics",
    "title": "1.3.3: Data Visualization",
    "section": "4. Summary Tables with Graphics",
    "text": "4. Summary Tables with Graphics\nThere are a number of other packages that allow you to insert small graphs or figures inside of a table.\nThe R Graph Gallery has a nice summary of table packages with these features. Some of these packages include:\n\ngtExtras\nhuxtable\nflextable\nskimr\nmodelsummary\nrhandsontable\n\nAs an example, let’s look at the code at https://r-graph-gallery.com/368-plotting-in-cells-with-gtextras.html for making a table of summary statistics with a plot overview including a list of categories and percentage of missing data for that variable.\n\nlibrary(gtExtras)\n\nmydata_corrected %&gt;%\n  select(Height_corrected,\n         WeightPRE_corrected,\n         WeightPOST_corrected,\n         GenderCoded.f,\n         SES.f) %&gt;%\n  gt_plt_summary()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\n\n\n21 rows x 5 cols\n\n\n\nColumn\nPlot Overview\nMissing\nMean\nMedian\nSD\n\n\n\n\n\nHeight_corrected\n5.16.5\n9.5%\n5.9\n5.8\n0.4\n\n\n\nWeightPRE_corrected\n110260\n4.8%\n192.9\n190.0\n42.3\n\n\n\nWeightPOST_corrected\n108240\n4.8%\n182.2\n190.0\n35.9\n\n\n\n\n\nGenderCoded.f\n\nMale and Female\n\n\n2 categories\n9.5%\n—\n—\n—\n\n\n\n\n\nSES.f\n\naverage income, low income and high income\n\n\n3 categories\n9.5%\n—\n—\n—",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#other-places-to-get-help-and-get-started",
    "href": "module133_DataVis.html#other-places-to-get-help-and-get-started",
    "title": "1.3.3: Data Visualization",
    "section": "5. Other Places to Get Help and Get Started",
    "text": "5. Other Places to Get Help and Get Started\n\nSee the summary of graphics resources at Additional Resources - R Graphics",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#r-code-for-this-module",
    "href": "module133_DataVis.html#r-code-for-this-module",
    "title": "1.3.3: Data Visualization",
    "section": "R Code For This Module",
    "text": "R Code For This Module\n\nmodule_133.R",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#references",
    "href": "module133_DataVis.html#references",
    "title": "1.3.3: Data Visualization",
    "section": "References",
    "text": "References\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, JooYoung Seo, Ken Brevoort, and Olivier Roy. 2024. Gt: Easily Create Presentation-Ready Display Tables. https://gt.rstudio.com.\n\n\nKassambara, Alboukadel. 2023. Ggpubr: Ggplot2 Based Publication Ready Plots. https://rpkgs.datanovia.com/ggpubr/.\n\n\nMeyer, David, Achim Zeileis, and Kurt Hornik. 2006. “The Strucplot Framework: Visualizing Multi-Way Contingency Tables with Vcd.” Journal of Statistical Software 17 (3): 1–48. https://doi.org/10.18637/jss.v017.i03.\n\n\nMeyer, David, Achim Zeileis, Kurt Hornik, and Michael Friendly. 2023. Vcd: Visualizing Categorical Data. https://doi.org/10.32614/CRAN.package.vcd.\n\n\nMock, Thomas. 2024. gtExtras: Extending Gt for Beautiful HTML Tables. https://github.com/jthomasmock/gtExtras.\n\n\nPedersen, Thomas Lin. 2024. Patchwork: The Composer of Plots. https://patchwork.data-imaginist.com.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to Ggplot2. https://ggobi.github.io/ggally/.\n\n\nSievert, Carson. 2020. Interactive Web-Based Data Visualization with r, Plotly, and Shiny. Chapman; Hall/CRC. https://plotly-r.com.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, and Pedro Despouy. 2024. Plotly: Create Interactive Web Graphics via Plotly.js. https://plotly-r.com.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nZeileis, Achim, David Meyer, and Kurt Hornik. 2007. “Residual-Based Shadings for Visualizing (Conditional) Independence.” Journal of Computational and Graphical Statistics 16 (3): 507–25. https://doi.org/10.1198/106186007X237856.",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#other-helpful-resources",
    "href": "module133_DataVis.html#other-helpful-resources",
    "title": "1.3.3: Data Visualization",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html",
    "href": "module131_IntroRRStudio.html",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "",
    "text": "Get acquainted with R and R Studio\nWrite simple R code in Console\nCreate your first R script\nInstall and load R packages (understanding your R session)\nCreate your first R Markdown report and produce output files in different formats (HTML, PDF, or DOCX)",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#session-objectives",
    "href": "module131_IntroRRStudio.html#session-objectives",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "",
    "text": "Get acquainted with R and R Studio\nWrite simple R code in Console\nCreate your first R script\nInstall and load R packages (understanding your R session)\nCreate your first R Markdown report and produce output files in different formats (HTML, PDF, or DOCX)",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#prework---before-you-begin",
    "href": "module131_IntroRRStudio.html#prework---before-you-begin",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\n\n\n\n\n\n\nR versus RStudio\n\n\n\nNote: R is the name of the programming language itself and RStudio is an integrated development environment (IDE) which is an enhanced interface for better organization, files management and analysis workflows.\n\n\nSoftware and Applications to Download\n\nFIRST, Download and install R onto your computer from https://cran.r-project.org/.\nNEXT, After installing R, download and install RStudio Desktop onto your computer from https://posit.co/download/rstudio-desktop/.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#get-aquainted-with-r-and-r-studio",
    "href": "module131_IntroRRStudio.html#get-aquainted-with-r-and-r-studio",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "1. Get aquainted with R and R Studio",
    "text": "1. Get aquainted with R and R Studio\nBasic R\nWhen you download R from CRAN and install it on your computer, there is an R application that you can run. However, it is very bare bones. Here is a screenshot of what it looks like on my computer (Windows 11 operating system).\n\nYou can type commands in the console window at the prompt “&gt;” but this is slow and tedious. You can also write and execute scripts from inside this application and see the output back in the console window as well as creating plots. But managing large projects using this interface is not efficient.\n\n\nRStudio IDE\n\n\n\n\n\n\nRStudio Desktop Software vs Posit the company\n\n\n\nRStudio was founded in 2009 https://posit.co/about/ when it published the “free and open source” RStudio software. But over time, the RStudio application has expanded beyond just being used for the R programming language. You can now use RStudio for writing and managing projects with Python code, Markdown, LaTeX, Cascading Style Sheets and more.\nSo, in 2022, RStudio the company became Posit https://posit.co/blog/rstudio-is-becoming-posit/ to encompass the broader computing community.\n\n\nThe RStudio Integrated Development Environment (IDE) application provides much better tools for managing files within a given “project”. This biggest advantage of working in an IDE is everything is contained and managed within a given project, which is linked to a specific folder (container) on your computer (or cloud drive you may have access to).\nHowever, you will still need to write and execute code using scripts and related files. An IDE is NOT a GUI (graphical user interface) which is the “point and click” workflow you may have experience with if you’ve used other analysis software applications such as SPSS, SAS Studio, Excel and similar.\n\nThe interface is usually arranged with the following 4 “window panes”:\n\nConsole\nSource\nEnvironment\nFiles\n\n\nThe typical arrangement, usually has the “Console” window pane at the bottom left. This window also usually has TABs for the “Terminal” and any “Background Jobs” that might be running.\n\n\nThe “Source” window pane is usually at the top left. This is where you will do most of your editing of your R program scripts (*.R) or Rmarkdown files (*.Rmd). This is also where the data viewer window will open. You can also open and edit other kinds of files here as well (*.tex, *.css, *.txt, and more).\n\n\nThe top right window pane should always have your “Environment”, “History” and “Tutorial” TABs but may also have TABs for “Build” and “Git” and others depending on your project type and options selected.\n\n\nThe bottom right window pane has TABs for your:\n\n“Files” directory\n“Plots” window for graphical output\n“Packages” - which lists all add-on R packages installed on your computer\n“Help” window\nas well as other TABs for “Viewer” and “Presentation” for viewing other kinds of output.\n\n\n\nCustomizing your RStudio interface\nYou also have the option to rearrange your window panes as well as change the look and feel of your programming interface and much more. To explore all of your options, click on the menu at the top for “Tools/Global Options”:\n\n\nTake a look at the left side for the list of all of the options. Some of the most useful options to be aware of are:\n\nGeneral\nAppearance, and\nPane Layout\n\n\nIn the “General” TAB is where you can see and confirm that R is installed and where the R programming language executable is installed on your computer.\n\n\nYou will probably want to explore fine-tuning these parameters to customize the appearance of your RStudio preferences. For example, you can change the ZOOM level to improve readability. You may also want to change the FONT sizes for the Editor and Help windows as needed.\n\n\n\n\n\n\nZOOM + FONT\n\n\n\nWhen making changes to your RStudio interface appearance, be aware that ZOOM and FONT size settings work together, so you may need to play around with the settings that work best for your monitor or device you are using.\n\n\nI also encourage you to try out different “Editor Themes” which will change the colors of the R code as well as background colors (light or dark).\n\n\nThe default “Editor Theme” is textmate.\n\n\nBut here is an example of changing the theme to “Tomorrow Night Blue”.\n\n\nI would also suggest NOT changing the layout of the window panes until you are very familiar with the default settings. But in “Pane Layout” is where you can see what the default layout settings are and what other options are available to you.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#write-simple-r-code-in-console",
    "href": "module131_IntroRRStudio.html#write-simple-r-code-in-console",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "2. Write simple R code in Console",
    "text": "2. Write simple R code in Console\nSimple math\nSo, let’s start with some simple R code using the Console window and typing commands at the &gt; prompt (which is the greater than symbol).\nYou can write simple math expressions like 5 + 5.\n\n5 + 5\n\n[1] 10\n\n\n\nNotice that the output shows the number 1 enclosed in square brackets [] followed by the answer (or output) of 10.\nThis is because R performed the addition operation using the + operator and then “saved” the output in temporary memory as a scalar object with 1 element, which is the number 10.\nYou can actually see this temporary object by typing .Last.value - which is only temporary and will be overwritten after the execution of the next R command.\n.Last.value\n[1] 10\n\nHowever, if we look at our current computing environment (see “Global Environment” upper right window pane), it is still showing as empty.\n\nThis is because we have not yet “saved” the output into an object that we created. Let’s save the output from 5 + 5 into an object called ten.\nTo do this we need to do 2 things:\n\nCreate the object called ten by\nUsing the “assign” operator &lt;- to take the result of 5 + 5 and move it (save it or pipe it) into the object ten.\n\n\nten &lt;- 5 + 5\n\n\n\n\n\n\n\n\n\nTL;DR What is the Assign Operator &lt;-?\n\n\n\nThe “R” language is actually a derivative of the original “S” language which stood for the “language of statistics” - it was written by statisticians for statisticians (and now data scientists). The original S language was written in the mid-1970’s by programmers/statisticians at Bell Labs/AT&T.\nThe &lt;- actually comes from the physical key on their “APL” keyboards, for the APL programming language they were using at Bell Labs.\nA Nice Blog Post on the History of &lt;-\n\n\nTo “see” the output of this object called ten - you can either see it now in your Global Environment or type the object name in the Console to view it.\n\nten\n\n[1] 10\n\n\n\n\nIt is important to remember that R is an “object-oriented” programming language - everything in R is an object.\n\nBuilt in constants\nThere are several built in “constants” in R. Try typing these in at the Console to see the results.\n\npi\n\n[1] 3.141593\n\nletters\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n\n\n\n\n\n\n\n\nR is Case Sensitive!\n\n\n\nA pro and con of the R language is that it is case sensitive. Lower case x and uppercase X are different objects. As seen above, the lowercase letters object is a vector of the 26 lowercase letters, whereas LETTERS is a different object vector of the 26 uppercase letters. Be on the lookout for case sensitive spelling and formatting of object, package and function names in R.\n\n\nFor the constants like letters you get a list of the 26 lower case letters in the alphabet. Notice that the number in [square brackets] updates for each new line printed out. This allows you to keep track of the number of elements in the output object. letters is an “character” array (or vector) with 26 elements.\nTo confirm these details, we can use the class() function to determine that the letters object has all “character” elements. The length() function will let you know that there are 26 elements.\n\nclass(letters)\n\n[1] \"character\"\n\nlength(letters)\n\n[1] 26\n\n\n\n\nGetting help\nIf you would like to learn more about these built-in “constants”, you can get help in one of two ways. You can either type help(pi) in the “Console” (lower left) or type pi in the “Help” window (lower right).\n\nhelp(pi)\n\n\n\nThe help() function defaults to searching for a built-in object, function or dataset by default in the base R package. But some functions may exist in multiple packages, so it is always a good idea to add the package when running the help() function if possible.\nSince pi is in the base R package, it would be better to run:\n\nhelp(pi, package = \"base\")\n\nIf you have no idea what package a function may be in, you can use the ?? search operator. For example, many packages include a plotting related function. If you want to see how many R packages currently installed on your computer have a plot related function, type the following:\n\n??plot\n\n\nTry out a built-in R function\nThe majority of the R programming language is driven by functions. Technically the + operator is actually a function that performs a sum.\nYou can even get help on these operators, by typing help(\"+\"). We have to add the quotes \"\" so that R knows we are looking for this operator and not trying to perform an addition operation inside the function call.\n\nhelp(\"+\")\n\n\nBut let’s try a function to create a sequence of numbers - for example, let’s use the seq() function to create a sequence of numbers from 1 to 10.\n\nseq(10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nAnd let’s look at the help page for the seq() function.\n\nR allows for what is called “lazy” coding. This basically means you can provide very minimal input and R will try to figure out what you want using the default settings for a given function. In the case of seq() the function begins by default at 1 and creates the output in steps of 1 up to the value of 10.\nWhile “lazy” coding practices are common with R, it would actually be better to explicitly define each argument to make sure you get the exact output you want. To do this, inside the parentheses () we should assign a value to each argument.\nNotice in the “Help” page for seq() shown above that the first 3 arguments are: from, to and by. Each of these can be defined inside the () by using the syntax of the name of the argument, an equals sign =, and then the value (or object) you want to assign:\n\\[argument = value\\]\nFor example, the explicit function call should be:\n\nseq(from = 1,\n    to = 10,\n    by = 1)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nYou could easily change these values to get a sequence from 0 to 5 in increments of 0.1 as follows:\n\nseq(from = 0,\n    to = 5,\n    by = 0.1)\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8\n[20] 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7\n[39] 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5.0\n\n\n\nNotice the incremental counter [#] on the left to help you keep track of how many elements are in the resulting numeric vector that was the “result” or “output” from the seq() function.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#create-your-first-r-script",
    "href": "module131_IntroRRStudio.html#create-your-first-r-script",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "3. Create your first R script",
    "text": "3. Create your first R script\nSave your code in a new script\nSo, as you can tell, the R Console is useful but slow and tedious. Let’s create an R script to save all of these commands in a file so that we can easily access everything we’ve done so far and re-run these commands as needed.\n\n\n\n\n\n\nGood Reproducible Research Coding Practice\n\n\n\nIt is a good coding practice to create R code (saved in *.R script files or *.Rmd Rmarkdown files) for every step in your data preparation and analysis so that:\n\nyou have a record of everything you’ve done and why\nother people on your team (including yourself in a few weeks) will know what you did and why\nyou can share your code with others so they will understand what you did and why (and to publish your code and data with your research articles - YES you can get a DOI citation to add to your CV for data and code as well as for the article)!\n\n\n\nIn RStudio go to the top menu “File/New File/R Script”:\n\nOnce the R Script file is created, type in some of the commands we did above in the Console and put one command on each line.\nJust select each line and click “Run”.\n\nThen you can save the file on your computer as “myscript.R”, for example.\nYou can also select all of the rows and click run to execute all of the code in sequence and see the output in the “Console” Window.\n\nHere is the code and output:\n\n4 + 4\n\n[1] 8\n\nsqrt(25)\n\n[1] 5\n\npi\n\n[1] 3.141593\n\nseq(from=1, to=10, by=0.5)\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\n\n\n\nCreate R objects and Use Them\nLet’s try out some more built-in R functions, save the output in objects in your “Global Environment” and then use them in other functions and subsequent analysis steps.\nCreate a sequence of numbers and save them as an object called x. I also added a comment in the R code block below. Everything after the # hashtag is a comment which R will ignore. It is a good idea to add comments in your code to make sure that you and others understand what each part of your code does (including yourself in a few weeks when you’ve forgotten why you wrote that code step).\n\n# save sequence of numbers \n# from 1 to 10 in steps of 0.5\n# in an object named x\nx &lt;- seq(from=1, to=10, by=0.5)\n\n# Type x to view the contents\nx\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\n\n\nAlso take a look at the “Global Environment” to see the new object x.\n\n\n# use x to create new object y\ny &lt;- x*x\n\n\n\nOnce the object y is created, we can make a simple 2-dimensional scatterplot using the built-in plot() base R function.\n\n# plot x and y\nplot(x,y)\n\n\n\n\n\n\n\n\nThe plot is shown above, but if you are running this code interactively in the RStudio desktop, check the “Plots” window pane (lower right).\n\nOn your own\nDownload Rscript_01.R (right click the linked file and “Save As” the file on your computer), then open it in your RStudio and run through the code. Try out new variations on your own.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#install-and-load-r-packages-understanding-your-r-session",
    "href": "module131_IntroRRStudio.html#install-and-load-r-packages-understanding-your-r-session",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "4. Install and load R packages (understanding your R session)\n",
    "text": "4. Install and load R packages (understanding your R session)\n\nStatus of your current computing R session with sessionInfo()\n\nWhile the base installation of R is pretty powerful on it’s own, the beauty of R and the R programming community is that there are literally hundreds of thousands if not millions of people programming in R and creating new functions everyday.\nIn order to use these new functions, the developers put them together in packages that we can install to extend the functionality of R.\nBut first, let’s take a look at the packages that are part of the basic installation of R. One way to see which packages are currently installed and loaded into your current R computing session, is by running the command sessionInfo().\n\n\n\n\n\n\nWatch spelling - R is case sensitive!\n\n\n\nNotice: This function name is all lowercase except for the capital “I” in the middle. Be sure you are typing sessionInfo() and not sessioninfo().\n\n\nYou will also notice that the sessionInfo() command also lists the version of R I’m currently running (4.4.2), my operating system (Windows 11) and and my locale (USA, East Coast). These details can sometimes be helpful for collaborating with others who may be working with different system settings and for debugging errors.\n\nsessionInfo()\n\n\n7 Base R Packages\nThe basic installation of R includes 7 packages:\n\nstats\ngraphics\ngrDevices\nutils\ndatasets\nmethods\nbase\n\nTo learn more click on the “Packages” TAB in the lower right window pane to see the list of packages installed on your computer. I have a lot of packages on my computer, but here is a screenshot of the base R packages.\nSee the packages listed under “System Library” which are the ones that were installed with base R. You’ll notice that only some of these have checkmarks next to them. The checkmark means those are also loaded into your R session. Only some of them are loaded into memory by default to minimize the use of your computer’s memory.\n\n\nInstall a Package and Load it into R session memory\nLet’s install a “new” R package, like ggplot2.\nGo to the RStudio menu “Tools/Install” Packages\n\n\nThis will then open up a window where you can type in the name of the package you want. As soon as we start typing ggplot2 the menu begins listing all packages with that partial spelling…\n\n\nYou’ll notice that there are 3 parts to the installation:\n\nWhere you want to get the package from (i.e., which repository - more on repositories below).\nThe name of the package. You can actually type more than one package name at a time separated by commas if you want to install several packages at once.\nThe file location on your computer where the new package is installed - your file location may be different than mine. But this is useful to know in case something goes wrong. I would suggest keeping the default settings.\n\n\n\n\nWhere to get packages - CRAN Repository\nUsing the “Tools/Install” Packages menu from within RStudio automatically links to CRAN, which is the “The Comprehensive R Archive Network”. You’ve already been here once to download and install the R programming language application.\nHere is a screenshot of the CRAN homepage.\n\n\nNext click on “Packages” at the left to see the full list of packages currently available. As of right now (01/10/2025 at 5:12pm EST) there are 21,872 packages. This number increases every day as people create, validate and publish their packages on CRAN. You can get a list of all of the packages or if you have no idea what package you need, you can also look at the “Task Views” to see groupings of packages.\n\n\nHere is what the list of Packages looks like sorted by name:\n\n\nHowever, you can also browse Packages by “Task View”:\n\n\nFor example, suppose you are interested in survival analysis, here is a screenshot of the Survival Task View.\nAs you can see each Task View has a person(s) listed who help to maintain these collections. As you scroll through the webpage, you’ll see links to packages they recommend along with a description of what the packages do. For example, see the links below to the survival and rms packages.\n\n\nWhere to get packages - Bioconductor Repository\nWhile the list of R packages on CRAN is impressive, if you plan to do analyses of biological data, there is a good chance you will need a package from Bioconductor.org.\nAs of right now (01/10/2025 at 6:45pm EST) there are 2289 packages on Bioconductor. Similar to CRAN, Bioconductor requires each package to meet certain validation criteria and code testing requirements but these criteria are even more stringent on Bioconductor than on CRAN. You’ll notice that you can search for packages under the biocViews (left side column) or you can sort them alphabetically or search for individual packages in the section on the right side.\n\nThe one disadvantage of R packages from Bioconductor is that you cannot install them directly using the RStudio menu for “Tools/Install” Packages because you cannot “see” the Bioconductor repository from inside RStudio. Instead you have to install Bioconductor packages using R code.\n\nFor example, here is what you need to do to install the phyloseq package which “… provides a set of classes and tools to facilitate the import, storage, analysis, and graphical display of microbiome census data”.\nTo install phyloseq you need to (see the black box of code in the screenshot shown below):\n\nInstall BiocManager from CRAN - this package you can install from the RStudio menu for “Tools/Install Packages” - or you can run the code shown below for install.packages().\n\n\ninstall.packages(\"BiocManager\")\n\n\nThen go to the Console or open an R script and run:\n\n\nBiocManager::install(\"phyloseq\")\n\n\n\nWhere to get packages - Github, friends, teammates, …\nIn addition to the CRAN and Bioconductor repositories, you can also get packages from Github (and other cloud-based repositories), friends, teammates or write your own.\nTo get an idea of how many packages may be currently on Github, we can “search” for “R package” at https://github.com/search?q=R+package&type=repositories and as you can see this is well over 118,000+ packages.\n\n\nWhile you can find packages on Github that have not (yet) been published on CRAN or Bioconductor, the developers of packages currently on CRAN and Bioconductor also often publish their development version (think of these as in “beta” and still undergoing testing) on Github.\nFor example, the current published version of the data wrangling R package dplyr on CRAN was last updated on 11/17/2023 (over a year ago).\n\n\nHowever, the development version of dplyr on Github was last updated 5 months ago in August 2024. So, there is probably a new version of dplyr coming soon for CRAN.\n\nWhile the developers haven’t published this “Github” version of dplyr yet on CRAN, if you want to test out new dplyr functions and updates under development, you can go to the R Console or write an R script to install the development version using these commands (see below) which is explained on the dplyr on Github website.\n\n# install.packages(\"pak\")\npak::pak(\"tidyverse/dplyr\")\n\n\nFinding and vetting R packages\nSo, as you have seen there are numerous ways to find R packages and there are hundreds of thousands of them out there. Your company or team may also have their own custom R package tailored for your specific research areas and data analysis workflows.\nFinding R packages is similar to finding new questionnaires, surveys or instruments for your research. For example, if you want to measure someone’s depression levels, you should use a validated instrument like the Center for Epidemiological Studies-Depression (CESD) or the Beck Depression Index (BDI). These measurement instruments have both been well published and are well established for depression research.\nFinding R packages is similar - do your research! Make sure that the R package has been published and is well established to do the analysis you want. In terms of reliability, getting packages from CRAN or Bioconductor are the best followed by Github or other individuals. The best suggestion is look to see which R packages are being used by other people in your field.\n\n\n\n\n\n\nNo oversight company or agency\n\n\n\nWhile it may seem worrisome that there is no governing company or organization that verifies and validates and certifies all R packages, the good news is that the R community is a vast Global community. The development of R is not controlled by a limited number of people hired within a single company - instead there are literally millions of R programmers across the Globe testing and providing feedback on a 24/7 basis. If there is a problem with a package or function, there will be people posting about these issues - see Additional Resources.\nThis is the power of Open Source computing!!\n\n\n\nPopularity of R Packages\nTo get an idea of how long a package has been in use and if it is still being actively supported and how it relates to other similar packages, check out this interactive Shiny app website for package downloads from CRAN https://hadley.shinyapps.io/cran-downloads/. Type in the packages you want (separated by commas) to compare and put in the date range of interest.\nHere is an example comparing the arsenal, gtsummary, and tableone packages all of which are useful for making tables of summary statistics (aka, “Table 1”) - showing the number of downloads since the beginning of Jan 1, 2024.\nAs you can see the most downloaded of these 3 packages is gtsummary followed by tableone and then arsenal having the fewest downloads. This does NOT necessarily imply quality, but it does give you some insight into the popularity of these packages. I actually prefer the arsenal table package but tableone has been around longer and gtsummary is written by members of the RStudio/Posit development community and is more well known and popular. All 3 of these packages can be found in use in current research literature.\nYou will see examples of all 3 of these table-making packages in Module lesson 1.3.2\n\n\nHere is an example of two specific packages I like. The rggobi package which was great for visualizing multiple dimensions of data simultaneously but which is no longer supported. But there is now a newer tourr package which was written by the same developers to replace the rggobi package. You can see that in the middle of 2020, the number of downloads for rggobi dropped almost to 0 and the tourr package downloads started to rise - this is about when rggobi was archived on CRAN and they switched over to maintaining the newer tourr package.\n\n\nrggobi on CRAN moved to archived status in July 2020, but\n\ntourr on CRAN was last updated in April 2024.\n\n\nIn summary:\n\ndo your homework,\ncheck to see when the package was last updated,\nresearch who maintains it and\nreview how good their documentation is for the package and what it does, and\nsee if the package has been used by others in your research area.\n\n\nLoad the new R package into your R session\nAfter you’ve decided what package you want and have installed it onto your computer, you must load it into memory for EVERY new R session for which you want those functions available.\n\n\n\n\n\n\nPackages - install once, (re)-load every R session\n\n\n\nUnless you upgrade R or change computers, you only need to install a given R package once. But you do need to (re)-load the package into your current R session every time you (re)-start R (or RStudio).\n\n\nFor example, suppose I want to make a plot using the ggplot2 package. Before I can use the ggplot() function, I have to load that package into my computing session.\nHere is my current R session status BEFORE I load the ggplot2 package.\n\n# show current sessionInfo\nsessionInfo()\n\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 22000)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.1    fastmap_1.1.1     cli_3.6.3        \n [5] tools_4.5.1       htmltools_0.5.8.1 rstudioapi_0.15.0 yaml_2.3.8       \n [9] rmarkdown_2.26    knitr_1.50        jsonlite_1.8.8    xfun_0.52        \n[13] digest_0.6.35     rlang_1.1.4       evaluate_0.23    \n\n\nSince I have not yet loaded the ggplot2 package into the session, I will get an error.\n\n# I have not yet loaded ggplot2 into the session\n# try the ggplot() function with the\n# built-in pressure dataset to see error\nggplot(pressure, aes(temperature, pressure)) +\n  geom_point()\n\nError in ggplot(pressure, aes(temperature, pressure)): could not find function \"ggplot\"\n\n\n\nThe code above generates an error since these functions are not yet available in our session.\n\nTo fix this error, we need to use the library() function to load the ggplot2 functions into current working memory.\n\n# load ggplot2 package\nlibrary(ggplot2)\n\n# look at sessionInfo again\nsessionInfo()\n\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 22000)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.5.1\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.5       cli_3.6.3         knitr_1.50        rlang_1.1.4      \n [5] xfun_0.52         generics_0.1.3    jsonlite_1.8.8    glue_1.8.0       \n [9] colorspace_2.1-0  htmltools_0.5.8.1 scales_1.3.0      fansi_1.0.6      \n[13] rmarkdown_2.26    grid_4.5.1        evaluate_0.23     munsell_0.5.0    \n[17] tibble_3.2.1      fastmap_1.1.1     yaml_2.3.8        lifecycle_1.0.4  \n[21] compiler_4.5.1    dplyr_1.1.4       htmlwidgets_1.6.4 pkgconfig_2.0.3  \n[25] rstudioapi_0.15.0 digest_0.6.35     R6_2.5.1          tidyselect_1.2.1 \n[29] utf8_1.2.4        pillar_1.9.0      magrittr_2.0.3    withr_3.0.2      \n[33] tools_4.5.1       gtable_0.3.6     \n\n\n\nNotice that under other attached packages we can now see ggplot2_3.5.1 indicating that yes ggplot2 is installed and in memory and that version 3.5.1 is the version I am currently using.\n\nLet’s try the plot again with the ggplot() function from the ggplot2 package.\n\n# try the plot again\nggplot(pressure, aes(temperature, pressure)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReload packages for every new R session\n\n\n\nEverything you close out your R/RStudio computing session (or restart your R session) you will need to load all of your package again. I know this seems like a HUGE pain, but there is a rationale for this.\n\nYou may not need the same packages for every new computing session - so R begins with the minimum loaded to save computing memory.\nThe GOOD NEWS is you do not have to re-install the packages - these are already saved on your computer. You only have to re-load them into memory using the library() function.\nThis workflow forces you to document (in your code) which packages you need for your computing sessions and why you are using them.\n\nBUT … If you do have a core set of packages that you would like to make sure get loaded into memory every time you start R/RStudio, see these helpful posts on customizing your startup:\n\nhttps://www.datacamp.com/doc/r/customizing\nhttps://www.r-bloggers.com/2014/09/fun-with-rprofile-and-customizing-r-startup/",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#create-your-first-r-markdown-report-and-produce-output-files-in-different-formats-html-pdf-or-docx",
    "href": "module131_IntroRRStudio.html#create-your-first-r-markdown-report-and-produce-output-files-in-different-formats-html-pdf-or-docx",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "5. Create your first R Markdown report and produce output files in different formats (HTML, PDF, or DOCX)",
    "text": "5. Create your first R Markdown report and produce output files in different formats (HTML, PDF, or DOCX)\nCreate a new Rmarkdown File\nWe will do more in the later lesson 1.3.6: Putting reproducible research principles into practice, but let’s take a look at an Rmarkdown file and how we can use it to create a report that combines together data + code + documentation to produce a seamless report.\nGo to the RStudio menu and click “File/New File/R Markdown”:\n\n\nType in a title, your name, the date and choose the format you’d like to create. For your first document I encourage you to try HTML. But you can create WORD (DOC) documents and even PDFs. In addition to documents, you can also create slide deck presentations, Shiny apps and other custom products like R packages, websites, books, dashboards and many more.\n\n\n\n\n\n\nRmarkdown ideas and inspiration\n\n\n\n\nRmarkdown Gallery\nRmarkdown Formats\nRmarkdown Cookbook\n\n\n\nTo get started, use the built-in template:\n\nType in a title\nType in your name as author\nChoose and output document format\n\nHTML is always a good place to start - only need a browser to read the output *.html file.\n\nDOC usually works OK - but you need MS Word or Open Office installed on your computer.\n\nPDF NOTE: You need a TEX compiler on your computer - Learn about installing the tinytex https://yihui.org/tinytex/ R package to create PDFs.\n\n\n\n\n\nRmarkdown sections\nHere is the Example RMarkdown Template provided by RStudio to help you get started with your first Rmarkdown document.\n\n\nThis document consists of the following 3 key sections:\n\nYAML (yet another markup language) - this is essentially the metadata for your document and defines elements like the title, author, date and type of output document to be created (HTML in this example).\n\n\n\n\nR code blocks - the goal is to “interweave” code and documentation so these 2 elements live together. That way the analysis output and any associated tables or figures are updated automatically without having to cut-and-paste from other applications into your document - which is time consuming and prone to human errors.\n\nNotice that the code block starts and ends with 3 backticks ``` and includes the {r} Rlanguage designation inside the curly braces.\n\n\n\n\n\n\nRmarkdown\n\n\n\nRmarkdown can be used for many different programming languages including python, sas, and more, see rmarkdown - language-engines.\n\n\n\n\n\nAlong with the R code blocks, we can also create our document with “marked up (or marked down)” text. Rmarkdown is a version of “markdown” which is a simplified set of tags that tell the computer how you want a piece of text formatted.\n\nFor example putting 2 asterisks ** before and after a word will make it bold, putting one _ underscore before and after a word will make the word italics; one or more hashtags # indicate a header at certain levels, e.g. 2 hashtags ## indicate a header level 2.\n\n\n\n\n\n\nRmarkdown Tutorial\n\n\n\nI encourage you to go through the step by step tutorial at https://rmarkdown.rstudio.com/lesson-1.html.\n\n\n\n\nHere are all 3 sections outlined.\n\n\nAt the top of the page you’ll notice a little blue button that says “knit” - this will “knit” (or combine) the output from the R code chunks and format the text as “marked up” and produce this HTML file (which will open in a browser window):",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#r-code-for-this-module",
    "href": "module131_IntroRRStudio.html#r-code-for-this-module",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "R Code For This Module",
    "text": "R Code For This Module\n\nmodule_131.R\nRscript_01.R",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#references",
    "href": "module131_IntroRRStudio.html#references",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "References",
    "text": "References\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#other-helpful-resources",
    "href": "module131_IntroRRStudio.html#other-helpful-resources",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "additionalResources.html",
    "href": "additionalResources.html",
    "title": "Additional Help and Resources",
    "section": "",
    "text": "Download: R from CRAN\n\nThis is where you can download the R language software for FREE for your own computer.\nChoose your operating system (Mac OS or Windows or Linux/Unix)\nNOTE: For Windows, you should also download and install Rtools - this is technically optional, but is useful to have. Make sure to download the one for your R version.\n\nR Cookbook\n\n\n\n\n\nDownload: RStudio IDE Desktop\n\nNote: Windows is listed at the top - just scroll down to see the installer for the Mac OS as well. There are also installers for the versions of Linux/Unix.\n\nRStudio Education\nRStudio Cloud Tutorials\n** Quick-R **",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#r-and-rstudio-resources",
    "href": "additionalResources.html#r-and-rstudio-resources",
    "title": "Additional Help and Resources",
    "section": "",
    "text": "Download: R from CRAN\n\nThis is where you can download the R language software for FREE for your own computer.\nChoose your operating system (Mac OS or Windows or Linux/Unix)\nNOTE: For Windows, you should also download and install Rtools - this is technically optional, but is useful to have. Make sure to download the one for your R version.\n\nR Cookbook\n\n\n\n\n\nDownload: RStudio IDE Desktop\n\nNote: Windows is listed at the top - just scroll down to see the installer for the Mac OS as well. There are also installers for the versions of Linux/Unix.\n\nRStudio Education\nRStudio Cloud Tutorials\n** Quick-R **",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#rmarkdown-resources",
    "href": "additionalResources.html#rmarkdown-resources",
    "title": "Additional Help and Resources",
    "section": "RMarkdown Resources",
    "text": "RMarkdown Resources\n\nRmarkdown Tutorial\nBook: R Markdown: The Definitive Guide\nBook: R Markdown Cookbook",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#r-graphics",
    "href": "additionalResources.html#r-graphics",
    "title": "Additional Help and Resources",
    "section": "R Graphics",
    "text": "R Graphics\n\nBook: ggplot2\nR Graphics Cookbook - online book\nCookbook for R (graphics) - earlier version\nR Graph Gallery\nBook: R Graphics, 3rd edition by Paul Murrell and accompanying R Graphics - book website\nR Charts Website\nBook: Interactive web-based data visualization with R, plotly, and shiny\nBook: R For Data Science - Layers\nBook: The Epidemiologist R Handbook - Data Visualization Section",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#r-packages-for-tables",
    "href": "additionalResources.html#r-packages-for-tables",
    "title": "Additional Help and Resources",
    "section": "R Packages for Tables",
    "text": "R Packages for Tables\n\nMaking Tables (without Rmarkdown)\n\nThe table() function, run help(table, package = \"base\")\ngtsummary\ngmodels, see CrossTable() function\n\n\n\nRmarkdown Tables\n\ngt\ngmodels\ngtsummary\narsenal\nhuxtable\nflextable\n\n\n\nTables with Graphics\n\nR Gallery Tables Summary\ngtExtras\nskimr",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#online-training-and-courses",
    "href": "additionalResources.html#online-training-and-courses",
    "title": "Additional Help and Resources",
    "section": "Online Training and Courses",
    "text": "Online Training and Courses\n\nCode Academy\nSoftware Carpentry\nswiRl - Learn R in R\nDatacamp\n\nR for SAS Users - My Datacamp Course\n\nCoursera\n\nReproducible Templates for Analysis and Dissemination - My Coursera Course\n\nMy Courses at Emory:\n\nEmory N741\nEmory N736",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#more-helpful-online-books-on-r-and-statistics-with-r",
    "href": "additionalResources.html#more-helpful-online-books-on-r-and-statistics-with-r",
    "title": "Additional Help and Resources",
    "section": "More Helpful Online Books on R and Statistics with R",
    "text": "More Helpful Online Books on R and Statistics with R\n\nBook: Statistical Inference via Data Science\nBook: The Epidemiologist R Handbook\nBook/Course: Stat 545\nBook: Statistical Inference via Data Science: A ModernDive into R and the Tidyverse\nBook: R in Action\nOpenIntro Statistics\nMastering Software Development in R",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#other-places-to-get-help",
    "href": "additionalResources.html#other-places-to-get-help",
    "title": "Additional Help and Resources",
    "section": "Other places to get HELP",
    "text": "Other places to get HELP\n\nStackOverflow\n\nI encourage you to create an account so you can post questions. But even without an account you can search for and find answers to your questions and error messages.\n\nGoogle\n\nYou can often cut and paste error messages in Google to find answers - most likely will redirect you to Stack Overflow.\n\nPackage vignettes for packages on CRAN\n\nHere is one vignette for dplyr\nThese will often help you get started.\n\nGithub package issues\n\nMany packages will host their code on Github which includes an “issues” tab. This can be a good place to see what other problems people may be having with a given package.\ndplyr issues on Github\n\nCRAN package site\n\ndplyr on CRAN - spend time looking at:\n\nthe README for the package or\nbug reports or\nNEWS which will detail the changes for each version updates\n\n\nR Bloggers\n\nThis is a really good website which curates thousands of people who are R developers, users and programmers who post articles about R.\n\nSTHDA Website for “Statistical tools for high-throughput data analysis”\n\nThis website will often come up when “Googling” for answers. It has a lot of ads but often has very helpful examples.\n\nQuick-R\n\nThis website was originally created by Robert I. Kabacoff, who wrote the Book: R in Action.\nHowever, the website has now been taken over by Datacamp.",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#r-packages-used-in-tidal-modules",
    "href": "additionalResources.html#r-packages-used-in-tidal-modules",
    "title": "Additional Help and Resources",
    "section": "R Packages Used in TIDAL Modules",
    "text": "R Packages Used in TIDAL Modules\n\ntidyverse",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "PRAMS.html",
    "href": "PRAMS.html",
    "title": "PRAMS Data Analysis",
    "section": "",
    "text": "PRAMS is the Pregnancy Risk Assessment Monitoring System (PRAMS). According to the CDC’s website for About PRAMS:\n\n\n\n\n\n\nWhat is PRAMS?\n\n\n\nPRAMS is the Pregnancy Risk Assessment Monitoring System. It is a joint surveillance project between state, territorial, or local health departments and CDC’s Division of Reproductive Health. PRAMS was developed in 1987 to reduce infant morbidity and mortality by influencing maternal behaviors before, during, and immediately after live birth.\n\n\n\n\n\n\n\n\nWhat is the purpose of PRAMS?\n\n\n\nThe purpose of PRAMS is to find out why some infants are born healthy and others are not. The survey asks new mothers questions about their pregnancy and their new infant. The questions give us important information about the mother and the infant and help us learn more about the impacts of health and behaviors.\n\n\n\n\nYou can request the PRAMS Data from the CDC.\nOnce granted access, follow the instructions from the CDC to download the data and sign the data sharing agreement.\nFor the purposes of the TIDAL R training session, we will be working with PRAMS Phase 8 ARF (Automated Research File) dataset.\n\n\nSee the details on the PRAMS Questionnaires.\nLearn more about the PRAMS Data Methodology including details on how the samples are weighted.\n\nDownload and Read this helpful paper on PRAMS design and methodology (Shulman, D’Angelo, Harrison, Smith, and Warner, 2018).\nThere are also helpful tutorial videos on working with PRAMS data by ASSOCIATION OF STATE AND TERRITORIAL HEALTH OFFICIALS (ASTHO.org).",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#prams-data",
    "href": "PRAMS.html#prams-data",
    "title": "PRAMS Data Analysis",
    "section": "",
    "text": "PRAMS is the Pregnancy Risk Assessment Monitoring System (PRAMS). According to the CDC’s website for About PRAMS:\n\n\n\n\n\n\nWhat is PRAMS?\n\n\n\nPRAMS is the Pregnancy Risk Assessment Monitoring System. It is a joint surveillance project between state, territorial, or local health departments and CDC’s Division of Reproductive Health. PRAMS was developed in 1987 to reduce infant morbidity and mortality by influencing maternal behaviors before, during, and immediately after live birth.\n\n\n\n\n\n\n\n\nWhat is the purpose of PRAMS?\n\n\n\nThe purpose of PRAMS is to find out why some infants are born healthy and others are not. The survey asks new mothers questions about their pregnancy and their new infant. The questions give us important information about the mother and the infant and help us learn more about the impacts of health and behaviors.\n\n\n\n\nYou can request the PRAMS Data from the CDC.\nOnce granted access, follow the instructions from the CDC to download the data and sign the data sharing agreement.\nFor the purposes of the TIDAL R training session, we will be working with PRAMS Phase 8 ARF (Automated Research File) dataset.\n\n\nSee the details on the PRAMS Questionnaires.\nLearn more about the PRAMS Data Methodology including details on how the samples are weighted.\n\nDownload and Read this helpful paper on PRAMS design and methodology (Shulman, D’Angelo, Harrison, Smith, and Warner, 2018).\nThere are also helpful tutorial videos on working with PRAMS data by ASSOCIATION OF STATE AND TERRITORIAL HEALTH OFFICIALS (ASTHO.org).",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#prework---before-you-begin",
    "href": "PRAMS.html#prework---before-you-begin",
    "title": "PRAMS Data Analysis",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\nInstall R Packages\nBefore you begin, please go ahead and install (or make sure these are already installed) on your computer for these following packages - these are all on CRAN, so you can install them using the RStudio Menu Tools/Install Packages interface:\n\nhaven\ndplyr\nsurvey\n\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(survey)\n\nCreate a NEW RStudio Project\nBEFORE you being any new analysis project, it is ALWAYS a good idea to begin with the NEW RStudio project.\nGo to the RStudio menu “File/New Project” and create your new project (ideally in a NEW directory, but it is also ok to use an exisiting directory/folder on your computer).\nThis new directory (or folder) will be where all of your files will “live” for your current analysis project.\nSee the step-by-step instructions for creating a new RStudio project in Module 1.3.2.",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#get-prams-data-and-select-subset-for-analysis",
    "href": "PRAMS.html#get-prams-data-and-select-subset-for-analysis",
    "title": "PRAMS Data Analysis",
    "section": "1. Get PRAMS Data and Select Subset for Analysis",
    "text": "1. Get PRAMS Data and Select Subset for Analysis\nA. Read-in the PRAMS Phase 8 2016-2021 combined dataset\nThe PRAMS data provided by the CDC will be in SAS format (*.sas7bdat). We can read the native SAS file into R using the haven package and the read_sas() function.\n\n\n\n\n\n\nMemory Warning\n\n\n\nThe size of the phase8_arf_2016_2021.sas7bdat dataset is a little over 1GB. So, make sure your computer has enough available memory to fully load this dataset. I will provide some more details below on how we can reduce the size of the dataset and improve the memory issues below.\n\n\nYou can check your available memory, by checking your “Global Environment” TAB (upper right window pane) click on the down arrow next to the icon with “XX MiB” just to the left of the little broom:\n\n\n\nClick on the “Memory Usage Report” to see a detailed breakdown. This window will show:\n\nMemory used by R objects (in your “Global Environment”)\nMemory used on your computer by your current R Session\nMemory currently in use for everything currently running on your computer (all apps running - active and in background) - you can compare this to your “task manager” memory viewer.\nFree System Memory - when this gets low the “XX MiB” graphic will change color from green - to yellow - to orange - to red. Once you get to red, your R session will most likely crash since there is not enough memory to perfom operations or run analyses.\n\nThis is a screen shot of my computer (yours will look different) BEFORE I load the PRAMS dataset.\n\n\n\nRun the following R code to load the PRAMS Phase 8 dataset into your R Session and check the “Global Environment”.\n\nlibrary(haven)\nprams &lt;- \n  read_sas(\"phase8_arf_2016_2021.sas7bdat\")\n\nHere is my memory AFTER loading the PRAMS dataset into my “Global Environment”.\n\n\n\n\n\nB. Save the data as a *.RData binary file for use in later analyses\nOne way to reduce the size of the PRAMS dataset is to save it as a native *.RData binary file format. So, let’s save the PRAMS dataset in this format on your computer.\n\n# save the whole dataset as *.RData format\nsave(prams, \n     file = \"prams.RData\")\n\nOn my computer, here is a comparison of the size of these 2 files:\n\n\nphase8_arf_2016_2021.sas7bdat is 1,095,499,776 bytes (which is 1.02 GB)\n\nprams.RData is only 34,713,319 (which is only 0.0323 GB)\n\nThis is a file size reduction of 96.83%!!\n\n\n\nNow that we’ve reduced the file size of the dataset on your computer’s hard drive (or cloud storage), let’s also clear up the “Global Environment” back in your current RStudio computing session.\nC. Clean up files to save memory\nNow that we’ve saved the data, let’s remove the PRAMS data object from the RStudio session.\n\nFor now we can simply remove everything using the rm(list=ls()).\nHowever, if you have other objects you want to keep, you can specifically only remove the PRAMS dataset using rm(prams).\n\n\n# remove all objects from Global Environment\nrm(list=ls())\n\n# confirm Global Environment is empty\n# list all objects\nls()\n\ncharacter(0)\n\n# and free any currently unused memory\ngc()\n\n          used  (Mb) gc trigger   (Mb)  max used  (Mb)\nNcells 2117797 113.2    4160094  222.2   4160094 222.2\nVcells 3873673  29.6  153306787 1169.7 112056277 855.0\n\n\nAfter we remove everything, let’s look at the session memory again.\n\n\n\nNow let’s read the PRAMS data back in, but this time read in the prams.RData binary R data formatted file. We will use the built-in load() function.\n\n# load back only the prams dataset\nload(file = \"prams.RData\")\n\nLet’s check the R session memory again:\n\n\n\nI know this didn’t make a large difference for the R session available memory, but by doing this process:\n\nThe PRAMS dataset now takes up less memory on your computer’s file storage, and\nThe load() function for the prams.RData file should run faster when beginning your R computing session instead of having to use the haven package to read in the SAS formatted file everytime.\n\nAs a quick comparison on my computer (Windows 11), the time to read in the SAS formatted file was about 14 sec:\n&gt; system.time(\n+   prams &lt;- \n+     read_sas(\"phase8_arf_2016_2021.sas7bdat\")\n+ )\n   user  system elapsed \n  13.44    0.47   13.96\nAnd the time to read in the prams.RData file was only about 1.5 sec.\n&gt; system.time(\n+   load(\"prams.RData\")\n+ )\n   user  system elapsed \n   1.45    0.08    1.54",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#getting-started-with-prams-data",
    "href": "PRAMS.html#getting-started-with-prams-data",
    "title": "PRAMS Data Analysis",
    "section": "2. Getting started with PRAMS Data",
    "text": "2. Getting started with PRAMS Data\nBreastfeeding summary - UNWEIGHTED data\nLet’s look at whether the mother ever breastfed her baby - this is variable BF5EVER, where 1 = “NO” and 2 = “YES”.\nPRAMS Phase 8 Codebook\n\n# create a factor variable\n# and add labels\nprams$BF5EVER.f &lt;- factor(\n  prams$BF5EVER,\n  levels = c(1, 2),\n  labels = c(\"NO\", \"YES\")\n)\n\nFor the UNWEIGHTED data, let’s get a simple table of breastfeeding by STATE (variable STATE) and YEAR (variable NEST_YR).\nAs we can see below, in 2017 for the state of GA, 919 women responded to this question:\n\n919 women responded\n\n170 said NO\n749 said YES\n\n\n36 were missing a response (indicated by &lt;NA&gt;)\n\n\nprams %&gt;%\n  filter(NEST_YR == 2017) %&gt;% \n  with(., table(STATE, BF5EVER.f, \n                useNA = \"ifany\"))\n\n     BF5EVER.f\nSTATE   NO  YES &lt;NA&gt;\n   AK   71  927   47\n   AL  181  659   42\n   CO   73 1037   18\n   DE  126  728   37\n   GA  170  749   36\n   IA  136  867   30\n   IL  140 1048   36\n   KS   81  856   58\n   KY  139  536   27\n   LA  285  586   23\n   MA  115 1268   40\n   MD   97  928   35\n   ME   88  754   30\n   MI  290 1532   75\n   MO  166  908   37\n   MT   66  851   20\n   ND  102  472   17\n   NH   42  523   15\n   NJ  125 1102   31\n   NM  123 1038   19\n   NY  109  706   33\n   PA  164 1023   42\n   PR   81  928   23\n   RI  105  960   37\n   SD  150  946   35\n   UT   93 1305   49\n   VA   88  969   26\n   VT   54  780   14\n   WA   69 1138   31\n   WI  221 1051   74\n   WV  186  475   38\n   WY   49  438   16\n   YC   99 1125   69\n\n\nThis aligns with the CDC PRAMS Indicators Report for GA in 2020 - scroll to the bottom to see the RAW count of 919 women who responded to “Ever Breastfed” in GA in 2017.\nBreastfeeding summary - WEIGHTED data\nIn the CDC PRAMS Indicators Report for GA in 2020 the columns that have the 95% CI (confidence intervals) for the percentages are the population weighted percentage estimates for the Stats of GA during that year.\nTo get the estimated percentage of women in the stats of GA who had “ever breastfed” in 2017, we need to use the survey package and apply the proper sample weighting to get these estimates.\n\nlibrary(survey)\n\n# Let's look at just GA to start with\n# use dplyr to filter out just GA\nprams_ga &lt;- prams %&gt;%\n  filter(STATE == \"GA\")\n\n# create the survey design file for GA\nprams_ga.svy &lt;- \n  svydesign(ids = ~0, strata = ~SUD_NEST, \n            fpc = ~TOTCNT, weights = ~WTANAL, \n            data = prams_ga)\n\n# get a table of ever breastfed\n# by YEAR\nsvyby(~BF5EVER.f, ~NEST_YR,\n      design = prams_ga.svy,\n      svytotal, na.rm=TRUE)\n\n     NEST_YR BF5EVER.fNO BF5EVER.fYES se.BF5EVER.fNO se.BF5EVER.fYES\n2017    2017    17639.96    101686.10       2045.415        2271.075\n2018    2018    20187.62     98909.35       2151.496        2351.330\n2019    2019    24099.04     95019.86       2273.415        2279.851\n2020    2020    21827.55     94125.72       2209.745        2457.097\n2021    2021    23724.68     93896.73       2266.811        2256.488\n\n\nFrom this we can see that the population estimates for 2017 are:\n\nBreastfed ever = NO: 17639.96 +/- 2045.415\nBreastfed ever = YES: 101686.10 +/- 2271.075\n\nThis leads to a percentage of YES estimate of 101686.10 * 100 / (101686.10 + 17639.96) = 85.2170096% which should match pretty closely to what is in the CDC PRAMS Indicators Report for GA in 2020.\nWe can also get the percentage of overall breastfeeding YES for the USA for the 40 “states” (technically 38 states, Puerto Rico, and New York City) that were included in the PRAMS dataset in 2020 (see the last column in the CDC report), using the following R code. Note: 2 “states” did not have data in 2020: Connecticut and Florida.\n\n# get overall for 2020 - all states\n# make survey design file\nprams.svy &lt;- \n  svydesign(ids = ~0, strata = ~SUD_NEST, \n            fpc = ~TOTCNT, weights = ~WTANAL, \n            data = prams)\n\nsvyby(~BF5EVER.f, ~NEST_YR,\n      design = prams.svy,\n      svytotal, na.rm=TRUE)\n\n     NEST_YR BF5EVER.fNO BF5EVER.fYES se.BF5EVER.fNO se.BF5EVER.fYES\n2016    2016    187666.4      1324171       4398.541        4798.836\n2017    2017    208863.1      1497127       4762.339        5452.232\n2018    2018    242991.5      1716913       5220.222        5979.598\n2019    2019    236841.9      1680987       5404.761        6877.697\n2020    2020    225560.3      1609464       4884.871        5540.240\n2021    2021    212618.8      1521303       5196.234        6058.572\n\n\nFrom this we can see that the population estimates for the “whole USA” for 2020 were:\n\nBreastfed ever = NO: 225560.3 +/- 4884.871\nBreastfed ever = YES: 1609464 +/- 5540.240\n\nThis leads to a percentage of YES estimate of 1609464 * 100 / (1609464 + 225560.3) = 87.7080483% which is pretty close to what is in the CDC PRAMS Indicators Report for GA in 2020 - with some numerical precision variation due to software algorithms.\nCongratulations on getting started with the PRAMS Dataset",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#data-wrangling-with-prams",
    "href": "PRAMS.html#data-wrangling-with-prams",
    "title": "PRAMS Data Analysis",
    "section": "3. Data Wrangling with PRAMS",
    "text": "3. Data Wrangling with PRAMS\nData wrangling with the PRAMS data isn’t much different from the methods already covered in Module 1.3.2.\nExamples will be posted here working with the PRAMS Dataset for recoding, creating or modifying variables.\n\n# create a factor variable\n# and add labels\nprams$VITAMIN.f &lt;- factor(\n  prams$VITAMIN,\n  levels = c(1, 2, 3, 4),\n  labels = c(\"1 = DIDNT TAKE VITAMIN\",\n             \"2 = 1-3 TIMES/WEEK\",\n             \"3 = 4-6 TIMES/WEEK\",\n             \"4 = EVERY DAY/WEEK\")\n)\n\n# create variable for anyone who\n# took vitamins 4+ times a week\nprams$VITAMIN_4plus &lt;- \n  ifelse(prams$VITAMIN &gt; 2, 1, 0)\n\n# add labels, make a factor\nprams$VITAMIN_4plus.f &lt;- factor(\n  prams$VITAMIN_4plus,\n  levels = c(0, 1),\n  labels = c(\"3x/week or less\",\n             \"4x/week or more\")\n)\n\n# get stats for 2020 for GA\nprams %&gt;%\n  filter(NEST_YR == 2020) %&gt;% \n  filter(STATE == \"GA\") %&gt;%\n  with(., table(STATE, VITAMIN_4plus.f, \n                useNA = \"ifany\"))\n\n     VITAMIN_4plus.f\nSTATE 3x/week or less 4x/week or more &lt;NA&gt;\n   GA             443             247    2\n\nprams_ga &lt;- prams %&gt;%\n  filter(STATE == \"GA\")\n\n# create the survey design file for GA\nprams_ga.svy &lt;- \n  svydesign(ids = ~0, strata = ~SUD_NEST, \n            fpc = ~TOTCNT, weights = ~WTANAL, \n            data = prams_ga)\n\n# get a table of vitamins 4+ times per week\n# by YEAR\nsvyby(~VITAMIN_4plus.f, ~NEST_YR,\n      design = prams_ga.svy,\n      svytotal, na.rm=TRUE)\n\n     NEST_YR VITAMIN_4plus.f3x/week or less VITAMIN_4plus.f4x/week or more\n2017    2017                       86492.91                       37312.18\n2018    2018                       76796.28                       43028.81\n2019    2019                       74523.87                       46236.67\n2020    2020                       75313.80                       42263.67\n2021    2021                       69861.41                       48766.71\n     se.VITAMIN_4plus.f3x/week or less se.VITAMIN_4plus.f4x/week or more\n2017                          2715.819                          2653.349\n2018                          2817.611                          2732.988\n2019                          2786.899                          2666.692\n2020                          2779.229                          2802.984\n2021                          2824.624                          2693.566\n\n\nThe unweighted breakdown for GA in 2020\n\nNO Vitamins =&lt; 3x/wk 443 64.2%\nYES Vitamins =&gt; 4x/wk 247 35.8%\nTotal 690\n\nWeighted Breakdown for GA in 2020\n\nNO Vitamins =&lt; 3x/wk 75313.80 +/- 2779.229 (64.1%) []\nYES Vitamins =&gt; 4x/wk 42263.67 +/- 2802.984 (35.9%) [33.6%, 38.3%]\nTotal 117,577.47\n\nget proportions and ci’s\n\nprams_ga2000 &lt;- prams %&gt;%\n  filter(STATE == \"GA\") %&gt;%\n  filter(NEST_YR == 2020)\n\n# create the survey design file for GA\n# for year 2020\nprams_ga2000.svy &lt;- \n  svydesign(ids = ~0, strata = ~SUD_NEST, \n            fpc = ~TOTCNT, weights = ~WTANAL, \n            data = prams_ga2000)\n\nsvytable(~VITAMIN_4plus, \n         prams_ga2000.svy)\n\nVITAMIN_4plus\n       0        1 \n75313.80 42263.67 \n\nsvyciprop(~VITAMIN_4plus, \n          prams_ga2000.svy, \n          na.rm = T)\n\n                     2.5% 97.5%\nVITAMIN_4plus 0.359 0.315 0.407\n\n\nCompare the results below to the EXCEL spreadsheet Pregnancy Risk Assessment Monitoring System (PRAMS) MCH Indicators (standard version) - see 2020 for GA - 1st set of indicators for Vitamins taken 4x a week or more.\n\nprams_ga2000 &lt;- prams %&gt;%\n  filter(STATE == \"GA\") %&gt;%\n  filter(NEST_YR == 2020)\n\n# create the survey design file for GA\n# for year 2020\nprams_ga2000.svy &lt;- \n  svydesign(ids = ~0, strata = ~SUD_NEST, \n            fpc = ~TOTCNT, weights = ~WTANAL, \n            data = prams_ga2000)\n\n# get a table of vitamins 4+ times per week\n# by YEAR\nsvyby(~VITAMIN_4plus.f, ~NEST_YR,\n      design = prams_ga2000.svy,\n      svytotal, na.rm=TRUE)\n\n     NEST_YR VITAMIN_4plus.f3x/week or less VITAMIN_4plus.f4x/week or more\n2020    2020                        75313.8                       42263.67\n     se.VITAMIN_4plus.f3x/week or less se.VITAMIN_4plus.f4x/week or more\n2020                          2779.229                          2802.984\n\n# add custom statistic forconfidence intervals\nconfidence_intervals &lt;- function(data, variable, by, ...) {\n  \n  ## extract the confidence intervals and multiply to get percentages\n  props &lt;- svyciprop(as.formula(paste0( \"~\" , variable)),\n              data, na.rm = TRUE)\n  \n  ## extract the confidence intervals \n  as.numeric(confint(props) * 100) %&gt;% ## make numeric and multiply for percentage\n    round(., digits = 1) %&gt;%           ## round to one digit\n    c(.) %&gt;%                           ## extract the numbers from matrix\n    paste0(., collapse = \"-\")          ## combine to single character\n}\n\nlibrary(gtsummary)\ntbl_svysummary(\n  data = prams_ga2000.svy,\n  include = c(VITAMIN_4plus),\n  statistic = list(everything() ~ c(\"{n} ({p}%)\"))\n  ) %&gt;%\n  add_n() %&gt;%  \n  add_stat(fns = everything() ~ confidence_intervals) %&gt;%\n  modify_header(\n    list(\n      n ~ \"**Weighted total (N)**\",\n      stat_0 ~ \"**Weighted Count**\",\n      add_stat_1 ~ \"**95%CI**\"\n    ))\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nWeighted total (N)\n\nWeighted Count1\n\n95%CI\n\n\n\nVITAMIN_4plus\n117,577\n42,264 (36%)\n31.5-40.7\n\n\n    Unknown\n\n363\n\n\n\n\n\n1 n (%)",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#visualizing-prams-data",
    "href": "PRAMS.html#visualizing-prams-data",
    "title": "PRAMS Data Analysis",
    "section": "4. Visualizing PRAMS Data",
    "text": "4. Visualizing PRAMS Data\nExamples will be posted here for making graphs and figures with suggestions on handling very large datasets.\nlet’s look at maternal age variable MAT_AGE_PU, see PRAMS Codebook.\nHistogram of Maternal Age - Unweighted\n\nhist(prams$MAT_AGE_PU)\n\n\n\n\n\n\n\nHistogram of Maternal Age - Complex Survey Weighted\n\n# use survey design data\n# get histogram using svyhist() function\nsvyhist(formula = ~MAT_AGE_PU,\n        design = prams.svy)\n\n\n\n\n\n\n\nMaternal weight gain in lbs\n\n# MOMLBS\nhist(prams$MOMLBS)\n\n\n\n\n\n\nsummary(prams$MOMLBS)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00   19.00   28.00   28.48   37.00   97.00    6275 \n\nsvyhist(formula = ~MOMLBS,\n        design = prams.svy)\n\n\n\n\n\n\n\nscatterplot of weight gain by age\nlook at ga for 2020\n\nlibrary(ggplot2)\nggplot(prams_ga2000, aes(x=MAT_AGE_PU, y=MOMLBS)) +\n  geom_point()\n\n\n\n\n\n\n\nweighted plot - notice the varying sizes of the dots (bubbles)\n\nsvyplot(MOMLBS~MAT_AGE_PU, \n        prams_ga2000.svy,\n        style = \"bubble\")\n\n\n\n\n\n\n\nanother option - gray scale hex symbols - darker indicate higher counts, see help(svyplot, package = \"survey\").\n\nsvyplot(MOMLBS~MAT_AGE_PU, \n        prams_ga2000.svy,\n        style = \"grayhex\")",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#missing-data-in-prams",
    "href": "PRAMS.html#missing-data-in-prams",
    "title": "PRAMS Data Analysis",
    "section": "5. Missing Data in PRAMS",
    "text": "5. Missing Data in PRAMS\nExamples will be posted here for summarizing and visualizing missing data in PRAMS.\n\nprams_ga2000 &lt;- prams %&gt;%\n  filter(STATE == \"GA\") %&gt;%\n  filter(NEST_YR == 2020)\n\n# amount of missing data for VITAMIN\n# unweighted\n #   1    2    3    4 &lt;NA&gt; \n # 390   53   33  214    2\n# 2/692 = 0.289%\ntable(prams_ga2000$VITAMIN, useNA = \"ifany\")\n\n\n   1    2    3    4 &lt;NA&gt; \n 390   53   33  214    2 \n\n# add missing indicator for VITAMIN\nprams_ga2000$VITAMIN_na &lt;-\n  as.numeric(is.na(prams_ga2000$VITAMIN))\nsum(prams_ga2000$VITAMIN_na)\n\n[1] 2\n\n# create the survey design file for GA\n# for year 2020\nprams_ga2000.svy &lt;- \n  svydesign(ids = ~0, strata = ~SUD_NEST, \n            fpc = ~TOTCNT, weights = ~WTANAL, \n            data = prams_ga2000)\n\ntbl_svysummary(\n  data = prams_ga2000.svy,\n  include = c(VITAMIN_na),\n  statistic = list(everything() ~ c(\"{n} ({p}%)\"))\n  ) %&gt;%\n  add_n() %&gt;%  \n  add_stat(fns = everything() ~ confidence_intervals) %&gt;%\n  modify_header(\n    list(\n      n ~ \"**Weighted total (N)**\",\n      stat_0 ~ \"**Weighted Count**\",\n      add_stat_1 ~ \"**95%CI**\"\n    ))\n\n\n\n\n\nCharacteristic\nWeighted total (N)\n\nWeighted Count1\n\n95%CI\n\n\nVITAMIN_na\n117,940\n363 (0.3%)\n0.1-1.8\n\n\n\n1 n (%)\n\n\n\n\n# weighted 0.3%, CI: 0.1 to 1.8%",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#prams-statistical-tests-and-models",
    "href": "PRAMS.html#prams-statistical-tests-and-models",
    "title": "PRAMS Data Analysis",
    "section": "6. PRAMS Statistical Tests and Models",
    "text": "6. PRAMS Statistical Tests and Models\nExamples will be posted here for statistical tests and models (such as linear and logistic regression) for both the unweighted and weighted data approaches.\nassociation of age and weight gain - linear regression\n\nlm1 &lt;- lm(MOMLBS ~ MAT_AGE_PU,\n          data = prams_ga2000)\nsummary(lm1)\n\n\nCall:\nlm(formula = MOMLBS ~ MAT_AGE_PU, data = prams_ga2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.566  -8.733  -1.089   8.161  68.722 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 26.23389    2.80325   9.358   &lt;2e-16 ***\nMAT_AGE_PU   0.07572    0.09545   0.793    0.428    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.18 on 686 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.0009166, Adjusted R-squared:  -0.0005398 \nF-statistic: 0.6294 on 1 and 686 DF,  p-value: 0.4279\n\n\nweighted\n\nsummary(svyglm(MOMLBS ~ MAT_AGE_PU, \n               design = prams_ga2000.svy))\n\n\nCall:\nsvyglm(formula = MOMLBS ~ MAT_AGE_PU, design = prams_ga2000.svy)\n\nSurvey design:\nsvydesign(ids = ~0, strata = ~SUD_NEST, fpc = ~TOTCNT, weights = ~WTANAL, \n    data = prams_ga2000)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.210104   3.935380   7.677 5.65e-14 ***\nMAT_AGE_PU  -0.005842   0.135304  -0.043    0.966    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 231.9695)\n\nNumber of Fisher Scoring iterations: 2\n\n\ncontingency tables - vitamin use by breastfeeding\n\ntable(prams_ga2000$VITAMIN_4plus.f, \n      prams_ga2000$BF5EVER.f,\n      useNA = \"ifany\")\n\n                 \n                   NO YES &lt;NA&gt;\n  3x/week or less  95 337   11\n  4x/week or more  18 225    4\n  &lt;NA&gt;              1   1    0\n\nlibrary(gmodels)\nCrossTable(x = prams_ga2000$VITAMIN_4plus.f, \n           y = prams_ga2000$BF5EVER.f,\n           expected = TRUE,\n           prop.r = FALSE,\n           prop.c = TRUE,\n           prop.t = FALSE,\n           prop.chisq = FALSE,\n           chisq = TRUE,\n           format = \"SPSS\")\n\n\n   Cell Contents\n|-------------------------|\n|                   Count |\n|         Expected Values |\n|          Column Percent |\n|-------------------------|\n\nTotal Observations in Table:  675 \n\n                             | prams_ga2000$BF5EVER.f \nprams_ga2000$VITAMIN_4plus.f |       NO  |      YES  | Row Total | \n-----------------------------|-----------|-----------|-----------|\n             3x/week or less |       95  |      337  |      432  | \n                             |   72.320  |  359.680  |           | \n                             |   84.071% |   59.964% |           | \n-----------------------------|-----------|-----------|-----------|\n             4x/week or more |       18  |      225  |      243  | \n                             |   40.680  |  202.320  |           | \n                             |   15.929% |   40.036% |           | \n-----------------------------|-----------|-----------|-----------|\n                Column Total |      113  |      562  |      675  | \n                             |   16.741% |   83.259% |           | \n-----------------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  23.72972     d.f. =  1     p =  1.108573e-06 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  22.69497     d.f. =  1     p =  1.898642e-06 \n\n \n       Minimum expected frequency: 40.68 \n\n\nweighted\n\nsvytable(~VITAMIN_4plus.f + BF5EVER.f, \n         prams_ga2000.svy)\n\n                 BF5EVER.f\nVITAMIN_4plus.f          NO       YES\n  3x/week or less 18458.169 55008.856\n  4x/week or more  3334.398 38789.322\n\nsvychisq(~VITAMIN_4plus.f + BF5EVER.f, \n         prams_ga2000.svy, \n         statistic = \"Chisq\")\n\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  svychisq(~VITAMIN_4plus.f + BF5EVER.f, prams_ga2000.svy, statistic = \"Chisq\")\nX-squared = 31.025, df = 1, p-value = 1.222e-05\n\n\nlogistic regression\nlook at vitamin use by breastfeeding and maternal age\n\nglm1 &lt;-glm(VITAMIN_4plus ~ MAT_AGE_PU + BF5EVER.f,\n           data = prams_ga2000,\n           family = \"binomial\")\ngtsummary::tbl_regression(glm1,\n                          exponentiate = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\nMaternal age grouped\n1.07\n1.04, 1.11\n&lt;0.001\n\n\nBF5EVER.f\n\n\n\n\n\n    NO\n—\n—\n\n\n\n    YES\n2.82\n1.67, 4.99\n&lt;0.001\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nweighted\n\nwtglm1 &lt;- svyglm(VITAMIN_4plus ~ MAT_AGE_PU + BF5EVER.f, \n                 design = prams_ga2000.svy,\n                 family=quasibinomial())\nsummary(wtglm1)\n\n\nCall:\nsvyglm(formula = VITAMIN_4plus ~ MAT_AGE_PU + BF5EVER.f, design = prams_ga2000.svy, \n    family = quasibinomial())\n\nSurvey design:\nsvydesign(ids = ~0, strata = ~SUD_NEST, fpc = ~TOTCNT, weights = ~WTANAL, \n    data = prams_ga2000)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -3.36672    0.60317  -5.582 3.46e-08 ***\nMAT_AGE_PU    0.06250    0.01901   3.287 0.001064 ** \nBF5EVER.fYES  1.18775    0.33359   3.561 0.000396 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 1.006803)\n\nNumber of Fisher Scoring iterations: 4\n\nexp(coef(wtglm1))\n\n (Intercept)   MAT_AGE_PU BF5EVER.fYES \n   0.0345025    1.0644955    3.2797036",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#prams-reproducible-research-report",
    "href": "PRAMS.html#prams-reproducible-research-report",
    "title": "PRAMS Data Analysis",
    "section": "7. PRAMS Reproducible Research Report",
    "text": "7. PRAMS Reproducible Research Report\nA Rmarkdown analysis report will be provided here as a template to “kick start” your research with the PRAMS dataset.\n\nDownload this Rmarkdown template PRAMS Report.\nKnit to HTML\nKnit to DOC\nKnit to PDF (if you’ve installed tinytex package)\nKnit with Parameters - Change the year from 2020 to 2018 and re-knit the document",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#references",
    "href": "PRAMS.html#references",
    "title": "PRAMS Data Analysis",
    "section": "References",
    "text": "References\n\n\nBates, Douglas, Martin Maechler, and Mikael Jagan. 2024. Matrix: Sparse and Dense Matrix Classes and Methods. https://Matrix.R-forge.R-project.org.\n\n\nBoettiger, Carl. 2021. Knitcitations: Citations for Knitr Markdown Files. https://github.com/cboettig/knitcitations.\n\n\nLumley, Thomas. 2004. “Analysis of Complex Survey Samples.” Journal of Statistical Software 9 (1): 1–19.\n\n\n———. 2010. Complex Surveys: A Guide to Analysis Using r: A Guide to Analysis Using r. John Wiley; Sons.\n\n\nLumley, Thomas, Peter Gao, and Ben Schneider. 2024. Survey: Analysis of Complex Survey Samples. http://r-survey.r-forge.r-project.org/survey/.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nShulman, Holly B., Denise V. D’Angelo, Leslie Harrison, Ruben A. Smith, and Lee Warner. 2018. “The Pregnancy Risk Assessment Monitoring System (PRAMS): Overview of Design and Methodology.” American Journal of Public Health 108 (10): 1305–13. https://doi.org/10.2105/ajph.2018.304563.\n\n\nSjoberg, Daniel D., Joseph Larmarange, Michael Curry, Jessica Lavery, Karissa Whiting, and Emily C. Zabor. 2024. Gtsummary: Presentation-Ready Data Summary and Analytic Result Tables. https://github.com/ddsjoberg/gtsummary.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery, and Joseph Larmarange. 2021. “Reproducible Summary Tables with the Gtsummary Package.” The R Journal 13: 570–80. https://doi.org/10.32614/RJ-2021-053.\n\n\nTerry M. Therneau, and Patricia M. Grambsch. 2000. Modeling Survival Data: Extending the Cox Model. New York: Springer.\n\n\nTherneau, Terry M. 2024. Survival: Survival Analysis. https://github.com/therneau/survival.\n\n\nWarnes, Gregory R., Ben Bolker, Thomas Lumley, Randall C Johnson. Contributions from Randall C. Johnson are Copyright SAIC-Frederick, Inc. Funded by the Intramural Research Program, of the NIH, National Cancer Institute, and Center for Cancer Research under NCI Contract NO1-CO-12400. 2022. Gmodels: Various r Programming Tools for Model Fitting. https://doi.org/10.32614/CRAN.package.gmodels.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. Haven: Import and Export SPSS, Stata and SAS Files. https://haven.tidyverse.org.",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#other-helpful-resources",
    "href": "PRAMS.html#other-helpful-resources",
    "title": "PRAMS Data Analysis",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theme 1. Measurement and Prediction of IPV",
    "section": "",
    "text": "Presented by Melinda Higgins, PhD\nModule 1.3 will introduce the learner to writing code in the R language and utilizing the RStudio IDE (integrated development environment). This module will include 6 sessions - the first 3 will be asynchronous-online (AO) and the last 3 will be in-person (IP).\nThe 6 sessions are:\n\n1.3.1: Introduction to R and R Studio (AO)\n1.3.2: Data Wrangling (AO)\n1.3.3: Data Visualization (AO)\n1.3.4: Missing data and Sampling Weights (IP)\n1.3.5: Statistical tests and models (IP)\n1.3.6: Putting reproducible research principles into practice (IP)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#module-1.3-data-analytics-using-r",
    "href": "index.html#module-1.3-data-analytics-using-r",
    "title": "Theme 1. Measurement and Prediction of IPV",
    "section": "",
    "text": "Presented by Melinda Higgins, PhD\nModule 1.3 will introduce the learner to writing code in the R language and utilizing the RStudio IDE (integrated development environment). This module will include 6 sessions - the first 3 will be asynchronous-online (AO) and the last 3 will be in-person (IP).\nThe 6 sessions are:\n\n1.3.1: Introduction to R and R Studio (AO)\n1.3.2: Data Wrangling (AO)\n1.3.3: Data Visualization (AO)\n1.3.4: Missing data and Sampling Weights (IP)\n1.3.5: Statistical tests and models (IP)\n1.3.6: Putting reproducible research principles into practice (IP)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#project-tidal",
    "href": "index.html#project-tidal",
    "title": "Theme 1. Measurement and Prediction of IPV",
    "section": "Project TIDAL",
    "text": "Project TIDAL\nProject TIDAL is an NIH-funded R25 study (1 R25 NR021324-01 ), co-led by Drs. Sangmi Kim and Ran Xiao from Nell Hodgson Woodruff School of Nursing at Emory University.\nThe objective of the study is to develop a short course titled “Trauma-Informed Data Science and Digital Health Technologies to Prevent Intimate Partner Violence (IPV) among Pregnant/Postpartum Women” targeting early-career researchers (predoctoral, postdoctoral, and junior faculty) from diverse backgrounds across the U.S.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html",
    "href": "module132_DataWrangling.html",
    "title": "1.3.2: Data Wrangling",
    "section": "",
    "text": "To read in data.\nTo view the Data.\nTo subset the data - select and filter.\nTo create and modify variables.\nTo get data summary and descriptive statistics\nExporting/Saving Data",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#session-objectives",
    "href": "module132_DataWrangling.html#session-objectives",
    "title": "1.3.2: Data Wrangling",
    "section": "",
    "text": "To read in data.\nTo view the Data.\nTo subset the data - select and filter.\nTo create and modify variables.\nTo get data summary and descriptive statistics\nExporting/Saving Data",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#prework---before-you-begin",
    "href": "module132_DataWrangling.html#prework---before-you-begin",
    "title": "1.3.2: Data Wrangling",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\nInstall Packages\nBefore you begin, please go ahead and install the following packages - these are all on CRAN, so you can install them using the RStudio Menu “Tools/Install” Packages interface:\n\n\nreadr on CRAN and readr package website\n\n\nreadxl on CRAN and readxl package website\n\n\nhaven on CRAN and haven package website\n\n\ndplyr on CRAN and dplyr package website\n\n\nHmisc on CRAN and Hmiscpackage website\n\n\npsych on CRAN and psych package website\n\n\narsenal on CRAN and arsenal package website\n\n\ngtsummary on CRAN and gtsummary package website\n\ntableone on CRAN\ngmodels on CRAN\npkgsearch on CRAN\npalmerpenguins on CRAN\n\nSee Module 1.3.1 on Installing Packages",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-read-in-data.",
    "href": "module132_DataWrangling.html#to-read-in-data.",
    "title": "1.3.2: Data Wrangling",
    "section": "1. To read in data.",
    "text": "1. To read in data.\nBegin with a NEW RStudio Project\nLet’s begin with a new RStudio Project.\n\nFirst click on the menu at top for “File/New Project”:\n\n\n\n\nNext choose either an “Existing Directory” or “New Directory” depending on whether you want to use a folder that already exists on your computer or you want to create a new folder.\n\n\n\nFor now, let’s choose a “New Directory” and then select “New Project”\n\n\n\n\nWhen the next window opens, as an example, I’m creating a new project folder called myfirstRproject for my RStudio project under my parent directory, C:\\MyGithub. Your folder names and directories will most likely be different than mine.\n\n\n\n\nSo, if I look back on my computer in my file manager (I’m on a computer with Windows 11 operating system) - I can now see this new folder on my computer for C:\\MyGithub\\myfirstRproject.\n\n\n\n\nNow let’s put some data into this folder. Feel free to move datasets of your own into this new RStudio project directory. But here are some test datasets you can download and place into this new directory on your computer - choose at least one to try out - right click on each link and use “Save As” to save the file on your computer in your new project folder.\n\n\nmydata.csv - CSV (comma separated value) formatted data\nmydata.xlsx - EXCEL file\nmydata.sav - SPSS Dataset\nmydata.sas7bdat - SAS Dataset\nMydata_Codebook.pdf - Codebook for “mydata” dataset\n\n\nAfter putting these files into your new RStudio project folder, you should see something like this now in your RStudio Files Listing (bottom right window pane):\n\n\n\nImporting Data\nNow that you’ve got some data in your RStudio project folder, let’s look at options for importing these datasets into your RStudio computing session.\nClick on “File/Import Dataset” - and then choose the file format you want.\nImport a CSV file\n\n\n\n\n\n\nWhat is a CSV file?\n\n\n\nCSV stands for “comma separated value” format. This format is what you would think - each value for a different column (or variable) is separated by a column and each new row represents a new record in the dataset.\nCSV is widely accepted as a “universal” standard as a data format for easy exchange between different software and databases.\n\nWikipedia Page on CSV\nLibrary of Congress Page on CSV\nThere is even a conference on CSV\n\n\n\n\nHere is an example of importing the mydata.csv - CSV formatted data. Let’s use the From Text (readr) option.\n\n\nWhy should we use the “from text” option? Why do I not see a CSV option?\nTechnically the CSV format is TEXT. You can open a CSV file in a text editor and easily read it - even if you do not have proprietary software like Excel, Access, SPSS, SAS, etc. Here is a screen shot of what the “mydata.csv” file looks like in my text editor “Notepad” on my Windows 11 computer:\nNotice that:\n\nThe first row has text labels for the “variables” (columns) in the dataset - there are 14 column labels with each value separated by a , comma.\nThe remaining rows are the “data” for the dataset.\nAfter the 1st row of labels, there are 21 rows of data.\nTake a minute and notice there are some odd values, and odd patterns of missing data (two commas ,, together indicate that value is missing for that column (variable)). We’ll explore these issues further in later lesson modules.\n\n\n\n\nOnce the “File/Import Data/From Text (readr)” opens, click on “Browse” and choose the mydata.csv file. Assuming all goes well, this window will read the top of the datafile and show you a quick “Data Preview” to check that the import will work.\nAnd on the bottom right, the “Code Preview” shows you the R code commands needed to import this dataset. You can then click on the little “clipboard” on the bottom right to copy this R code to your “clipboard”, (the R code option will be explained below).\nOR You can also just click “Import” and the R code will be executed for you and the dataset brought into your R computing session (but this is NOT a good practice for reproducible research!).\n\nThe better way is to save the R code commands to import the data so you will be able to reproduce all steps in your data analysis workflow using code as opposed to non-reproducible point-and-click steps.\nOnce you copied the R code above to your clipboard, go to “File/New File/R Script” to open a script programming window:\n\nAnd then “paste” your R code into this window.\nAs you can see importing the mydata.csv dataset, involves 2 steps:\n\nLoading the readr package into your RStudio computing session, by running library(readr)\n\nRunning the read_csv() function from the readr package and then assigning &lt;- this output into a new R data object called mydata.\n\n\nTo import the dataset, select these 2 lines of code and then click “Run” to run the R code. And be sure to click “Save” to save your first R program - for example “importdata.R”.\n\n\nAfter running these 2 lines of code, you should see something like this - the code messages in the bottom left “Console” window pane and a new R data object “mydata” in the top right “Global Environment” window pane.\n\n\nImport an EXCEL file\nLet’s try another format. While you will probably encounter CSV (comma separated value) data files often (since nearly all data collection platforms, databases and software will be able to export this simple non-proprietary format), many people natively open/read CSV files in the EXCEL software. So you will probably also encounter EXCEL (*.XLS or *.XLSX) formatted data files.\nIn addition to an EXCEL file using a Microsoft proprietary format, EXCEL files can have formatting (font sizes, colors, borders) and can have multiple TABs (or SHEETs). Here are some screen shots of the mydata.xlsx - EXCEL file file.\nThe first “Data” TAB:\n\n\nThe second “Codebook” TAB:\n\n\nTo import an EXCEL file into R, we will use the same process as above, but this time we will select “File/Import Dataset/From Excel”:\n\nThis process uses the read_excel() function from the readxl package.\nWith the read_excel() function, we can specify several options including:\n\nWhich TAB do you want to import (for now we are only importing one data TAB at a time). We are selecting the “Data” TAB.\nI’m leaving all of the rest as their defaults which include:\n\nnot changing the “Range”,\nleaving “Max Rows” blank,\nand leaving rows to “Skip” as 0, which can be useful if you receive files with a lot of “header” information at the top,,\nleaving the “NA” box blank - but you could put in a value like “99” if you want all 99’s treated as missing - but this is applied to the ENTIRE dataset. We will look at these issues for individual variables below.\n\n\nAlso notice that the checkboxes are selected for “First Row as Names” (which is the usual convention) and “Open Data Viewer”, which creates the View(mydata) in the “Code Preview” window to the right. You can skip this if you like.\n\nSo in the “Code Preview” window to the right, we have specified the name of the data file \"mydata.xlsx\" and the “Data” TAB using the option sheet = \"Data\". Remember to copy this code to the clipboard and save it in a *.R program script.\n\n\nHere is the importdata.R program script we have so far for reading in the \"mydata.csv\" and \"mydata.xlsx\" data files. At the moment, the second time we “create” the mydata R data object we are overwriting the previous one in the sequential code steps below.\nAlso notice I have added some comments which start with a # hashtag. Any text following a # will be ignored by R and not executed.\n\n\nImport SPSS data\nFor data files from other “common” statistics software like SPSS, SAS and Stata, we can use the “File/Import Dataset/From SPSS (or From SAS or From Stata)”. All of these use read_xxx() functions from the haven package.\n\nHere is the code generated to import a SPSS datafile:\n\n\nImport SAS data\nImporting a *.sas7bdat SAS datafile, is similar to SPSS - here is that code.\nNotice that in addition to the datafile \"mydata.sas7bdat\", the read_sas() function also shows NULL. When reading in a SAS file, you can also add arguments for the catalog file and encoding specifics. You can read more on the Help pages for the haven::read_sas() function.\n\n\n\nHere is a quick summary of all of the data import codes shown above importdata.R:\n\n\n\n\n\n\nUsing = equals for parameter options inside a function\n\n\n\nNotice that we used sheet = \"Data\" inside the readxl::read_excel() function. The single = equals sign is used to assign a value to a parameter or option inside a function.\n\n\n\n# Import the CSV file\nlibrary(readr)\nmydata &lt;- read_csv(\"mydata.csv\")\n\n# Import the EXCEL file\n# Choose the \"Data\" TAB\nlibrary(readxl)\nmydata &lt;- read_excel(\"mydata.xlsx\", sheet = \"Data\")\n\n# Import a SPSS file\nlibrary(haven)\nmydata &lt;- read_sav(\"mydata.sav\")\n\n# Import a SAS file\nlibrary(haven)\nmydata &lt;- read_sas(\"mydata.sas7bdat\", NULL)\n\n\n\n\n\n\n\nhaven and foreign packages\n\n\n\nIn addition to the haven package which is part of tidyverse and has been around since 2015, there is also another useful package for importing and exporting other statistical software formats that has been around since 1999 and it still being maintained - the foreign package.\nIn addition to SPSS and Stata, the foreign package also can read in other formats like DBF, EPI INFO, Minitab, Octave, SSD (SAS Permanent Datasets via XPORT) SYSTAT, and ARFF.\nCompare current downloads of these 2 packages at https://hadley.shinyapps.io/cran-downloads/.\nWe can also review the history of these 2 packages using the pkgsearch package and the cran_package_history() function.\n\n# optionally install pkgsearch\n# install.packages(\"pkgsearch\")\nlibrary(pkgsearch)\n\n# get history of haven package\nhavenhistory &lt;- cran_package_history(\"haven\")\n\n# get history of foreign package\nforeignhistory &lt;- cran_package_history(\"foreign\")\n\n# display the earliest date on CRAN\n# for these 2 packages\nhavenhistory$date[1]\n\n[1] \"2015-03-01T08:18:16+00:00\"\n\nforeignhistory$date[1]\n\n[1] \"1999-12-17T02:05:13+00:00\"\n\n\n\n\n\nExploring Built-in Datasets\nIf you are looking for other datasets to test out functions or just need some data to play around with, the base R packages and other R packages (like palmerpenguins) have data built-in to them. You can use these datasets.\nWe can take a look at what datasets are available using the data() function:\n\n# take a look at the datasets available in the\n# \"datasets\" base R package\ndata()\n\nThis will open a viewer window (top left) - also notice that if you search for “Help” on the pressure dataset, you get a description of the dataset and the original source and citation. Notice in the “Help” window, the word pressure is followed by curly brackets indicating that the pressure dataset is in the built-in R package {datasets}.\n\n\nWe can see the pressure dataset is indeed in the datasets package if we keep scrolling down in the viewer window - also notice the mtcars dataset which you will often find in R tutorials and coding examples.\n\n\nOnce you know where to look, you can then explore lots of these datasets. For example, we can take a look at the built-in pressure dataset, which includes 19 values showing the relationship between temperature in degrees Celsius and pressure in mm (or mercury). To “see” this built-in data object, just type the name pressure to see (or print out) the object.\n\npressure\n\n   temperature pressure\n1            0   0.0002\n2           20   0.0012\n3           40   0.0060\n4           60   0.0300\n5           80   0.0900\n6          100   0.2700\n7          120   0.7500\n8          140   1.8500\n9          160   4.2000\n10         180   8.8000\n11         200  17.3000\n12         220  32.1000\n13         240  57.0000\n14         260  96.0000\n15         280 157.0000\n16         300 247.0000\n17         320 376.0000\n18         340 558.0000\n19         360 806.0000\n\n\nNormally most datasets are much larger than this little dataset. So, I would not advise trying to view most datasets by printing them to the “Console” window pane. Instead you can either click on the object in your “Global Environment” to view it - or you can run the View() function to open the viewer window.\nYou can “load” the built-in pressure dataset using the data(pressure) function to load the pressure dataset to load into your “Global Environment”, which loads the dataset into your R session.\n\nIf we click on the little “Table icon” all the way to the right of the pressure dataset in the “Global Environment” window - or run View(pressure) - we can open the dataset in the Viewer window:\n\ndata(pressure)\nView(pressure)\n\n\n\n\n\n\n\n\nExplore Datasets in R Packages\n\n\n\nI encourage you to use the data(package = \"xxx\") function to see what, if any, datasets may be built-in to the various packages you may install and load during your R computing sessions.\n\n\n\nIf you are interested in seeing other datasets in other R packages, go ahead and install the palmerpenguins package and take a look at the penguins dataset included:\n\n# look at datasets included with the\n# palmerpenguins dataset\ndata(package = \"palmerpenguins\")\n\nYou can learn more about the penguins dataset, by opening up the “Help” page for the dataset. You can also load the palmerpenguins package and then load the penguins dataset using this code.\n\nhelp(penguins, package = \"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(penguins)\n\n\n\nAnd clicking the the little data table icon after loading the penguins dataset into the “Global Environment”, you can see the dataset in the viewer window.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-view-the-data.",
    "href": "module132_DataWrangling.html#to-view-the-data.",
    "title": "1.3.2: Data Wrangling",
    "section": "2. To view The Data.",
    "text": "2. To view The Data.\nLook at small data in Console\nLet’s work with the mydata dataset that we imported above using the readr::read_csv() function.\n\n# import the mydata.csv dataset\nmydata &lt;- readr::read_csv(\"mydata.csv\")\n\nThis is not a very large dataset - mydata has 21 rows (or observations) and 14 variables (or columns). So, we can view the whole thing by printing it to the “Console” window.\nYou’ll notice that depending on the size of your current “Console” window, font size, zoom settings and more, what you see may vary. Since we read this dataset in using the readr package, the data object is now a “tibble” dataframe which only shows the columns and rows that will reasonably show up in your “Console” window.\n\n\n\n\n\n\nWhat is a “tibble” tbl_df?\n\n\n\nAs stated on the homepage for the tibble package at https://tibble.tidyverse.org/, a “tibble” is\n\n“… a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not.”\n\nAlso a “tibble” has\n\n“… an enhanced print() method which makes them easier to use with large datasets containing complex objects.”\n\n\n\nAnd the output below also lists what kind of column each variable is. For example,\n\n\nAge is a &lt;dbl&gt; indicating it is a numeric variable saved using double-precision, whereas\n\nGenderSTR is &lt;chr&gt; indicating this is a text or character (or “string”) type variable.\n\n\n# print the dataset into the Console\nmydata\n\n# A tibble: 21 × 14\n   SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n 1         1    45        68        145    5.6     9 m                   1     4\n 2         2    50       167        166    5.4     2 f                   2     3\n 3         3    35       143        135    5.6     2 &lt;NA&gt;               NA     3\n 4         4    44       216        201    5.6     2 m                   1     4\n 5         5    32       243        223    6       2 m                   1     5\n 6         6    48       165        145    5.2     2 f                   2     2\n 7         8    50        60        132    3.3     2 m                   1     3\n 8         9    51       110        108    5.1     3 f                   2     1\n 9        12    46       167        158    5.5     2 F                   2     1\n10        14    35       190        200    5.8     1 Male                1     4\n# ℹ 11 more rows\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n\n\nLook the “structure” of the dataset\nYou can also view the different kinds of variables in the dataset using the str() or “structure” function - which lists the type of variable, the number of elements in each column [1:21] indicates each column has 21 elements (or 21 rows) and the other values are a quick “peek” at the data inside the dataset. For example, the first 3 people in this dataset are ages 45, 50 and 35.\n\nstr(mydata)\n\nspc_tbl_ [21 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ SubjectID  : num [1:21] 1 2 3 4 5 6 8 9 12 14 ...\n $ Age        : num [1:21] 45 50 35 44 32 48 50 51 46 35 ...\n $ WeightPRE  : num [1:21] 68 167 143 216 243 165 60 110 167 190 ...\n $ WeightPOST : num [1:21] 145 166 135 201 223 145 132 108 158 200 ...\n $ Height     : num [1:21] 5.6 5.4 5.6 5.6 6 5.2 3.3 5.1 5.5 5.8 ...\n $ SES        : num [1:21] 9 2 2 2 2 2 2 3 2 1 ...\n $ GenderSTR  : chr [1:21] \"m\" \"f\" NA \"m\" ...\n $ GenderCoded: num [1:21] 1 2 NA 1 1 2 1 2 2 1 ...\n $ q1         : num [1:21] 4 3 3 4 5 2 3 1 1 4 ...\n $ q2         : num [1:21] NA 4 4 2 3 5 NA 4 1 44 ...\n $ q3         : num [1:21] NA 1 2 2 5 5 4 1 5 1 ...\n $ q4         : num [1:21] 4 40 3 1 2 1 3 3 5 1 ...\n $ q5         : num [1:21] 4 3 5 1 4 4 9 1 1 4 ...\n $ q6         : num [1:21] 5 2 2 9 1 5 2 4 2 5 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   SubjectID = col_double(),\n  ..   Age = col_double(),\n  ..   WeightPRE = col_double(),\n  ..   WeightPOST = col_double(),\n  ..   Height = col_double(),\n  ..   SES = col_double(),\n  ..   GenderSTR = col_character(),\n  ..   GenderCoded = col_double(),\n  ..   q1 = col_double(),\n  ..   q2 = col_double(),\n  ..   q3 = col_double(),\n  ..   q4 = col_double(),\n  ..   q5 = col_double(),\n  ..   q6 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nYou can also interactively View the data by clicking on the data icon and you can also click the little “table” icon to the far right next to the dataset in the “Global Environment”to open the data viewer window on the left.\nYou can also click on the little blue circle to the left of the mydata dataset to change the arrow from facing right  to facing down  to see the “structure” of the data in the “Global Environment”.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-subset-the-data---select-and-filter.",
    "href": "module132_DataWrangling.html#to-subset-the-data---select-and-filter.",
    "title": "1.3.2: Data Wrangling",
    "section": "3. To subset the data - select and filter.",
    "text": "3. To subset the data - select and filter.\nUsing base R packages and functions\nView parts of the dataset\nNow let’s “explore” the data by viewing sections of it.\nUsing base R commands, we can use functions like head() and tail() with each showing either the top or bottom 6 rows of the dataset. We can add a number to the function call to see more or less rows if we wish.\n\n# look at top 6 rows of data\nhead(mydata)\n\n# A tibble: 6 × 14\n  SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1         1    45        68        145    5.6     9 m                   1     4\n2         2    50       167        166    5.4     2 f                   2     3\n3         3    35       143        135    5.6     2 &lt;NA&gt;               NA     3\n4         4    44       216        201    5.6     2 m                   1     4\n5         5    32       243        223    6       2 m                   1     5\n6         6    48       165        145    5.2     2 f                   2     2\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n# look at the bottom 10 rows of data\ntail(mydata, n=10)\n\n# A tibble: 10 × 14\n   SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n 1        19    40       200        195    6.1     1 f                   2     1\n 2        21    99       180        185    5.9     3 f                   2     2\n 3        22    52       240        220    6.5     2 m                   1     2\n 4        23    24       250        240    6.4     2 M                   1     5\n 5        24    35       175        174    5.8     2 F                   2     5\n 6        27    51       220        221    6.3     2 m                   1     4\n 7        28    43       230         98    2.6     2 m                   1    11\n 8        30    36       190        180    5.7     1 female              2     5\n 9        32    44       260        109    6.4     3 male                1     1\n10        NA    NA        NA         NA   NA      NA &lt;NA&gt;               NA    NA\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nWhat are these wierd NAs?\n\n\n\nThe NA letters that show up is how R stores missing data. If the dataset you import has a blank cell (for either numeric or character type data), then R interprets that as “not available” which is indicated by NA. NA is a reserved word in R specifically set aside for handling missing values. \nYou can learn more about NA by running:\n\nhelp(NA, package = \"base\")\n\n\n\nYou can also view different parts of the data by using square brackets [] to select specific rows and columns using [row, column] index indicators.\n\n# Select the values in rows 1-4\n# and in columns 1-3\nmydata[1:4, 1:3]\n\n# A tibble: 4 × 3\n  SubjectID   Age WeightPRE\n      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1         1    45        68\n2         2    50       167\n3         3    35       143\n4         4    44       216\n\n\nTo select all of a given row or column just leave that index blank.\n\n# show all of rows 1-2\nmydata[1:2, ]\n\n# A tibble: 2 × 14\n  SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1         1    45        68        145    5.6     9 m                   1     4\n2         2    50       167        166    5.4     2 f                   2     3\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n# show all of columns 3-4\nmydata[ ,3:4]\n\n# A tibble: 21 × 2\n   WeightPRE WeightPOST\n       &lt;dbl&gt;      &lt;dbl&gt;\n 1        68        145\n 2       167        166\n 3       143        135\n 4       216        201\n 5       243        223\n 6       165        145\n 7        60        132\n 8       110        108\n 9       167        158\n10       190        200\n# ℹ 11 more rows\n\n\n\nView variables in dataset by name\nWe can also select columns from a dataset using the variable (or column) name. To see the names of all of the variables in a dataset, use the names() function.\n\n# list variable names in mydata\nnames(mydata)\n\n [1] \"SubjectID\"   \"Age\"         \"WeightPRE\"   \"WeightPOST\"  \"Height\"     \n [6] \"SES\"         \"GenderSTR\"   \"GenderCoded\" \"q1\"          \"q2\"         \n[11] \"q3\"          \"q4\"          \"q5\"          \"q6\"         \n\n\nWe can use the $ “dollar sign” operator to “select” named variables out of a dataset. Let’s look at all of the ages in mydata.\n\n# look at all of the ages\n# of the 21 people in mydata\nmydata$Age\n\n [1] 45 50 35 44 32 48 50 51 46 35 36 40 99 52 24 35 51 43 36 44 NA\n\n\nWe can also use these variable names with the [] brackets in base R syntax. And we use the c() combine function to help us put a list together. Let’s look at the 2 weight columns in the dataset. Put the variable names inside \"\" double quotes.\n\n# show all rows for\n# the 2 weight variables in mydata\nmydata[ , c(\"WeightPRE\", \"WeightPOST\")]\n\n# A tibble: 21 × 2\n   WeightPRE WeightPOST\n       &lt;dbl&gt;      &lt;dbl&gt;\n 1        68        145\n 2       167        166\n 3       143        135\n 4       216        201\n 5       243        223\n 6       165        145\n 7        60        132\n 8       110        108\n 9       167        158\n10       190        200\n# ℹ 11 more rows\n\n\n\nUsing dplyr functions\nUsing tidyverse packages and functions\nAs you can see while base R is very powerful on it’s own, the syntax is less than intuitive. There is a whole suite of R packages that are designed to work together and use a different syntax that improves programming workflow and readability.\nLearn more about the suite of tidyverse packages. You’ve already used two of these, readr and haven are both part of tidyverse for importing datasets.\nAnother one of these tidyverse packages, dplyr is a very good package for “data wrangling”.\nPick columns using dplyr::select()\nInstead of using the base R $ selector, the dplyr package has a select() function where you simply choose variables using their name. Let’s look at Height and q1 from the mydata dataset.\n\n\n\n\n\n\nUsing package::function() syntax\n\n\n\nIt is good coding practice, especially when loading several packages at once into your computing session, to make sure you are calling the exact function you want from a specific package. So, I’m using the syntax of package::function() to help keep track of which package and which function is being used below.\n\n\n\n# load dplyr package\nlibrary(dplyr)\n\n# select Height and q1 from mydata\ndplyr::select(mydata, c(Height, q1))\n\n# A tibble: 21 × 2\n   Height    q1\n    &lt;dbl&gt; &lt;dbl&gt;\n 1    5.6     4\n 2    5.4     3\n 3    5.6     3\n 4    5.6     4\n 5    6       5\n 6    5.2     2\n 7    3.3     3\n 8    5.1     1\n 9    5.5     1\n10    5.8     4\n# ℹ 11 more rows\n\n\nWorkflow using the pipe %&gt;% operator\nAnother improvement of the tidyverse approach of R programming is to use the pipe %&gt;% operator. Basically what this syntax does is take the results from “A” and pipe it into –&gt; the next “B” function, e.g. A %&gt;% B so we can begin to “daisy-chain” a sequence of programming steps together into a logical workflow that is easy to “read” and follow.\nHere is a working example to show the same variable selection process we did above, but now we will be using the dplyr::select() function. The code below takes the mydata dataset and pipes %&gt;% it into the select() function. We were also able to drop using the c() function here.\n\n# start with mydata and then \n# select Height and q1 from mydata\nmydata %&gt;% dplyr::select(Height, q1)\n\n# A tibble: 21 × 2\n   Height    q1\n    &lt;dbl&gt; &lt;dbl&gt;\n 1    5.6     4\n 2    5.4     3\n 3    5.6     3\n 4    5.6     4\n 5    6       5\n 6    5.2     2\n 7    3.3     3\n 8    5.1     1\n 9    5.5     1\n10    5.8     4\n# ℹ 11 more rows\n\n\nWe could even add the base R head() function here. If we put each code step on a separate line, you can now see that we are [1] taking the mydata dataset “and then” [2] selecting 2 variables “and then” [3] looking at the top 6 rows of the dataset.\n\n# select Height and q1 from mydata\n# and show only the top 6 rows\nmydata %&gt;% \n  dplyr::select(Height, q1) %&gt;%\n  head()\n\n# A tibble: 6 × 2\n  Height    q1\n   &lt;dbl&gt; &lt;dbl&gt;\n1    5.6     4\n2    5.4     3\n3    5.6     3\n4    5.6     4\n5    6       5\n6    5.2     2\n\n\n\n\n\n\n\n\nTL;DR If %&gt;% is a pipe, then what is |&gt;??\n\n\n\nThe %&gt;% pipe operator is implemented within tidyverse from the magrittr package which is used by the tidyverse packages which started being used quite extensively by R programmers over the last decade.\nHowever, the rest of the R development community (which is much larger than just those who use the tidyverse suite) also recently added a new base R pipe operator |&gt; (since R version 4.1.0).\nLearn more in this tidyverse blog post from 2023\n\n\nSo, you do have the option to also use the base R |&gt; pipe operator.\n\n# select Height and q1 from mydata\n# and show only the top 6 rows\nmydata |&gt; \n  dplyr::select(Height, q1) |&gt;\n  head()\n\n# A tibble: 6 × 2\n  Height    q1\n   &lt;dbl&gt; &lt;dbl&gt;\n1    5.6     4\n2    5.4     3\n3    5.6     3\n4    5.6     4\n5    6       5\n6    5.2     2\n\n\nFor now, we will stay with the %&gt;% operator for consistency. But be aware that you will see both approaches on the Internet when “Googling” for answers.\nSelect variables with matching using starts_with()\nWhen using dplyr::select() to select variables, there are several “helper functions” that are useful for “selection”. You can see a list of these functions by running help(\"starts_with\", package = \"tidyselect\"). These “selection helper” functions are actually in the tidyselect package which is loaded with the dplyr package.\nLet’s use these functions to pull out all of the Likert-scaled “question” variables that start with the letter \"q\".\n\nmydata %&gt;%\n  dplyr::select(starts_with(\"q\"))\n\n# A tibble: 21 × 6\n      q1    q2    q3    q4    q5    q6\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     4    NA    NA     4     4     5\n 2     3     4     1    40     3     2\n 3     3     4     2     3     5     2\n 4     4     2     2     1     1     9\n 5     5     3     5     2     4     1\n 6     2     5     5     1     4     5\n 7     3    NA     4     3     9     2\n 8     1     4     1     3     1     4\n 9     1     1     5     5     1     2\n10     4    44     1     1     4     5\n# ℹ 11 more rows\n\n\n\nPick rows using dplyr::filter()\nIn addition to selecting columns or variables from your dataset, you can also pull out a subset of your data by “filtering” out only the rows you want.\nFor example, suppose we only want to look at the Age, WeightPRE for the Females in the dataset indicates by GenderCoded equal to 2.\nFor reference, take a look at the mydata codebook - and here is a screenshot as well:\n\n\nNotice that:\n\nI changed the order of the columns, which is OK,\nand to filter out and KEEP only the rows for females, I typed GenderCoded == 2 using two equal signs ==. R uses two == equal signs to perform a logical operation to ask does the variable GenderCoded equal the value of 2, with either a TRUE or FALSE result. Only the rows with a TRUE result are shown.\n\n\n\n\n\n\n\nBe careful not to mix up = and ==\n\n\n\nOdds are you will get errors at some point due to typos or other issues, but a common error is to use a single = equals sign when trying to perform a logic operation. Remember to use 2 equals signs == if you are trying to perform a TRUE/FALSE operation and use only 1 equals sign = when assigning a value to a function argument.\n\n\n\n# select columns from mydata\n# and then only show rows for females\nmydata %&gt;%\n  select(GenderCoded, Age, WeightPRE) %&gt;%\n  filter(GenderCoded == 2)\n\n# A tibble: 8 × 3\n  GenderCoded   Age WeightPRE\n        &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1           2    50       167\n2           2    48       165\n3           2    51       110\n4           2    46       167\n5           2    40       200\n6           2    99       180\n7           2    35       175\n8           2    36       190\n\n\nHere is an example of the error you will get if you use a single = sign instead of == two.\n\n# select columns from mydata\n# and then only show rows for females\nmydata %&gt;%\n  select(GenderCoded, Age, WeightPRE) %&gt;%\n  filter(GenderCoded = 2)\n\nError in `filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `GenderCoded == 2`?\nRun `rlang::last_trace()` to see where the error occurred.\n\nFilter rows using matching %in% operator\nAnother helpful operator in R is the %in% operator used for matching. Let’s suppose we wanted to pull out the rows for specific subject IDs - perhaps you want to review only these records.\nLet’s pull out the data for only IDs 14, 21 and 24. Rather than writing a complicated if-then-else set of code steps, we can search for these IDs and only the rows with these IDs will be kept.\n\nmydata %&gt;%\n  filter(SubjectID %in% c(14, 21, 24))\n\n# A tibble: 3 × 14\n  SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1        14    35       190        200    5.8     1 Male                1     4\n2        21    99       180        185    5.9     3 f                   2     2\n3        24    35       175        174    5.8     2 F                   2     5\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n\nSort/arrange rows using dplyr::arrange()\nHere is another helpful function from dplyr. Suppose we want to find the 5 oldest people in mydata and show their IDs.\nLet’s use the dplyr::arrange() function which will sort our data based on the variable we specify in increasing order (lowest to highest) by default. We will add the desc() function to sort decreasing from largest to smallest.\nLearn more by running help(arrange, package = \"dplyr\")\nNote: There was someone with age 99 in this made-up dataset.\n\n# take mydata\n# select SubjectID and Age\n# sort descending by Age\n# show the top 5 IDs and Ages\nmydata %&gt;%\n  select(SubjectID, Age) %&gt;%\n  arrange(desc(Age)) %&gt;%\n  head(n=5)\n\n# A tibble: 5 × 2\n  SubjectID   Age\n      &lt;dbl&gt; &lt;dbl&gt;\n1        21    99\n2        22    52\n3         9    51\n4        27    51\n5         2    50\n\n\nThe oldest people are subject IDs 21, 22, 9, 27 and 2 who are age 99, 52, 51, 51 and 50 years old respectively.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-create-and-modify-variables.",
    "href": "module132_DataWrangling.html#to-create-and-modify-variables.",
    "title": "1.3.2: Data Wrangling",
    "section": "4. To create and modify variables.",
    "text": "4. To create and modify variables.\nTo create and add new variables to the dataset, we can use either a base R approach or use the mutate() function from the dplyr package. Let’s take a look at both approaches. In the mydata dataset, we have Height in decimal feet and we have WeightPRE and WeightPOST in pounds.\nSo, let’s compute BMI (body mass index) as follows from Height (in inches) and Weight (in pounds):\n\\[BMI = \\left(\\frac{weight_{(lbs)}}{(height_{(inches)})^2}\\right) * 703\\]\nCreate New Variable - Base R Approach\nCreate a new variable using the $ selector operator. Then write out the mathematical equation. I also had to multiply the height in decimal feet * 12 to get inches.\n\n# Compute BMI for the PRE Weight\nmydata$bmiPRE &lt;- \n  (mydata$WeightPRE * 703) / (mydata$Height * 12)^2\n\n# look at result\nmydata$bmiPRE\n\n [1]  10.58585  27.95901  22.26142  33.62564  32.95312  29.78997  26.89777\n [8]  20.64644  26.95156  27.57341  29.21039  26.23996  25.24418  27.73176\n[15]  29.79702  25.39656  27.06041 166.10166  28.54938  30.98891        NA\n\n\nLook at the “Global Environment” or run the str() function to see if a new variable was added to mydata - which should now have 15 variables instead of only 14.\nYou can also list the variable names in the updated dataset.\n\n# look at updated data structure\nstr(mydata)\n\nspc_tbl_ [21 × 15] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ SubjectID  : num [1:21] 1 2 3 4 5 6 8 9 12 14 ...\n $ Age        : num [1:21] 45 50 35 44 32 48 50 51 46 35 ...\n $ WeightPRE  : num [1:21] 68 167 143 216 243 165 60 110 167 190 ...\n $ WeightPOST : num [1:21] 145 166 135 201 223 145 132 108 158 200 ...\n $ Height     : num [1:21] 5.6 5.4 5.6 5.6 6 5.2 3.3 5.1 5.5 5.8 ...\n $ SES        : num [1:21] 9 2 2 2 2 2 2 3 2 1 ...\n $ GenderSTR  : chr [1:21] \"m\" \"f\" NA \"m\" ...\n $ GenderCoded: num [1:21] 1 2 NA 1 1 2 1 2 2 1 ...\n $ q1         : num [1:21] 4 3 3 4 5 2 3 1 1 4 ...\n $ q2         : num [1:21] NA 4 4 2 3 5 NA 4 1 44 ...\n $ q3         : num [1:21] NA 1 2 2 5 5 4 1 5 1 ...\n $ q4         : num [1:21] 4 40 3 1 2 1 3 3 5 1 ...\n $ q5         : num [1:21] 4 3 5 1 4 4 9 1 1 4 ...\n $ q6         : num [1:21] 5 2 2 9 1 5 2 4 2 5 ...\n $ bmiPRE     : num [1:21] 10.6 28 22.3 33.6 33 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   SubjectID = col_double(),\n  ..   Age = col_double(),\n  ..   WeightPRE = col_double(),\n  ..   WeightPOST = col_double(),\n  ..   Height = col_double(),\n  ..   SES = col_double(),\n  ..   GenderSTR = col_character(),\n  ..   GenderCoded = col_double(),\n  ..   q1 = col_double(),\n  ..   q2 = col_double(),\n  ..   q3 = col_double(),\n  ..   q4 = col_double(),\n  ..   q5 = col_double(),\n  ..   q6 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# list the variable names in the\n# updated dataset\nnames(mydata)\n\n [1] \"SubjectID\"   \"Age\"         \"WeightPRE\"   \"WeightPOST\"  \"Height\"     \n [6] \"SES\"         \"GenderSTR\"   \"GenderCoded\" \"q1\"          \"q2\"         \n[11] \"q3\"          \"q4\"          \"q5\"          \"q6\"          \"bmiPRE\"     \n\n\n\nCreate New Variable - dplyr::mutate() Approach\nIn the dplyr package, you can create or modify variables using the mutate() function.\n\n# Compute BMI for the POST Weight\n# use the dplyr::mutate() function\nmydata &lt;- mydata %&gt;%\n  mutate(\n    bmiPOST = (WeightPOST * 703) / (Height * 12)^2\n    )\n\n# check updates\nstr(mydata)\n\ntibble [21 × 16] (S3: tbl_df/tbl/data.frame)\n $ SubjectID  : num [1:21] 1 2 3 4 5 6 8 9 12 14 ...\n $ Age        : num [1:21] 45 50 35 44 32 48 50 51 46 35 ...\n $ WeightPRE  : num [1:21] 68 167 143 216 243 165 60 110 167 190 ...\n $ WeightPOST : num [1:21] 145 166 135 201 223 145 132 108 158 200 ...\n $ Height     : num [1:21] 5.6 5.4 5.6 5.6 6 5.2 3.3 5.1 5.5 5.8 ...\n $ SES        : num [1:21] 9 2 2 2 2 2 2 3 2 1 ...\n $ GenderSTR  : chr [1:21] \"m\" \"f\" NA \"m\" ...\n $ GenderCoded: num [1:21] 1 2 NA 1 1 2 1 2 2 1 ...\n $ q1         : num [1:21] 4 3 3 4 5 2 3 1 1 4 ...\n $ q2         : num [1:21] NA 4 4 2 3 5 NA 4 1 44 ...\n $ q3         : num [1:21] NA 1 2 2 5 5 4 1 5 1 ...\n $ q4         : num [1:21] 4 40 3 1 2 1 3 3 5 1 ...\n $ q5         : num [1:21] 4 3 5 1 4 4 9 1 1 4 ...\n $ q6         : num [1:21] 5 2 2 9 1 5 2 4 2 5 ...\n $ bmiPRE     : num [1:21] 10.6 28 22.3 33.6 33 ...\n $ bmiPOST    : num [1:21] 22.6 27.8 21 31.3 30.2 ...\n\nnames(mydata)\n\n [1] \"SubjectID\"   \"Age\"         \"WeightPRE\"   \"WeightPOST\"  \"Height\"     \n [6] \"SES\"         \"GenderSTR\"   \"GenderCoded\" \"q1\"          \"q2\"         \n[11] \"q3\"          \"q4\"          \"q5\"          \"q6\"          \"bmiPRE\"     \n[16] \"bmiPOST\"    \n\n\n\nCreate New Variable - add labels to codes by creating a “factor” type variable\nAs you probably noticed in the views of the mydata dataset above, there was originally a variable where people were allowed to enter their gender using free text (the GenderSTR variable). There were entries like “f”, “F”, “female”, “male”, “Male” and other variations. So, another variable GenderCoded was included where 1=male and 2=female, but when we look at mydata$GenderCoded all we see are 1’s and 2’s and NAs.\n\nmydata$GenderCoded\n\n [1]  1  2 NA  1  1  2  1  2  2  1  1  2  2  1  1  2  1  1  2  1 NA\n\n\nIt would be nice if we could add some labels. One way to do this is to convert GenderCoded from being a simple “numeric” variable to a new object class called a “factor” which includes both numeric values and text labels.\nHere is the base R approach to create a new factor type variable. Learn more by looking at the help page for factor(), run help(factor, package = \"base\").\n\n# create a new factor with labels\nmydata$GenderCoded.f &lt;-\n  factor(mydata$GenderCoded,\n         levels = c(1, 2),\n         labels = c(\"Male\", \"Female\"))\n\n# look at new variable\nmydata$GenderCoded.f\n\n [1] Male   Female &lt;NA&gt;   Male   Male   Female Male   Female Female Male  \n[11] Male   Female Female Male   Male   Female Male   Male   Female Male  \n[21] &lt;NA&gt;  \nLevels: Male Female\n\n\nWe can check the type each variable using the class() function.\n\nclass(mydata$GenderCoded)\n\n[1] \"numeric\"\n\nclass(mydata$GenderCoded.f)\n\n[1] \"factor\"\n\n\nAnother quick way to see these class type differences is to use the table() function to get the frequencies of each distinct value. I’m also adding the useNA = \"ifany\" option to also get a count of any missing values. Learn more by running help(table, package = \"base\").\n\n# table of frequencies of GenderCoded - numeric class\ntable(mydata$GenderCoded, useNA = \"ifany\")\n\n\n   1    2 &lt;NA&gt; \n  11    8    2 \n\n# table of GenderCoded.f - factor class\ntable(mydata$GenderCoded.f, useNA = \"ifany\")\n\n\n  Male Female   &lt;NA&gt; \n    11      8      2",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-get-data-summary-and-descriptive-statistics.",
    "href": "module132_DataWrangling.html#to-get-data-summary-and-descriptive-statistics.",
    "title": "1.3.2: Data Wrangling",
    "section": "5. To get data summary and descriptive statistics.",
    "text": "5. To get data summary and descriptive statistics.\nGetting summary statistics\nsummary() function\nOne of the best functions that is part of base R is the summary() function. Let’s see what this gives us for the mydata dataset.\nAs you can see for all of the numeric class variables, the summary() function gives us the min, max, median, mean, 1st quartile and 3rd quartile and a count of the the number of missing NAs. So, you can see the mean Age is 44.8 and the median Age is 44.0.\nFor the character variable GenderSTR all we know is it has a length of 21.\nBut for the factor type variable GenderCoded.f we get the number of Males, Females and NAs.\n\nsummary(mydata)\n\n   SubjectID          Age          WeightPRE       WeightPOST   \n Min.   : 1.00   Min.   :24.00   Min.   : 60.0   Min.   : 98.0  \n 1st Qu.: 5.75   1st Qu.:35.75   1st Qu.:166.5   1st Qu.:142.5  \n Median :15.00   Median :44.00   Median :190.0   Median :177.0  \n Mean   :15.30   Mean   :44.80   Mean   :185.2   Mean   :172.2  \n 3rd Qu.:23.25   3rd Qu.:50.00   3rd Qu.:230.0   3rd Qu.:203.2  \n Max.   :32.00   Max.   :99.00   Max.   :260.0   Max.   :240.0  \n NA's   :1       NA's   :1       NA's   :1       NA's   :1      \n     Height           SES       GenderSTR          GenderCoded   \n Min.   :2.600   Min.   :1.0   Length:21          Min.   :1.000  \n 1st Qu.:5.475   1st Qu.:2.0   Class :character   1st Qu.:1.000  \n Median :5.750   Median :2.0   Mode  :character   Median :1.000  \n Mean   :5.550   Mean   :2.3                      Mean   :1.421  \n 3rd Qu.:6.125   3rd Qu.:2.0                      3rd Qu.:2.000  \n Max.   :6.500   Max.   :9.0                      Max.   :2.000  \n NA's   :1       NA's   :1                        NA's   :2      \n       q1              q2               q3             q4        \n Min.   : 1.00   Min.   : 1.000   Min.   :1.00   Min.   : 1.000  \n 1st Qu.: 1.75   1st Qu.: 2.000   1st Qu.:1.00   1st Qu.: 2.000  \n Median : 3.00   Median : 4.000   Median :3.00   Median : 3.000  \n Mean   : 3.35   Mean   : 5.526   Mean   :3.15   Mean   : 5.062  \n 3rd Qu.: 4.25   3rd Qu.: 4.500   3rd Qu.:4.25   3rd Qu.: 4.000  \n Max.   :11.00   Max.   :44.000   Max.   :9.00   Max.   :40.000  \n NA's   :1       NA's   :2        NA's   :1      NA's   :5       \n       q5               q6            bmiPRE          bmiPOST     \n Min.   : 1.000   Min.   :1.000   Min.   : 10.59   Min.   :12.99  \n 1st Qu.: 2.000   1st Qu.:2.000   1st Qu.: 26.03   1st Qu.:25.38  \n Median : 4.000   Median :4.000   Median : 27.65   Median :26.42  \n Mean   : 9.176   Mean   :3.706   Mean   : 33.78   Mean   :29.43  \n 3rd Qu.: 5.000   3rd Qu.:5.000   3rd Qu.: 29.79   3rd Qu.:28.71  \n Max.   :99.000   Max.   :9.000   Max.   :166.10   Max.   :70.77  \n NA's   :4        NA's   :4       NA's   :1        NA's   :1      \n GenderCoded.f\n Male  :11    \n Female: 8    \n NA's  : 2    \n              \n              \n              \n              \n\n\nSo, the summary() function is helpful, but you’ll notice we do not get the standard deviation. For some reason that was left out of the original summary() statistics function.\nThere are a few other descriptive statistics functions that can be useful. There is a describe() function in both the Hmisc package and the psych packages.\nHmisc::describe() function\nLet’s look at Hmisc::describe() for a couple of the variables.\nYou’ll notice that this still doesn’t give us the standard deviation, but we get the min, max, mean, median, as well as the .05 (5th percentile) and others, and the output includes a summary of the frequency of the distinct values.\n\nmydata %&gt;%\n  select(Age, GenderCoded.f, bmiPRE) %&gt;%\n  Hmisc::describe()\n\n. \n\n 3  Variables      21  Observations\n--------------------------------------------------------------------------------\nAge \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n      20        1       14    0.994     44.8       43    13.81    31.60 \n     .10      .25      .50      .75      .90      .95 \n   34.70    35.75    44.00    50.00    51.10    54.35 \n                                                                           \nValue        24   32   35   36   40   43   44   45   46   48   50   51   52\nFrequency     1    1    3    2    1    1    2    1    1    1    2    2    1\nProportion 0.05 0.05 0.15 0.10 0.05 0.05 0.10 0.05 0.05 0.05 0.10 0.10 0.05\n               \nValue        99\nFrequency     1\nProportion 0.05\n\nFor the frequency table, variable is rounded to the nearest 0\n--------------------------------------------------------------------------------\nGenderCoded.f \n       n  missing distinct \n      19        2        2 \n                        \nValue        Male Female\nFrequency      11      8\nProportion  0.579  0.421\n--------------------------------------------------------------------------------\nbmiPRE \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n      20        1       20        1    33.78    27.73    18.52    20.14 \n     .10      .25      .50      .75      .90      .95 \n   22.10    26.03    27.65    29.79    33.02    40.25 \n\n10.5858489229025 (1, 0.05), 20.6464394036482 (1, 0.05), 22.2614175878685 (1,\n0.05), 25.2441827061189 (1, 0.05), 25.3965599815035 (1, 0.05), 26.2399593896503\n(1, 0.05), 26.8977655341292 (1, 0.05), 26.9515610651974 (1, 0.05),\n27.0604126424232 (1, 0.05), 27.5734079799181 (1, 0.05), 27.7317554240631 (1,\n0.05), 27.9590096784027 (1, 0.05), 28.5493827160494 (1, 0.05), 29.2103855937103\n(1, 0.05), 29.7899716469428 (1, 0.05), 29.7970241970486 (1, 0.05),\n30.9889051649305 (1, 0.05), 32.953125 (1, 0.05), 33.6256377551021 (1, 0.05),\n166.101660092045 (1, 0.05)\n\nFor the frequency table, variable is rounded to the nearest 0\n--------------------------------------------------------------------------------\n\n\n\npsych::describe() function\nThe psych::describe() function only works on numeric data. So, let’s look at Age and bmiPRE. This function now gives us the standard deviation sd and even the mad which is the mean absolute deviation.\n\nmydata %&gt;%\n  select(Age, bmiPRE) %&gt;%\n  psych::describe()\n\n       vars  n  mean    sd median trimmed   mad   min   max  range skew\nAge       1 20 44.80 14.87  44.00   43.06 10.38 24.00  99.0  75.00 2.21\nbmiPRE    2 20 33.78 31.53  27.65   27.79  3.17 10.59 166.1 155.52 3.66\n       kurtosis   se\nAge        6.10 3.32\nbmiPRE    12.53 7.05\n\n\nBase R specific statistics functions\nThere are many built-in functions in base R for computing specific statistics like mean(), sd() for standard deviation, median(), min(), max() and quantile() to get specific percentiles.\nLet get some summary statistics for different variables in mydata.\n\n# get min, max for Age\nmin(mydata$Age)\n\n[1] NA\n\nmax(mydata$Age)\n\n[1] NA\n\n\nWAIT!? - why did I get NA? Since there is missing data in this dataset, we need to tell these R functions how to handle the missing data. We need to add na.rm=TRUE to remove the NAs and then compute the min() and max() for the non-missing values.\n\nmin(mydata$Age, na.rm = TRUE)\n\n[1] 24\n\nmax(mydata$Age, na.rm = TRUE)\n\n[1] 99\n\n\nIf we want, we could get the non-parametric statistics of median (which is the 50th percentile), 25th and 75th percentiles for the interquartile range. Let’s get these statistics for bmiPRE.\n\n# get median bmiPRE\n# and 25th and 75th percentiles for bmiPRE\nmedian(mydata$bmiPRE,\n       na.rm = TRUE)\n\n[1] 27.65258\n\nquantile(mydata$bmiPRE, \n         probs = 0.25,\n         na.rm = TRUE)\n\n     25% \n26.02911 \n\nquantile(mydata$bmiPRE, \n         probs = 0.75,\n         na.rm = TRUE)\n\n     75% \n29.79173 \n\n\nWe can also get the mean() and sd() for Height.\n\nmean(mydata$Height, na.rm = TRUE)\n\n[1] 5.55\n\nsd(mydata$Height, na.rm = TRUE)\n\n[1] 0.9795273\n\n\n\ndplyr::summarize() function\nThe dplyr package also has a summarize() function you can use to get specific statistics of your choosing. For example, let’s get the mean() and sd() for Age in one code step.\n\nmydata %&gt;%\n  dplyr::summarise(\n    mean_age = mean(Age, na.rm = TRUE),\n    sd_age = sd(Age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 2\n  mean_age sd_age\n     &lt;dbl&gt;  &lt;dbl&gt;\n1     44.8   14.9\n\n\nWe can do this same code again but add the dplyr::group_by() function to add a grouping variable to get the statistics by.\nNOTE: The dplyr::group_by() function must come BEFORE dplyr::summarise().\nLet’s get the summary stats (mean and sd) for Age by GenderCoded.f.\n\nmydata %&gt;%\n  dplyr::group_by(GenderCoded.f) %&gt;%\n  dplyr::summarise(\n    mean_age = mean(Age, na.rm = TRUE),\n    sd_age = sd(Age, na.rm = TRUE)\n  )\n\n# A tibble: 3 × 3\n  GenderCoded.f mean_age sd_age\n  &lt;fct&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n1 Male              41.5   8.77\n2 Female            50.6  20.5 \n3 &lt;NA&gt;              35    NA   \n\n\n\n\n\n\n\n\n\nEach code step may result in different object classes\n\n\n\nAs you work through a series of code steps in an analysis or computational workflow, each step may produce an output object with a different class.\n\n\nLet’s look at each step of the code above to produce a table of means and standard deviations of Age by GenderCoded.f.\nSTEP 1 - begin with the dataset\nWe start with the dataset mydata which is a “tibble” “data.frame” since we imported the data using one of the tidyverse packages: readr, readxl or haven all of which create a tbl_df class object.\n\n# Step 1\nclass(mydata)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nSTEP 2 - create a “grouped” data.frame\nNotice that as soon as we use the dplyr::group_by() function, the result is an updated type of “tibble”“data.frame” which is now a grouped_df class object. This object class is described at https://dplyr.tidyverse.org/articles/grouping.html.\nThe grouped_df is similar to:\n\napplying the SPLIT FILE command in the SPSS software\n\nor using the BY command in SAS to “work with grouped data”\n\n\n\n# save the output of step 2\nstep2 &lt;- mydata %&gt;%\n  dplyr::group_by(GenderCoded.f)\n\nclass(step2)\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nSTEP 3 - after the summarise step\nAfter STEP 3, another tbl_df is created.\n\nstep3 &lt;- mydata %&gt;%\n  dplyr::group_by(GenderCoded.f) %&gt;%\n  dplyr::summarise(\n    mean_age = mean(Age, na.rm = TRUE),\n    sd_age = sd(Age, na.rm = TRUE)\n  )\n\nclass(step3)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nSince this saved output object step3 is a tbl_df, we can use it like any other “data.frame” object. For example, we can pull out the mean_age column:\n\n# pull out the mean_age column using $\nstep3$mean_age\n\n[1] 41.45455 50.62500 35.00000\n\n# pull out the sd_age column using select()\nstep3 %&gt;%\n  select(sd_age)\n\n# A tibble: 3 × 1\n  sd_age\n   &lt;dbl&gt;\n1   8.77\n2  20.5 \n3  NA   \n\n\n\nMake summary tables\nCreating nicely formatted summary tables is an active area of development in the R community. So, I’m sure there are new functions and packages that I may not have shown here. But here are a few packages I use often for making tables of summary statistics. Most of these are designed to work within a Rmarkdown document.\narsenal package for tables\nThe arsenal package is useful for making tables - especially with Rmarkdown - to be explained further in a later session Module 1.3.6. Learn more at tableby() vignette.\nHere is a quick example of some summary statistics for Age, bmiPRE, and SES by GenderCoded.f using the tableby() function.\nFirst let’s add labels for SES and create a factor variable.\n\nmydata$SES.f &lt;- \n  factor(mydata$SES,\n         levels = c(1, 2, 3),\n         labels = c(\"low income\",\n                    \"average income\",\n                    \"high income\"))\n\nlibrary(arsenal)\ntab1 &lt;- tableby(GenderCoded.f ~ Age + bmiPRE +SES.f, \n                data = mydata)\nsummary(tab1)\n\n\n\n\n\n\n\n\n\n\nMale (N=11)\nFemale (N=8)\nTotal (N=19)\np value\n\n\n\nAge\n\n\n\n0.199\n\n\n   Mean (SD)\n41.455 (8.768)\n50.625 (20.493)\n45.316 (15.089)\n\n\n\n   Range\n24.000 - 52.000\n35.000 - 99.000\n24.000 - 99.000\n\n\n\nbmiPRE\n\n\n\n0.370\n\n\n   Mean (SD)\n40.230 (42.193)\n26.347 (2.785)\n34.384 (32.274)\n\n\n\n   Range\n10.586 - 166.102\n20.646 - 29.790\n10.586 - 166.102\n\n\n\nSES.f\n\n\n\n0.625\n\n\n   N-Miss\n1\n0\n1\n\n\n\n   low income\n2 (20.0%)\n2 (25.0%)\n4 (22.2%)\n\n\n\n   average income\n7 (70.0%)\n4 (50.0%)\n11 (61.1%)\n\n\n\n   high income\n1 (10.0%)\n2 (25.0%)\n3 (16.7%)\n\n\n\n\n\ngtsummary package for tables\nThe gtsummary package is also useful for making tables. We can even use it to make nicely formatted tables in the “Viewer” window pane or in Rmarkdown. Learn more at tbl_summary() vignette.\nHere is a quick example of some summary statistics for Age and bmiPRE by GenderCoded.f using the tbl_summary() function.\n\nlibrary(gtsummary)\n\nmydata %&gt;%\n  select(Age, bmiPRE, SES.f, GenderCoded.f) %&gt;%\n  tbl_summary(by = GenderCoded.f)\n\n\n\nTable 1\n\n\n\n\n\n\ntableone package for making summary tables\nThe output produced from tableone is simple text output.\n\nlibrary(tableone)\n\ntableone::CreateTableOne(\n  data = mydata,\n  vars = c(\"Age\", \"bmiPRE\", \"SES.f\"),\n  strata = \"GenderCoded.f\"\n)\n\n                    Stratified by GenderCoded.f\n                     Male          Female        p      test\n  n                     11             8                    \n  Age (mean (SD))    41.45 (8.77)  50.62 (20.49)  0.199     \n  bmiPRE (mean (SD)) 40.23 (42.19) 26.35 (2.79)   0.370     \n  SES.f (%)                                       0.625     \n     low income          2 (20.0)      2 (25.0)             \n     average income      7 (70.0)      4 (50.0)             \n     high income         1 (10.0)      2 (25.0)             \n\n\n\ngmodels package for R-x-C tables\nIf we want to look at a “cross-table” similar to output from SPSS or SAS, the gmodels package has the CrossTable() function that creates text-based tables similar to these other statistics software packages.\nLet’s get the frequencies and columns percentages for SES by gender. The first variable is the row variable, the second is the column variable.\n\nlibrary(gmodels)\n\nCrossTable(mydata$SES.f,          # row variable\n           mydata$GenderCoded.f,  # column variable\n           prop.t = FALSE,        # turn off percent of total\n           prop.r = FALSE,        # turn off percent of row\n           prop.c = TRUE,         # turn on percent of column\n           prop.chisq = FALSE,    # turn off percent for chisq test\n           format = \"SPSS\")       # format like SPSS\n\n\n   Cell Contents\n|-------------------------|\n|                   Count |\n|          Column Percent |\n|-------------------------|\n\nTotal Observations in Table:  18 \n\n               | mydata$GenderCoded.f \n  mydata$SES.f |     Male  |   Female  | Row Total | \n---------------|-----------|-----------|-----------|\n    low income |        2  |        2  |        4  | \n               |   20.000% |   25.000% |           | \n---------------|-----------|-----------|-----------|\naverage income |        7  |        4  |       11  | \n               |   70.000% |   50.000% |           | \n---------------|-----------|-----------|-----------|\n   high income |        1  |        2  |        3  | \n               |   10.000% |   25.000% |           | \n---------------|-----------|-----------|-----------|\n  Column Total |       10  |        8  |       18  | \n               |   55.556% |   44.444% |           | \n---------------|-----------|-----------|-----------|\n\n \n\n\nTable Inspiration\nMaking effective, nicely formatted tables from R and Rmarkdown has been an active area of development these past few years. In fact, I encourage you to check out the winners of the last few Table Contests:\n\n2024 Table Contest Winners\n2022 Table Contest Winners\n2021 Table Contest Winners\n\nOther Table Resources and Packages include:\n\nhttps://bookdown.org/yihui/rmarkdown-cookbook/table-other.html\nhttps://epirhandbook.com/en/new_pages/tables_descriptive.html\n\ngt package on CRAN and gt package website\n\n\nkableExtra package on CRAN and kableExtra package website\n\n\nflextable package on CRAN and flextable package website and flextable book\n\nhuxtable package on CRAN",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#exportingsaving-data",
    "href": "module132_DataWrangling.html#exportingsaving-data",
    "title": "1.3.2: Data Wrangling",
    "section": "6. Exporting/Saving Data",
    "text": "6. Exporting/Saving Data\nThroughout this lesson we have worked with the mydata dataset. We have made some changes and created new variables. Let’s save the updates to this little dataset for use in later modules.\nUsing the save() function\nSave mydata as *.Rdata native R binary format\nAs we move forward in our lesson modules, we will mostly be working with the “native” format for datafiles (and objects) which have the extension of *.RData or *.rda. These file formats are efficient in terms of saving memory and speed for faster loading of data.\n\n\n\n\n\n\nR data binary formats registered with Library of Congress\n\n\n\nThe “R Data Format Family (.rdata, .rda)” are registered with the Library of Congress under the “Sustainability of Digital Formats”. The description summary states:\n\n“The RData format (usually with extension .rdata or .rda) is a format designed for use with R, a system for statistical computation and related graphics, for storing a complete R workspace or selected”objects” from a workspace in a form that can be loaded back by R. The save function in R has options that result in significantly different variants of the format. This description is for the family of formats created by save and closely related functions. A workspace in R is a collection of typed “objects” and may include much more than the typical tabular data that might be considered a “dataset,” including, for example, results of intermediate calculations and scripts in the R programming language. A workspace may also contain several datasets, which are termed “data frames” in R.”\n\n\n\nLet’s save the mydata data.frame object as “mydata.RData”, using the save() function. See help(save, package = \"base\").\n\n# save the mydata dataset object\nsave(mydata,\n     file = \"mydata.RData\")\n\n\nSave All Objects in Global Environment as *.Rdata\nIt is worth noting that the code above specifically ONLY saves the mydata object. Assuming that your “Global Environment” was empty at the beginning of your computing session at the beginning of this Module 1.3.2, we have created 6 objects so far:\n\n\nforeignhistory - created above looking at the CRAN history for the foreign package\n\nhavenhistory - created above looking at the CRAN history for the haven package\n\nmydata - main dataset imported above\n\nstep2 - created to illustrate the %&gt;% stepwise programming workflow\n\nstep3 - created to illustrate the %&gt;% stepwise programming workflow\n\ntab1 - created above to make a table using the arsenal package\n\n\nSuppose we want to save ALL of these objects for a future computing session or if you’d like to share all of these objects with someone else on your team.\nWe can save the whole Global Environment or select objects in the environment also to a *.RData file to be read back into a future computing session.\nTo save all objects in the Global Environment, we can use save.image():\n\n# save all objects from module 1.3.2\nsave.image(file = \"module132.RData\")\n\n\nSave More than One Object in Global Environment as *.Rdata\nTo save one or more objects for future use - simply list the object names and then save them into an *.RData file.\n\n# save mydata and tab1\nsave(mydata, tab1,\n     file = \"mydata_tab1.RData\")\n\nReading Objects Saved as *.Rdata Back Into Session\nTo test and make sure these items were saved as we expect, let’s remove all objects from our Global Environment and load them back in.\n\n\n\n\n\n\nBe careful using rm(list= ls())\n\n\n\nThe use of the rm(list= ls()) should NOT be used unless you know you have saved everything up to this point. Once you remove all objects from your Global Environment, it cannot be undone. You can either rerun the R code to recreate these objects, or go through the steps described below to save and re-load your objects into your session.\n\n\n\n# remove all objects\nrm(list = ls())\n\n# check that global environment is empty\nls()\n\ncharacter(0)\n\n\nRead back in only the mydata file.\n\n# read in mydata\nload(file = \"mydata.RData\")\n\n# check objects in global environment\nls()\n\n[1] \"mydata\"\n\n\nI’ll remove all objects again for to illustrate the next use of load() function.\n\nrm(list = ls())\n\nRead back in both the mydata and tab1 objects\n\n# read in mydata_tab1\nload(file = \"mydata_tab1.RData\")\n\n# check objects in global environment\nls()\n\n[1] \"mydata\" \"tab1\"  \n\n\n\nrm(list = ls())\n\nRead back in all objects saved from Module 1.3.2.\n\n# read in module132.RData\nload(file = \"module132.RData\")\n\n# check objects in global environment\nls()\n\n[1] \"foreignhistory\" \"havenhistory\"   \"mydata\"         \"step2\"         \n[5] \"step3\"          \"tab1\"          \n\n\n\nrm(list = ls())\n\n\nSave/export data to other formats\nIn addition to use the built-in save() and save.image() functions, we can also export (or save) data objects from R into other formats like CSV and those for specific statistics software like SPSS (*.sav), SAS (*.XPT)and Stata (*.dta).\n\nExport/Write CSV and EXCEL\nIn the readr package, we can use write_csv() to save our updated data as a CSV file which can be read by other software like Excel.\nI’ll load the data back in and then save/export it as other formats.\n\n# read in mydata\nload(file = \"mydata.RData\")\n\n# write as CSV\nreadr::write_csv(mydata,\n                 file = \"mydata_updated.csv\")\n\n\nExport/Write for Other Software (SPSS, SAS, Stata)\nWe can also use the haven package to export/save the updated mydata dataset as a SPSS (*.sav), SAS (*.XPT) or Stata (*.dta) file format.\nCode to export to SPSS *.sav format\n\nhaven::write_sav(mydata,\n                 path = \"mydata_updated.sav\")\n\nRename variable “GenderCoded.f” and “SES.f” to “GenderCoded_f” and “SES_f” to export to SAS or Stata since the “*.f” won’t work in a variable name in these software.\n\n# rename GenderCoded.f and SES.f since the\n# xxx.f wont work for SAS or Stata\nnames(mydata)[names(mydata) == \"GenderCoded.f\"] &lt;-\n  \"GenderCoded_f\"\nnames(mydata)[names(mydata) == \"SES.f\"] &lt;-\n  \"SES_f\"\n\nCode to export to SAS using the “XPT” format\n\nhaven::write_xpt(mydata,\n                 path = \"mydata_updated.xpt\")\n\nCode to export to Stata *.dta format\n\nhaven::write_dta(mydata,\n                 path = \"mydata_updated.dta\")\n\nIf you have these other statistical software on your computer, try opening these new exported files into that software to confirm they worked.\n\n\n\n\n\n\nSome import/export work better than others\n\n\n\nBe aware that many of these import/export functions do work pretty well, but some features of functionality of native formats used by other software packages may not work fully. Read the documentation for each package and function to understand what the limitations may be. For example, importing and exporting SAS *.sas7bdat formatted files can be problematic.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#r-code-for-this-module",
    "href": "module132_DataWrangling.html#r-code-for-this-module",
    "title": "1.3.2: Data Wrangling",
    "section": "R Code For This Module",
    "text": "R Code For This Module\n\nmodule_132.R",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#references",
    "href": "module132_DataWrangling.html#references",
    "title": "1.3.2: Data Wrangling",
    "section": "References",
    "text": "References\n\n\nCsárdi, Gábor, and Maëlle Salmon. 2025. Pkgsearch: Search and Query CRAN r Packages. https://github.com/r-hub/pkgsearch.\n\n\nHeinzen, Ethan, Jason Sinnwell, Elizabeth Atkinson, Tina Gunderson, and Gregory Dougherty. 2021. Arsenal: An Arsenal of r Functions for Large-Scale Statistical Summaries. https://github.com/mayoverse/arsenal.\n\n\nIannone, Richard. 2023. Fontawesome: Easily Work with Font Awesome Icons. https://github.com/rstudio/fontawesome.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSjoberg, Daniel D., Joseph Larmarange, Michael Curry, Jessica Lavery, Karissa Whiting, and Emily C. Zabor. 2024. Gtsummary: Presentation-Ready Data Summary and Analytic Result Tables. https://github.com/ddsjoberg/gtsummary.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery, and Joseph Larmarange. 2021. “Reproducible Summary Tables with the Gtsummary Package.” The R Journal 13: 570–80. https://doi.org/10.32614/RJ-2021-053.\n\n\nWarnes, Gregory R., Ben Bolker, Thomas Lumley, Randall C Johnson. Contributions from Randall C. Johnson are Copyright SAIC-Frederick, Inc. Funded by the Intramural Research Program, of the NIH, National Cancer Institute, and Center for Cancer Research under NCI Contract NO1-CO-12400. 2022. Gmodels: Various r Programming Tools for Model Fitting. https://doi.org/10.32614/CRAN.package.gmodels.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nYoshida, Kazuki, and Alexander Bartel. 2022. Tableone: Create Table 1 to Describe Baseline Characteristics with or Without Propensity Score Weights. https://github.com/kaz-yos/tableone.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#other-helpful-resources",
    "href": "module132_DataWrangling.html#other-helpful-resources",
    "title": "1.3.2: Data Wrangling",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html",
    "href": "module134_MissingWeight.html",
    "title": "1.3.4: Missing Data and Sampling Weights (brief intro)",
    "section": "",
    "text": "Identify, summarize and visualize missing data.\nMissing Data Mechanisms (bias mechanisms or models)\nMissing Data Handling and Imputation Methods (brief intro)\nImpact of Sampling Weights for Survey Data (brief intro)",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#session-objectives-updated",
    "href": "module134_MissingWeight.html#session-objectives-updated",
    "title": "1.3.4: Missing Data and Sampling Weights (brief intro)",
    "section": "",
    "text": "Identify, summarize and visualize missing data.\nMissing Data Mechanisms (bias mechanisms or models)\nMissing Data Handling and Imputation Methods (brief intro)\nImpact of Sampling Weights for Survey Data (brief intro)",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#prework---before-you-begin",
    "href": "module134_MissingWeight.html#prework---before-you-begin",
    "title": "1.3.4: Missing Data and Sampling Weights (brief intro)",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\nA. Install packages\nIf you do not have them already, install the following packages from CRAN (using the RStudio Menu “Tools/Install” Packages interface):\n\n\nVIM and VIM package website\n\n(Optional) skimr and skimr website\n\n(Optional) modelsummary and modelsummary website\n\n(Optional) summarytools and summarytools on Github\n\n\npalmerpenguins and palmerpenguins website\n\n\nggplot2 and ggplot2 website\n\n\nnaniar and naniar website\n\n\ndplyr and dplyr website\n\n\ngtsummary and gtsummary website\n\n\nHmisc and Hmisc website\n\n\nmice and mice website\n\nB. Review these online Book Chapters:\n\nBOOK: Flexible Imputation of Missing Data, 2nd ed., by Stef van Buuren (mice package author) - Chapter 1 “Introduction”, Sections 1.1-1.4\nBOOK: The Epidemiologist R Handbook - Chapter 20 “Missing Data”\nC. Open/create an RStudio project for this lesson\nLet’s start with the myfirstRproject RStudio project you created in Module 1.3.2 - part 1. If you have not yet created this myfirstRproject RStudio project, go ahead and create a new RStudio Project for this lesson. Feel free to name your project whatever you want, it does not need to be named myfirstRproject.",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#identify-summarize-and-visualize-missing-data",
    "href": "module134_MissingWeight.html#identify-summarize-and-visualize-missing-data",
    "title": "1.3.4: Missing Data and Sampling Weights (brief intro)",
    "section": "1. Identify, summarize and visualize missing data",
    "text": "1. Identify, summarize and visualize missing data\nFind Missing Data in Your Dataset.\nOne simple way to find missing data is to open it in the Data Viewer window and sort the data.\nFor example, load the VIM package and take a look at the sleep dataset provided within this package.\n\nlibrary(VIM)\ndata(\"sleep\")\n\nClick on the sleep dataset to open it in the data viewer:\n\nNotice the light grey NAs shown for the missing data spots in this dataset.\nIf we click on the column for the Dream variable and sort these values, notice that the NAs all now show up at the bottom of the viewer window. It does not matter if you sort ascending or descending, the NAs are always at the bottom of the viewer.\n\nThis method is ok for a small dataset with not too many variables or rows of data. But let’s look at other ways to summarize the amounts of missing data in your dataset.\n\nDescribe Missing Data.\nBuilt-in summary() function\nAs we saw back in Module 1.3.2, Section 5, we can use the summary() function to get some basic statistics for each variable in the dataset, including the number of NAs.\n\nsummary(sleep)\n\n    BodyWgt            BrainWgt            NonD            Dream      \n Min.   :   0.005   Min.   :   0.14   Min.   : 2.100   Min.   :0.000  \n 1st Qu.:   0.600   1st Qu.:   4.25   1st Qu.: 6.250   1st Qu.:0.900  \n Median :   3.342   Median :  17.25   Median : 8.350   Median :1.800  \n Mean   : 198.790   Mean   : 283.13   Mean   : 8.673   Mean   :1.972  \n 3rd Qu.:  48.203   3rd Qu.: 166.00   3rd Qu.:11.000   3rd Qu.:2.550  \n Max.   :6654.000   Max.   :5712.00   Max.   :17.900   Max.   :6.600  \n                                      NA's   :14       NA's   :12     \n     Sleep            Span              Gest             Pred      \n Min.   : 2.60   Min.   :  2.000   Min.   : 12.00   Min.   :1.000  \n 1st Qu.: 8.05   1st Qu.:  6.625   1st Qu.: 35.75   1st Qu.:2.000  \n Median :10.45   Median : 15.100   Median : 79.00   Median :3.000  \n Mean   :10.53   Mean   : 19.878   Mean   :142.35   Mean   :2.871  \n 3rd Qu.:13.20   3rd Qu.: 27.750   3rd Qu.:207.50   3rd Qu.:4.000  \n Max.   :19.90   Max.   :100.000   Max.   :645.00   Max.   :5.000  \n NA's   :4       NA's   :4         NA's   :4                       \n      Exp            Danger     \n Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000  \n Median :2.000   Median :2.000  \n Mean   :2.419   Mean   :2.613  \n 3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :5.000   Max.   :5.000  \n                                \n\n\n\nskimr package\nAnother helpful package is the skimr package which has the skim() function which provides a count of the amount of missing data and the proportion of complete data for that variable.\n\n\n\n\n\n\nRmarkdown for skimr package\n\n\n\nWhen “knitting” to HTML the code below creates the summary table with the miniture histograms. However, when “knitting” to PDF (using the default portrait layout) the histograms get cutoff on the page. Additional LaTex customization is needed to change the layout to landscape to be able to see the histograms.\n\n\n\nlibrary(skimr)\nskim(sleep)\n\n\nData summary\n\n\nName\nsleep\n\n\nNumber of rows\n62\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nBodyWgt\n0\n1.00\n198.79\n899.16\n0.00\n0.60\n3.34\n48.20\n6654.0\n▇▁▁▁▁\n\n\nBrainWgt\n0\n1.00\n283.13\n930.28\n0.14\n4.25\n17.25\n166.00\n5712.0\n▇▁▁▁▁\n\n\nNonD\n14\n0.77\n8.67\n3.67\n2.10\n6.25\n8.35\n11.00\n17.9\n▅▇▆▃▂\n\n\nDream\n12\n0.81\n1.97\n1.44\n0.00\n0.90\n1.80\n2.55\n6.6\n▇▇▃▁▁\n\n\nSleep\n4\n0.94\n10.53\n4.61\n2.60\n8.05\n10.45\n13.20\n19.9\n▅▅▇▃▃\n\n\nSpan\n4\n0.94\n19.88\n18.21\n2.00\n6.62\n15.10\n27.75\n100.0\n▇▃▁▁▁\n\n\nGest\n4\n0.94\n142.35\n146.81\n12.00\n35.75\n79.00\n207.50\n645.0\n▇▃▁▁▁\n\n\nPred\n0\n1.00\n2.87\n1.48\n1.00\n2.00\n3.00\n4.00\n5.0\n▇▇▆▃▇\n\n\nExp\n0\n1.00\n2.42\n1.60\n1.00\n1.00\n2.00\n4.00\n5.0\n▇▃▁▂▃\n\n\nDanger\n0\n1.00\n2.61\n1.44\n1.00\n1.00\n2.00\n4.00\n5.0\n▇▆▅▅▃\n\n\n\n\n\n\nmodelsummary package\nAnother helpful package is the modelsummary package which has the datasummary_skim() function which is a slightly better version built off the skimr::skim() package and function.\n\nlibrary(modelsummary)\ndatasummary_skim(sleep) \n\n\n\n\n\n    \n\n      \n\n \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n\n\nBodyWgt\n                  60\n                  0\n                  198.8\n                  899.2\n                  0.0\n                  3.3\n                  6654.0\n                  \n                \n\nBrainWgt\n                  59\n                  0\n                  283.1\n                  930.3\n                  0.1\n                  17.2\n                  5712.0\n                  \n                \n\nNonD\n                  40\n                  23\n                  8.7\n                  3.7\n                  2.1\n                  8.4\n                  17.9\n                  \n                \n\nDream\n                  31\n                  19\n                  2.0\n                  1.4\n                  0.0\n                  1.8\n                  6.6\n                  \n                \n\nSleep\n                  45\n                  6\n                  10.5\n                  4.6\n                  2.6\n                  10.4\n                  19.9\n                  \n                \n\nSpan\n                  48\n                  6\n                  19.9\n                  18.2\n                  2.0\n                  15.1\n                  100.0\n                  \n                \n\nGest\n                  50\n                  6\n                  142.4\n                  146.8\n                  12.0\n                  79.0\n                  645.0\n                  \n                \n\nPred\n                  5\n                  0\n                  2.9\n                  1.5\n                  1.0\n                  3.0\n                  5.0\n                  \n                \n\nExp\n                  5\n                  0\n                  2.4\n                  1.6\n                  1.0\n                  2.0\n                  5.0\n                  \n                \n\nDanger\n                  5\n                  0\n                  2.6\n                  1.4\n                  1.0\n                  2.0\n                  5.0\n                  \n                \n\n\n\n\n\n\n\nsummarytools package\nAnother package that also provides a nice summary of the variables in the dataset, is the dfSummary() from the summarytools dataset.\nNOTE: Learn more about how to use summarytools::dfSummary() in an Rmarkdown document at https://cran.r-project.org/web/packages/summarytools/vignettes/rmarkdown.html.\n\nlibrary(summarytools)\nview(dfSummary(sleep))\n\n\n\n\n\n\n\n\n\nTry It On Your Own\n\n\n\nTry running summary() or skim() on the penguins dataset from the palmerpenguins package. Notice the summaries for the numeric and the factor type variables.\n\n\n\nlibrary(palmerpenguins)\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\n\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\n\nVisualize Missing Data.\nMaking plots with VIM package\nThe VIM package has an “aggregate” function aggr() which counts up the amounts of missing data for each variable and combinations of variables. The sleep dataset only has 10 variables.\n\n\n\n\n\n\nWARNING - Beware of Using Too Many Variables at Once\n\n\n\nBefore using the aggr() function, limit the number of variables. FIRST create a dataset with only the variables you are interested in BEFORE running the function - otherwise you may lock up your computer if you feed it too many variables at once.\n\n\n\n# get the amount of missing data in the sleep dataset\na &lt;- aggr(sleep, plot = FALSE)\na\n\n\n Missings in variables:\n Variable Count\n     NonD    14\n    Dream    12\n    Sleep     4\n     Span     4\n     Gest     4\n\n\nThe default output from above only lists the variables that have one or more rows with missing data. However, you can get a list of all of the variables with this code:\n\na$missings\n\n         Variable Count\nBodyWgt   BodyWgt     0\nBrainWgt BrainWgt     0\nNonD         NonD    14\nDream       Dream    12\nSleep       Sleep     4\nSpan         Span     4\nGest         Gest     4\nPred         Pred     0\nExp           Exp     0\nDanger     Danger     0\n\n\n\nNext, let’s get some plots of the missing data in the sleep dataset.\nThe plot on the LEFT below is a simple bar plot showing the missing counts for each variable in the dataset. Notice that there are only 5 variables with one or more missing values:\n* `NonD`\n* `Dream`\n* `Sleep`\n* `Span`\n* `Gest`\nThe plot on the RIGHT however, shows the amounts of missing data for the various patterns of missing data for the 10 variables in the sleep dataset. For example, notice that of the 62 rows of data in the sleep dataset:\n\nthere are only 42 rows with complete data with no missing data on all 10 variables (i.e., 42/62 = 67.7% of the data is complete for all 10 variables);\nthe next largest “pattern” of missing data is 9 rows that have both NonD and Dream variables with missing values; and\nthere are 3 rows of data with the gest variable having missing data.\n\n\n# make plots of the amounts and patterns of missing data\nplot(a, numbers = TRUE, prop = FALSE)\n\n\n\n\n\n\n\nExploring patterns of missingness can be informative to better understand why the data might be missing and possibly provide insights into the underlying mechanisms causing or leading to the missing data.\n\nMarginplots - see how missingness varies with other measures\nIn addition to a usual scatterplot, the marginplot() function in the VIM package, also shows information about missing values in the plot margins.\nThe red boxplot on the left shows the distrubution of all values of Sleep where Dream contains a missing value. The blue boxplot on the left shows the distribution of the values of Sleep where Dream is observed.\n\nx &lt;- sleep[, c(\"Dream\", \"Sleep\")]\nmarginplot(x)\n\n\n\n\n\n\n\n\nVisualize Missing Data with the naniar package\nThe naniar package “provides principled, tidy ways to summarise, visualise, and manipulate missing data with minimal deviations from the workflows in ggplot2 and tidy data.” See naniar website.\nFor example, let’s make a similar to plot to what we did above to visualize the scatterplot between Dream and Sleep but also consider the amounts of missing data of one variable relative to the other variable in the plot. We can do this using the geom_miss_point() function provided in the naniar package which works with ggplot2.\n\nlibrary(naniar)\nlibrary(ggplot2)\n\nggplot(sleep, \n       aes(x = Dream, \n           y = Sleep)) + \n  geom_miss_point()\n\n\n\n\n\n\n\n\nWe can also create an UpSet plot which is useful for visualizing intersections between sets. In the case of missing data, we are interested in visualizing how the missing data for each variable overlaps with each other (i.e., the missing data patterns).\nTo create an UpSet plot for the missing data patterns for the 10 variables in the sleep dataset, we can use the gg_miss_upset() function. The plot produced is similar to the plot above from the VIM package.\nNotice that the plot ONLY shows patterns for the 20 of 62 rows and for the 5 of 10 variables with any missing data. The plot shows that:\n\n9 rows have missing data for both the Dream and NonD variables\n3 rows have missing data for the Gest variable\n2 rows have missing data for the Span variable\n2 rows have missing data for both Sleep and NonD\n\n2 rows have missing data for Sleep, Dream and NonD variables\n1 row has missing data for both Span and Gest\n\n1 row has missing data for Span, Dream and NonD variables\n\n\ngg_miss_upset(sleep)",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#missing-data-mechanisms-bias-mechanisms-or-models",
    "href": "module134_MissingWeight.html#missing-data-mechanisms-bias-mechanisms-or-models",
    "title": "1.3.4: Missing Data and Sampling Weights (brief intro)",
    "section": "2. Missing Data Mechanisms (bias mechanisms or models)",
    "text": "2. Missing Data Mechanisms (bias mechanisms or models)\nWhy should we worry about missing data?\nSetting aside bias concerns for the moment, missing data logistically causes issues with code - especially in R. At first glance this seems to be a huge pain since we get errors or nonsensical output. But these issues force us to deal with the missing data and provide explicit instructions to the computer code on how we want to addressthe missing data. Learn more in the Flexible Imputation of Missing Data BOOK.\nImpact of missing data for descriptive stats like the mean\nFor example, let’s find the mean of the Dream variable in the sleep dataset.\n\nmean(sleep$Dream)\n\n[1] NA\n\n\nWe get NA since there is missing data for the Dream variable, thus the mean of all rows is “not available”. So, we need to tell R to first remove the missing values (the NAs) prior to computing the mean.\n\nmean(sleep$Dream, na.rm = TRUE)\n\n[1] 1.972\n\n\n\nImpact of missing data for summary statistics\nWe did do a deep dive above and we know that there are 12 rows with missing values for the Dream variable. But if we had run the mean() function with na.rm = TRUE, we might not have know just how much data was missing. So, it is always a good idea to make sure to check for missing data and assess how much you have PRIOR to conducting any analyses.\nAs we saw in Module 1.3.2, section 5 we can use the gtsummary package with the tbl_summary() function to get better summary statistics including a list of the amount of unknown (missing) rows. Let’s get the means (and standard deviations) for 3 of the variables in the sleep dataset. Notice that there are no “unknowns” for BrainWgt since it has no missing values.\n\n\n\n\n\n\nCustomizing gtsummary::tbl_summary()\n\n\n\nLearn more at Multiline Summaries Using tbl_summary() on how I customized this table to include the count (N) of non-missing rows, mean and standard deviation along with the counts for the unknowns.\n\n\n\nlibrary(dplyr)\nlibrary(gtsummary)\n\nsleep %&gt;%\n  select(Dream, Gest, BrainWgt) %&gt;%\n  tbl_summary(\n    type = all_continuous() ~ \"continuous2\",\n    statistic = all_continuous() ~ c(\n      \"{N_nonmiss}\",\n      \"{mean} ({sd})\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 62\n\n\n\nDream\n\n\n\n    N Non-missing\n50\n\n\n    Mean (SD)\n1.97 (1.44)\n\n\n    Unknown\n12\n\n\nGest\n\n\n\n    N Non-missing\n58\n\n\n    Mean (SD)\n142 (147)\n\n\n    Unknown\n4\n\n\nBrainWgt\n\n\n\n    N Non-missing\n62\n\n\n    Mean (SD)\n283 (930)\n\n\n\n\n\n\n\nImpact of missing data for regression models\nWhen running a model, like a regression model between Dream and Sleep, let’s look at the summary output from fitting a linear model using the built-in lm() function:\n\nsummary(lm(Sleep ~ Dream, data = sleep))\n\n\nCall:\nlm(formula = Sleep ~ Dream, data = sleep)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2765 -2.0384 -0.1096  2.1599  9.2624 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.0273     0.7960   7.572 1.27e-09 ***\nDream         2.3051     0.3209   7.183 4.85e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.178 on 46 degrees of freedom\n  (14 observations deleted due to missingness)\nMultiple R-squared:  0.5287,    Adjusted R-squared:  0.5184 \nF-statistic: 51.59 on 1 and 46 DF,  p-value: 4.849e-09\n\n\nNotice the output tells us that (14 observations deleted due to missingness). So this model was fit with only 62-14 = 48 rows (77.4%) of the original 62 rows of data. The model was fit using only the complete dataset based on the 2 variables in the model: Dream and Sleep, where\n\nthere are 10 rows missing data for only the Dream variable,\n2 rows missing data for both Dream and Sleep variables,\nand 2 rows missing data for only the Sleep variable.\n\nKeep in mind, when you are fitting any model (linear or logistic regression, analysis of variance, etc), the default is (almost) always to use a LISTWISE deletion, which removes ALL rows with any missing data on any of the variables considered in the model - including predictors, covariates, and outcome(s).\n\nImpact of missing data for correlation matrix - cor() function\nLet’s also look at a small correlation matrix considering PAIRWISE versus LISTWISE deletion of missing data for 3 variables from the sleep dataset. Notice that the correlation between BrainWgt and Dream and Sleep are slightly different between LISTWISE and PAIRWISE approaches. These are all pearson correlations by default.\nLISTWISE deletion\n\n# LISTWISE deletion, use = \"complete.obs\"\nsleep %&gt;%\n  select(BrainWgt, Dream, Sleep) %&gt;%\n  cor(use = \"complete.obs\")\n\n            BrainWgt       Dream      Sleep\nBrainWgt  1.00000000 -0.08437367 -0.3221748\nDream    -0.08437367  1.00000000  0.7270870\nSleep    -0.32217479  0.72708696  1.0000000\n\n\nPAIRWISE deletion\n\n# PAIRWISE deletion, use = \"pairwise.complete.obs\"\nsleep %&gt;%\n  select(BrainWgt, Dream, Sleep) %&gt;%\n  cor(use = \"pairwise.complete.obs\")\n\n           BrainWgt      Dream     Sleep\nBrainWgt  1.0000000 -0.1051388 -0.358102\nDream    -0.1051388  1.0000000  0.727087\nSleep    -0.3581020  0.7270870  1.000000\n\n\n\nImpact of missing data for correlation matrix - Hmisc package\nThere is also a helpful correlation function rcorr() in the Hmisc package. From this function we can save the output and get the n’s and p-values in addition to the (Pearson) correlations. These numeric data have to be converted to a numeric matrix prior to inputting them to the rcorr() function, which is why as.matrix() is used in the code chunk below.\nNOTE: PAIRWISE deletion is the default setting for Hmisc::rcorr().\n\nlibrary(Hmisc)\nc1 &lt;- sleep %&gt;%\n  select(BrainWgt, Dream, Sleep) %&gt;%\n  as.matrix() %&gt;%\n  rcorr()\n\nShow the correlations:\n\nc1$r\n\n           BrainWgt      Dream     Sleep\nBrainWgt  1.0000000 -0.1051388 -0.358102\nDream    -0.1051388  1.0000000  0.727087\nSleep    -0.3581020  0.7270870  1.000000\n\n\nGet the sample sizes for each correlation in the matrix:\n\nc1$n\n\n         BrainWgt Dream Sleep\nBrainWgt       62    50    58\nDream          50    50    48\nSleep          58    48    58\n\n\nGet each individual p-value for each correlation:\n\nc1$P\n\n            BrainWgt        Dream        Sleep\nBrainWgt          NA 4.674359e-01 5.779531e-03\nDream    0.467435869           NA 4.849120e-09\nSleep    0.005779531 4.849120e-09           NA\n\n\n\n\n\n\n\n\n\nGetting complete data\n\n\n\nNote: The rcorr() function uses a PAIRWISE missing values deletion approach. If we want the LISTWISE correlations, we have to get complete data first. Use complete.cases() from the built-in stats package with dplyr::filter() to pull out only the 48 rows in the sleep dataset with complete cases on these 3 variables.\n\n\n\nc2 &lt;- sleep %&gt;%\n  select(BrainWgt, Dream, Sleep) %&gt;%\n  filter(complete.cases(.)) %&gt;% \n  as.matrix() %&gt;%\n  rcorr()\n\nShow the correlations:\n\nc2$r\n\n            BrainWgt       Dream      Sleep\nBrainWgt  1.00000000 -0.08437367 -0.3221748\nDream    -0.08437367  1.00000000  0.7270870\nSleep    -0.32217479  0.72708696  1.0000000\n\n\nGet the sample sizes for each correlation in the matrix:\n\nc2$n\n\n         BrainWgt Dream Sleep\nBrainWgt       48    48    48\nDream          48    48    48\nSleep          48    48    48\n\n\nGet each individual p-value for each correlation:\n\nc2$P\n\n           BrainWgt        Dream        Sleep\nBrainWgt         NA 5.685642e-01 2.553771e-02\nDream    0.56856416           NA 4.849120e-09\nSleep    0.02553771 4.849120e-09           NA\n\n\n\nMissing Data Mechanisms\nThere are entire books and entire courses dedicated to understanding and dealing with missing data mechanisms, which we will not have time to go into depth in this TIDAL course.\nLearn more at:\n\nhttps://stefvanbuuren.name/fimd/sec-MCAR.html\nBOOK: Statistical Analysis with Missing Data, Third Edition, by Roderick Little, Donald Rubin\n\nThe 3 missing data mechanisms to know are:\n\nMCAR = “missing completely at random”\n\nThis assumes that the missingness is not related to the data at all.\nIf the probability of being missing is the same for all cases, then the data are said to be missing completely at random (MCAR).\n\n\nMAR = “missing at random”\n\nIf the probability of being missing is the same only within groups defined by the observed data, then the data are missing at random (MAR).\nFor example, if people with higher levels of depression who were more likely to not answer a question or not complete the study, the the missingness is “dependent” upon depression levels measured in the study.\n\n\nMNAR (or NMAR) = “missing not at random (not missing at random)”\n\nMNAR means that the probability of being missing varies for reasons that are unknown to us.\nAn example of MNAR in public opinion research occurs if those with weaker opinions respond less often (e.g., non-response bias).\n\n\n\nThere is a nice summary of different approaches and assumptions and the effects to the models and statistical estimates, see FIMD Book, Section 1.3.8 Summary. This table illustrates that many “ad hoc” missing imputation approaches can result in standard errors that are too large or too small leading to incorrect calculations for p-values and confidence intervals.\n\nCompare rows with and without missing data\nLet’s take a look at the BodyWgt and BrainWgt variables and compare the values for rows with and without missing data for the Dream variable. Here are the steps involved:\n\nCreate an indicator variable where 0=not missing and 1=missing for the Dream variable.\nRun comparisons for the rows (subjects) with and without missing Dream data.\nPotentially make plots to compare the rows with and without missing values for Dream.\n\nNote: Both BodyWgt and BrainWgt are highly right skewed, so I did a log transform of both. Notice that the p-values for the non-parametric independent group test (Mann Whitney/Wilcoxon Rank Sum test) are the same, since the tests are based on ranks.\n\n# make small dataset\ns1 &lt;- sleep %&gt;%\n  select(BodyWgt, BrainWgt, Dream)\n\n# add missing indicator\ns1$Dream_missing &lt;- as.numeric(is.na(s1$Dream))\n\n# both BodyWgt and BrainWgt are highly right skewed\n# do a log transform of both\ns1 &lt;- s1 %&gt;%\n  mutate(log_BodyWgt = log(BodyWgt),\n         log_BrainWgt = log(BrainWgt)) \n\nTable comparing BodyWgt and BrainWgt for rows with and without missing Dream data. Both are statistically significant - the animals with missing Dream values are larger (bigger body and brain weights).\n\ns1 %&gt;%\n  tbl_summary(\n    by = Dream_missing,\n    include = c(BodyWgt, log_BodyWgt,\n                BrainWgt, log_BrainWgt)\n  ) %&gt;%\n  add_p()\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\n0 N = 501\n\n\n1 N = 121\n\n\np-value2\n\n\n\n\nBodyWgt\n2 (0, 28)\n25 (4, 144)\n0.020\n\n\nlog_BodyWgt\n0.61 (-1.27, 3.32)\n3.13 (1.31, 4.92)\n0.020\n\n\nBrainWgt\n12 (3, 169)\n77 (28, 282)\n0.041\n\n\nlog_BrainWgt\n2.50 (1.10, 5.13)\n4.31 (3.25, 5.53)\n0.041\n\n\n\n\n\n1 Median (Q1, Q3)\n\n\n\n2 Wilcoxon rank sum test\n\n\n\n\n\n\n\nSide-by-side boxplots for the (log) of BodyWgt for rows with missing Dream data and without.\n\nggplot(s1, aes(group = Dream_missing,\n               y = log_BodyWgt)) +\n  geom_boxplot()",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#missing-data-handling-and-imputation-methods-brief-intro",
    "href": "module134_MissingWeight.html#missing-data-handling-and-imputation-methods-brief-intro",
    "title": "1.3.4: Missing Data and Sampling Weights (brief intro)",
    "section": "3. Missing Data Handling and Imputation Methods (brief intro)",
    "text": "3. Missing Data Handling and Imputation Methods (brief intro)\nImputation - Mean Substitution\nThere are many ideas and options for creating data to “fill-in” or “impute” the spots for the missing values. Keep in mind that these methods are “making up” new (unobserved) data. Ideally, the missing imputation methods should be as unbiased as possible and should not increase or decrease the variability of the data. As good as some methods may be, always keep in mind that we will never know for sure if these new imputed data are “correct” or if they are the best they can be.\nA simple example, suppose we have a variable with 5 numbers. Let’s compute the mean and standard deviation of these numbers.\n\nx &lt;- c(2, 4, 3, 5, 10)\nmean(x, na.rm = TRUE)\n\n[1] 4.8\n\nsd(x, na.rm = TRUE)\n\n[1] 3.114482\n\n\nNow let’s set the 1st value to missing - replace the 2 with NA. Notice that the mean increases and the standard deviation decreased slightly from 3.114 to 3.109.\n\nxna &lt;- c(NA, 4, 3, 5, 10)\nmean(xna, na.rm = TRUE)\n\n[1] 5.5\n\nsd(xna, na.rm = TRUE)\n\n[1] 3.109126\n\n\n\nNow, let’s take the mean we just computed from the 4 non-missing values which was 5.5 and substitute it in for the missing value an recompute the new mean and standard deviation.\n\nxsub &lt;- c(5.5, 4, 3, 5, 10)\nmean(xsub, na.rm = TRUE)\n\n[1] 5.5\n\nsd(xsub, na.rm = TRUE)\n\n[1] 2.692582\n\n\nIMPORTANT The new mean of xsub is the same = 5.5 as the mean for xna, but the standard deviation for this new list of 5 numbers is much smaller = 2.693 down from 3.109. And this “shrinking” of the standard deviation is even more pronounced from the original 5 numbers which was 3.114 for x. And the mean of the original numbers was 4.8 which is smaller than the mean-subtituted list for xsub.\nThis is a simple example and illustrates why we don’t want to use mean-substitution for missing data. It also highlights that the goals of missing data imputation ideally shouldn’t cause bias for estimating statistics like the mean and shouldn’t increase or decrease the underlying variance of the original variables. And in theory the new variable with imputed data also shouldn’t change the correlations (relationships) between the variables in the dataset. As you can see this can get complex rapidly and no imputation method is perfect. Often multiple missing inputation methods should be explored and some methods may work better for different models and statistical tests than others.\n\n\n\n\n\n\nCommonly accepted use of mean-substitution\n\n\n\nThe one place where I see mean substitution used often is for survey instruments. For example the CESD (Center for Epidemiological Studies-Depression) which has 20 items with a 4-point Likert-scaled response coded 0, 1, 2, 3.\nFor this instrument you can compute a valid score from only 16 of the 20 items by taking the mean of these 16 items. This is equivalent to using mean-substitution for the 4 missing responses (e.g. CESD allows up to 20% missing items within a given subject, within a given row of data). But this is ROWWISE mean substitution. The example above is illustrating COLUMNWISE mean substitution.\nTo avoid mean-substitution bias even for ROWWISE substitution, you ideally want to use this on less than 5%-10% of your sample. If more than 5%-10% of your sample is not completing the survey, there may still be underlying response bias issues that need to be addressed.\n\n\n\nExample of k-nearest neighbor (kNN) missing imputation method\nInstead of using mean substitution, let’s look at another method - k-nearest neighbor, which is a “donor-based” method. Learn more at:\n\nVIM Vignette on “Donor based Imputation Methods”\nDatacamp Course on Imputation\n\nLet’s take the little dataset x which is a subset of the sleep dataset which has all 62 rows but only the Dream and Sleep variables. For these 2 variables, let’s see what the kNN() (k-nearest neighbor) function in the VIM package does.\n\nx &lt;- sleep[, c(\"Dream\", \"Sleep\")]\nx_imputed &lt;- kNN(x)\n\nNow look at a scatterplot plot for these new Dream and Sleep variables with imputed values from the k-nearest neighbor approach. Notice the coloring of the points - the blue are the original values and the other colors represent the structure of missings.\n\nbrown points represent values where Dream was missing initially\nbeige points represent values where Sleep was missing initially\nblack points represent values where both Dream and Sleep were missing initially\n\nThe kNN() method appears to preserve the correlation between Dream and Sleep.\n\nmarginplot(x_imputed, delimiter = \"_imp\")\n\n\n\n\n\n\n\n\nkNN imputed values - compare correlations before and after\nLet’s compare the results before and after the imputation for correlation and for a simple regression model. This is a “sensitivity” test of sorts. It is always a good idea to compare the results before and after applying any imputation method.\nCorrelation Original Data:\n\nx %&gt;%\n  as.matrix() %&gt;%\n  Hmisc::rcorr()\n\n      Dream Sleep\nDream  1.00  0.73\nSleep  0.73  1.00\n\nn\n      Dream Sleep\nDream    50    48\nSleep    48    58\n\nP\n      Dream Sleep\nDream        0   \nSleep  0         \n\n\nCorrelation kNN Imputed Data:\n\nx_imputed %&gt;%\n  select(Dream, Sleep) %&gt;%\n  as.matrix() %&gt;%\n  Hmisc::rcorr()\n\n      Dream Sleep\nDream  1.00  0.74\nSleep  0.74  1.00\n\nn= 62 \n\n\nP\n      Dream Sleep\nDream        0   \nSleep  0         \n\n\n\nkNN imputed values - compare regression before and after\nSimple Linear Regression Original Data:\n\nsummary(lm(Sleep ~ Dream, data = x))\n\n\nCall:\nlm(formula = Sleep ~ Dream, data = x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2765 -2.0384 -0.1096  2.1599  9.2624 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.0273     0.7960   7.572 1.27e-09 ***\nDream         2.3051     0.3209   7.183 4.85e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.178 on 46 degrees of freedom\n  (14 observations deleted due to missingness)\nMultiple R-squared:  0.5287,    Adjusted R-squared:  0.5184 \nF-statistic: 51.59 on 1 and 46 DF,  p-value: 4.849e-09\n\n\nSimple Linear Regression kNN Imputed Data:\n\nsummary(lm(Sleep ~ Dream, data = x_imputed))\n\n\nCall:\nlm(formula = Sleep ~ Dream, data = x_imputed)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.241 -2.283 -0.221  2.157  9.257 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.6222     0.6920   8.125 3.00e-11 ***\nDream         2.5106     0.2949   8.512 6.58e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.074 on 60 degrees of freedom\nMultiple R-squared:  0.547, Adjusted R-squared:  0.5395 \nF-statistic: 72.46 on 1 and 60 DF,  p-value: 6.579e-12\n\n\nNotice that the correlations were similar. The regression intercepts and slopes were slightly different and the “Std. Error” for the regression coefficients for the imputed model were smaller than for the original data.\n\nExample of multiple missing imputation method (using mice)\nLet’s take a look at the mice (Multivariate Imputation by Chained Equations) package. The mice package provides for Multiple imputation using Fully Conditional Specification (FCS) implemented by the MICE algorithm as described in Van Buuren and Groothuis-Oudshoorn (2011).\nLet’s re-run the simple linear regression model above, but this time let’s create 20 imputed datasets, run 20 regression models and then pool the results. See FIMD Book Section 1.4\nCompare these regression results to the models above. Notice that the “std.error” for the regression coefficients are larger than they were for the kNN results and closer to fitting the model with the original data.\n\nlibrary(mice)\nimp &lt;- mice(x, seed = 1, m = 20, print = FALSE)\nfit &lt;- with(imp, lm(Sleep ~ Dream))\nsummary(pool(fit))\n\n         term estimate std.error statistic       df      p.value\n1 (Intercept) 5.904120 0.7560889  7.808764 48.78611 3.858037e-10\n2       Dream 2.326442 0.3216144  7.233638 45.29561 4.431364e-09\n\n\n\n\n\n\n\n\nLook at one of the regression models from the impute data\n\n\n\nWe can “peek under the hood” to look at one of the individual imputed datasets by running the following code. If you’d like to look at another model just change 1 to another number up to 20 (since there were 20 imputations performed).\n\n# see simple output\nfit[[\"analyses\"]][[1]]\n\n# get summary detailed output\nsummary(fit[[\"analyses\"]][[1]])\n\n\n\n\n\n\nCall:\nlm(formula = Sleep ~ Dream)\n\nCoefficients:\n(Intercept)        Dream  \n      6.090        2.216  \n\n\n\nCall:\nlm(formula = Sleep ~ Dream)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.4782 -2.0541  0.0424  2.2630  9.3787 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.0897     0.7318   8.322 1.39e-11 ***\nDream         2.2158     0.2953   7.504 3.45e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.407 on 60 degrees of freedom\nMultiple R-squared:  0.4841,    Adjusted R-squared:  0.4755 \nF-statistic: 56.31 on 1 and 60 DF,  p-value: 3.449e-10",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#impact-of-sampling-weights-for-survey-data-brief-intro",
    "href": "module134_MissingWeight.html#impact-of-sampling-weights-for-survey-data-brief-intro",
    "title": "1.3.4: Missing Data and Sampling Weights (brief intro)",
    "section": "4. Impact of Sampling Weights for Survey Data (brief intro)",
    "text": "4. Impact of Sampling Weights for Survey Data (brief intro)\nsee Missing Data in PRAMS module.",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#r-code-for-this-module",
    "href": "module134_MissingWeight.html#r-code-for-this-module",
    "title": "1.3.4: Missing Data and Sampling Weights (brief intro)",
    "section": "R Code For This Module",
    "text": "R Code For This Module\n\nmodule_134.R",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#references",
    "href": "module134_MissingWeight.html#references",
    "title": "1.3.4: Missing Data and Sampling Weights (brief intro)",
    "section": "References",
    "text": "References\n\n\nArel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in R.” Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\n———. 2025a. Modelsummary: Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready. https://modelsummary.com.\n\n\n———. 2025b. Tinytable: Simple and Configurable Tables in HTML, LaTeX, Markdown, Word, PNG, PDF, and Typst Formats. https://vincentarelbundock.github.io/tinytable/.\n\n\nHarrell Jr, Frank E. 2025. Hmisc: Harrell Miscellaneous. https://hbiostat.org/R/Hmisc/.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2022. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://allisonhorst.github.io/palmerpenguins/.\n\n\nIhaka, Ross, Paul Murrell, Kurt Hornik, Jason C. Fisher, Reto Stauffer, Claus O. Wilke, Claire D. McWhite, and Achim Zeileis. 2023. Colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes. https://colorspace.R-Forge.R-project.org/.\n\n\nKowarik, Alexander, and Matthias Templ. 2016. “Imputation with the R Package VIM.” Journal of Statistical Software 74 (7): 1–16. https://doi.org/10.18637/jss.v074.i07.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSjoberg, Daniel D., Joseph Larmarange, Michael Curry, Jessica Lavery, Karissa Whiting, and Emily C. Zabor. 2024. Gtsummary: Presentation-Ready Data Summary and Analytic Result Tables. https://github.com/ddsjoberg/gtsummary.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery, and Joseph Larmarange. 2021. “Reproducible Summary Tables with the Gtsummary Package.” The R Journal 13: 570–80. https://doi.org/10.32614/RJ-2021-053.\n\n\nStauffer, Reto, Georg J. Mayr, Markus Dabernig, and Achim Zeileis. 2009. “Somewhere over the Rainbow: How to Make Effective Use of Colors in Meteorological Visualizations.” Bulletin of the American Meteorological Society 96 (2): 203–16. https://doi.org/10.1175/BAMS-D-13-00155.1.\n\n\nTempl, Matthias, Alexander Kowarik, Andreas Alfons, Gregor de Cillia, and Wolfgang Rannetbauer. 2022. VIM: Visualization and Imputation of Missing Values. https://github.com/statistikat/VIM.\n\n\nTierney, Nicholas, and Dianne Cook. 2023. “Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.” Journal of Statistical Software 105 (7): 1–31. https://doi.org/10.18637/jss.v105.i07.\n\n\nTierney, Nicholas, Di Cook, Miles McBain, and Colin Fay. 2024. Naniar: Data Structures, Summaries, and Visualisations for Missing Data. https://github.com/njtierney/naniar.\n\n\nvan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software 45 (3): 1–67. https://doi.org/10.18637/jss.v045.i03.\n\n\n———. 2025. Mice: Multivariate Imputation by Chained Equations. https://github.com/amices/mice.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible Summaries of Data. https://docs.ropensci.org/skimr/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nZeileis, Achim, Jason C. Fisher, Kurt Hornik, Ross Ihaka, Claire D. McWhite, Paul Murrell, Reto Stauffer, and Claus O. Wilke. 2020. “colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes.” Journal of Statistical Software 96 (1): 1–49. https://doi.org/10.18637/jss.v096.i01.\n\n\nZeileis, Achim, Kurt Hornik, and Paul Murrell. 2009. “Escaping RGBland: Selecting Colors for Statistical Graphics.” Computational Statistics & Data Analysis 53 (9): 3259–70. https://doi.org/10.1016/j.csda.2008.11.033.",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#other-helpful-resources",
    "href": "module134_MissingWeight.html#other-helpful-resources",
    "title": "1.3.4: Missing Data and Sampling Weights (brief intro)",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nMissing Data Resources\n\nCRAN Task View for Missing Data\nR-miss-tastic Website\nFlexible Imputation of Missing Data (online book for 2nd edition) by Stef van Buuren\nBlog post on Missing Data Visualization in R using ggplot2\nMissing data R tutorial\nCRAN Task View on Missing Data\nA resource website on missing values\nHandling missing values with R - tutorial\nBlog post “My favourite R package for: summarising data”\n\nand\nOther Helpful Resources",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module136_ReproducibleResearch.html",
    "href": "module136_ReproducibleResearch.html",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "",
    "text": "Module “1.3.6: Putting Reproducible Research Principles Into Practice” will be posted prior to the In-Person Workshops in Summer 2025.",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  },
  {
    "objectID": "module136_ReproducibleResearch.html#coming-summer-2025",
    "href": "module136_ReproducibleResearch.html#coming-summer-2025",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "",
    "text": "Module “1.3.6: Putting Reproducible Research Principles Into Practice” will be posted prior to the In-Person Workshops in Summer 2025.",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  },
  {
    "objectID": "module136_ReproducibleResearch.html#session-objectives",
    "href": "module136_ReproducibleResearch.html#session-objectives",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "Session Objectives",
    "text": "Session Objectives\n\nDiscuss reproducible research principles.\nApply reproducible research principles to data analysis using R Markdown.\n\nKey points to cover:\n\nReproducible research principles\nWhat is R Markdown\nHow to create a report using R Markdown\n\nCustomize the layout of presentations or reports\nInsert and create objects, such as tables, images, or videos, within a document",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  }
]