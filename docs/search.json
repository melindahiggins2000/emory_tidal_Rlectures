[
  {
    "objectID": "module135_StatisticalTests.html",
    "href": "module135_StatisticalTests.html",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "",
    "text": "Module “1.3.5: Statistical Tests and Models” will be posted prior to the In-Person Workshops in Summer 2025.",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#coming-summer-2025",
    "href": "module135_StatisticalTests.html#coming-summer-2025",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "",
    "text": "Module “1.3.5: Statistical Tests and Models” will be posted prior to the In-Person Workshops in Summer 2025.",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module135_StatisticalTests.html#session-objectives",
    "href": "module135_StatisticalTests.html#session-objectives",
    "title": "1.3.5: Statistical Tests and Models",
    "section": "Session Objectives",
    "text": "Session Objectives\n\nDevelop linear and logistic regression models.\n(Use a survey sampling weight to generate more representative descriptive and inferential statistical values.) - Currently, this objective is under the Module 1.3.4: Missing data and sampling weight.\nInterpret a model output.\n\nKey points to cover:\n\nRun multivariate linear regression models with R.\nRun multivariate logistic regression models with R.\nInclude interaction terms in regression models.\n(R packages for complex survey data (e.g., survey package)\n\nR codes to generate weighted descriptive statistics and contingency tables, as well as to develop weighted linear models)\n\n\nInterpret a model output.\n(Compare the outputs of unweighted and weighted models.)",
    "crumbs": [
      "1.3.5: Statistical Tests and Models"
    ]
  },
  {
    "objectID": "module133_DataVis.html",
    "href": "module133_DataVis.html",
    "title": "1.3.3: Data Visualization",
    "section": "",
    "text": "To visualize data using different R packages.\n\nKey points to cover:\n\nIntroduce to ggplot2 and other R packages.\nVisualize one, two, or more variables at a time.\nIntroduce other resources (e.g., books, blogs, or websites) trainees can refer to.",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#session-objectives",
    "href": "module133_DataVis.html#session-objectives",
    "title": "1.3.3: Data Visualization",
    "section": "",
    "text": "To visualize data using different R packages.\n\nKey points to cover:\n\nIntroduce to ggplot2 and other R packages.\nVisualize one, two, or more variables at a time.\nIntroduce other resources (e.g., books, blogs, or websites) trainees can refer to.",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#prework---before-you-begin",
    "href": "module133_DataVis.html#prework---before-you-begin",
    "title": "1.3.3: Data Visualization",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\nA. Install packages\nIf you do not have them already, install the following packages from CRAN:\n\n`ggplot2\nggthemes\nreadr\nB. Open/create your RStudio project\nLet’s start with the myfirstRproject RStudio project you created in Module 1.3.2 - part 1. If you have not yet created this myfirstRproject RStudio project, go ahead and create a new RStudio Project for this lesson. Feel free to name your project whatever you want, it does not need to be named myfirstRproject.\nC. Create a new R script and load data into your computing session\nAt the end of Module 1.3.2 - part 6 you saved the mydata dataset in the mydata.RData R binary format.\n\nGo ahead and create a new R script (*.R) for this computing session. We did this already in Module 1.3.1 - part 3 - refer to this section to remember how to create a new R script.\nPut this code into your new R script (*.R) to load mydata.RData into your current computing session.\n\n\n# load mydata\nload(file = \"mydata.RData\")\n\n\n\n\n\n\n\nData must/should be in your RStudio project\n\n\n\nREMEMBER R/RStudio automatically looks in your current RStudio project folder for all files for your current computing session. So, make sure the mydata.RData file is in your current RStudio project myfirstRproject folder on your computer.\nFor a more detailed overview of RStudio projects:\n\nread “Chapter 6: R projects” in the The Epidemiologist R Handbook and\nrefer to “Chapter 45 Directory interactions” in the The Epidemiologist R Handbook.\n\n\n\nD. Get Inspired!\n\nGet Inspired at The R Graph Gallery\n\nAlso see the Top Curated R Graphs\n\nAlso see Additional Resources - R Graphics",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#base-r-graphical-functions",
    "href": "module133_DataVis.html#base-r-graphical-functions",
    "title": "1.3.3: Data Visualization",
    "section": "1. Base R graphical functions",
    "text": "1. Base R graphical functions\nThe base R graphics package is very powerful on its own. As you saw in 1.3.1: Introduction to R and R Studio, we can make a simple 2-dimensional scatterplot with the plot() function.\nBase R - Scatterplot\nFor example, let’s make a plot of Height on the X-axis (horizontal) and WeightPRE on the Y-axis (vertical) from the mydata dataset. Since we are using base R function, we have to use the $selector to identify the variables we want inside the mydata dataset.\nLearn more about the plot() function and arguments by running help(plot, package = \"graphics\").\n\nplot(x = mydata$Height,\n     y = mydata$WeightPRE)\n\n\n\n\n\n\n\nThe plot does look a little odd - this is due to some data errors in the mydata dataset. We will fix these below. But for now, you can “see” that these data may have some issues that need to be addressed. For example:\n\nThere are 2 people with heights &lt; 5 feet tall which may be suspect\nThere are 2 people with a weight &lt; 100 pounds which may be data entry errors or incorrect units\n\nFor now, let’s add some additional graphical elements:\n\na better label for the x-axis\na better label for the y-axis\na title for the graph\na subtitle for the graph\n\n\nplot(x = mydata$Height,\n     y = mydata$WeightPRE,\n     xlab = \"Height (in decimal inches)\",\n     ylab = \"Weight (in pounds) - before intervention\",\n     main = \"Weight by Height in the Mydata Project\",\n     sub = \"Hypothetical Madeup mydata Dataset\")\n\n\n\n\n\n\n\nAnd we could also add color and change the shapes - for example, let’s color and shape the points by GenderCoded, the numeric coding for gender where 1=Male, 2=Female.\n\n\n\n\n\n\nPlot code inspiration\n\n\n\nI pulled this code together from code examples at:\n\nStackoverflow post on using pch\nSTHDA post on point shapes\n\n\n\n\nplot(x = mydata$Height,\n     y = mydata$WeightPRE,\n     col = c(\"blue\", \"green\")[mydata$GenderCoded],\n     pch = c(15, 19)[mydata$GenderCoded],\n     xlab = \"Height (in decimal inches)\",\n     ylab = \"Weight (in pounds) - before intervention\",\n     main = \"Weight by Height in the Mydata Project\",\n     sub = \"Hypothetical Madeup mydata Dataset\")\n\n\n\n\n\n\n\nThe STHDA website on “R Base Graphs” has a nice walkthrough of using the base R graphics package to make really nice plots.\nBase R - Histogram\nAs we noted above, let’s take a look at the distribution of the heights in the mydata dataset. There is a specific hist() function in the graphics package for making histograms, learn more by running help(hist, package = \"graphics\").\nNotice that we can use some of the same arguments as we did above for plot().\n\nhist(mydata$Height,\n     xlab = \"Height (in decimal inches)\",\n     col = \"lightblue\",\n     border = \"black\",\n     main = \"Histogram of Heights\",\n     sub = \"Hypothetical Madeup mydata Dataset\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nColors available\n\n\n\nThere are 657 names colors immediately available to you from the built-in grDevices Base R package which works in conjunction with graphics. You can view the names of all of these colors by running colors(). You can also learn more at:\n\nhttps://www.sthda.com/english/wiki/colors-in-r#google_vignette\nhttps://r-graph-gallery.com/42-colors-names.html\n\nhttps://r-graph-gallery.com/ggplot2-color.html - which explains how colors can be specified using the built-in color names, but cal also be specified using RGB (red, green, blue) indexes or even Hexcodes for which there are many online tools like https://htmlcolorcodes.com/.\n\n\n\n\n# list built-in colors\ncolors()\n\n  [1] \"white\"                \"aliceblue\"            \"antiquewhite\"        \n  [4] \"antiquewhite1\"        \"antiquewhite2\"        \"antiquewhite3\"       \n  [7] \"antiquewhite4\"        \"aquamarine\"           \"aquamarine1\"         \n [10] \"aquamarine2\"          \"aquamarine3\"          \"aquamarine4\"         \n [13] \"azure\"                \"azure1\"               \"azure2\"              \n [16] \"azure3\"               \"azure4\"               \"beige\"               \n [19] \"bisque\"               \"bisque1\"              \"bisque2\"             \n [22] \"bisque3\"              \"bisque4\"              \"black\"               \n [25] \"blanchedalmond\"       \"blue\"                 \"blue1\"               \n [28] \"blue2\"                \"blue3\"                \"blue4\"               \n [31] \"blueviolet\"           \"brown\"                \"brown1\"              \n [34] \"brown2\"               \"brown3\"               \"brown4\"              \n [37] \"burlywood\"            \"burlywood1\"           \"burlywood2\"          \n [40] \"burlywood3\"           \"burlywood4\"           \"cadetblue\"           \n [43] \"cadetblue1\"           \"cadetblue2\"           \"cadetblue3\"          \n [46] \"cadetblue4\"           \"chartreuse\"           \"chartreuse1\"         \n [49] \"chartreuse2\"          \"chartreuse3\"          \"chartreuse4\"         \n [52] \"chocolate\"            \"chocolate1\"           \"chocolate2\"          \n [55] \"chocolate3\"           \"chocolate4\"           \"coral\"               \n [58] \"coral1\"               \"coral2\"               \"coral3\"              \n [61] \"coral4\"               \"cornflowerblue\"       \"cornsilk\"            \n [64] \"cornsilk1\"            \"cornsilk2\"            \"cornsilk3\"           \n [67] \"cornsilk4\"            \"cyan\"                 \"cyan1\"               \n [70] \"cyan2\"                \"cyan3\"                \"cyan4\"               \n [73] \"darkblue\"             \"darkcyan\"             \"darkgoldenrod\"       \n [76] \"darkgoldenrod1\"       \"darkgoldenrod2\"       \"darkgoldenrod3\"      \n [79] \"darkgoldenrod4\"       \"darkgray\"             \"darkgreen\"           \n [82] \"darkgrey\"             \"darkkhaki\"            \"darkmagenta\"         \n [85] \"darkolivegreen\"       \"darkolivegreen1\"      \"darkolivegreen2\"     \n [88] \"darkolivegreen3\"      \"darkolivegreen4\"      \"darkorange\"          \n [91] \"darkorange1\"          \"darkorange2\"          \"darkorange3\"         \n [94] \"darkorange4\"          \"darkorchid\"           \"darkorchid1\"         \n [97] \"darkorchid2\"          \"darkorchid3\"          \"darkorchid4\"         \n[100] \"darkred\"              \"darksalmon\"           \"darkseagreen\"        \n[103] \"darkseagreen1\"        \"darkseagreen2\"        \"darkseagreen3\"       \n[106] \"darkseagreen4\"        \"darkslateblue\"        \"darkslategray\"       \n[109] \"darkslategray1\"       \"darkslategray2\"       \"darkslategray3\"      \n[112] \"darkslategray4\"       \"darkslategrey\"        \"darkturquoise\"       \n[115] \"darkviolet\"           \"deeppink\"             \"deeppink1\"           \n[118] \"deeppink2\"            \"deeppink3\"            \"deeppink4\"           \n[121] \"deepskyblue\"          \"deepskyblue1\"         \"deepskyblue2\"        \n[124] \"deepskyblue3\"         \"deepskyblue4\"         \"dimgray\"             \n[127] \"dimgrey\"              \"dodgerblue\"           \"dodgerblue1\"         \n[130] \"dodgerblue2\"          \"dodgerblue3\"          \"dodgerblue4\"         \n[133] \"firebrick\"            \"firebrick1\"           \"firebrick2\"          \n[136] \"firebrick3\"           \"firebrick4\"           \"floralwhite\"         \n[139] \"forestgreen\"          \"gainsboro\"            \"ghostwhite\"          \n[142] \"gold\"                 \"gold1\"                \"gold2\"               \n[145] \"gold3\"                \"gold4\"                \"goldenrod\"           \n[148] \"goldenrod1\"           \"goldenrod2\"           \"goldenrod3\"          \n[151] \"goldenrod4\"           \"gray\"                 \"gray0\"               \n[154] \"gray1\"                \"gray2\"                \"gray3\"               \n[157] \"gray4\"                \"gray5\"                \"gray6\"               \n[160] \"gray7\"                \"gray8\"                \"gray9\"               \n[163] \"gray10\"               \"gray11\"               \"gray12\"              \n[166] \"gray13\"               \"gray14\"               \"gray15\"              \n[169] \"gray16\"               \"gray17\"               \"gray18\"              \n[172] \"gray19\"               \"gray20\"               \"gray21\"              \n[175] \"gray22\"               \"gray23\"               \"gray24\"              \n[178] \"gray25\"               \"gray26\"               \"gray27\"              \n[181] \"gray28\"               \"gray29\"               \"gray30\"              \n[184] \"gray31\"               \"gray32\"               \"gray33\"              \n[187] \"gray34\"               \"gray35\"               \"gray36\"              \n[190] \"gray37\"               \"gray38\"               \"gray39\"              \n[193] \"gray40\"               \"gray41\"               \"gray42\"              \n[196] \"gray43\"               \"gray44\"               \"gray45\"              \n[199] \"gray46\"               \"gray47\"               \"gray48\"              \n[202] \"gray49\"               \"gray50\"               \"gray51\"              \n[205] \"gray52\"               \"gray53\"               \"gray54\"              \n[208] \"gray55\"               \"gray56\"               \"gray57\"              \n[211] \"gray58\"               \"gray59\"               \"gray60\"              \n[214] \"gray61\"               \"gray62\"               \"gray63\"              \n[217] \"gray64\"               \"gray65\"               \"gray66\"              \n[220] \"gray67\"               \"gray68\"               \"gray69\"              \n[223] \"gray70\"               \"gray71\"               \"gray72\"              \n[226] \"gray73\"               \"gray74\"               \"gray75\"              \n[229] \"gray76\"               \"gray77\"               \"gray78\"              \n[232] \"gray79\"               \"gray80\"               \"gray81\"              \n[235] \"gray82\"               \"gray83\"               \"gray84\"              \n[238] \"gray85\"               \"gray86\"               \"gray87\"              \n[241] \"gray88\"               \"gray89\"               \"gray90\"              \n[244] \"gray91\"               \"gray92\"               \"gray93\"              \n[247] \"gray94\"               \"gray95\"               \"gray96\"              \n[250] \"gray97\"               \"gray98\"               \"gray99\"              \n[253] \"gray100\"              \"green\"                \"green1\"              \n[256] \"green2\"               \"green3\"               \"green4\"              \n[259] \"greenyellow\"          \"grey\"                 \"grey0\"               \n[262] \"grey1\"                \"grey2\"                \"grey3\"               \n[265] \"grey4\"                \"grey5\"                \"grey6\"               \n[268] \"grey7\"                \"grey8\"                \"grey9\"               \n[271] \"grey10\"               \"grey11\"               \"grey12\"              \n[274] \"grey13\"               \"grey14\"               \"grey15\"              \n[277] \"grey16\"               \"grey17\"               \"grey18\"              \n[280] \"grey19\"               \"grey20\"               \"grey21\"              \n[283] \"grey22\"               \"grey23\"               \"grey24\"              \n[286] \"grey25\"               \"grey26\"               \"grey27\"              \n[289] \"grey28\"               \"grey29\"               \"grey30\"              \n[292] \"grey31\"               \"grey32\"               \"grey33\"              \n[295] \"grey34\"               \"grey35\"               \"grey36\"              \n[298] \"grey37\"               \"grey38\"               \"grey39\"              \n[301] \"grey40\"               \"grey41\"               \"grey42\"              \n[304] \"grey43\"               \"grey44\"               \"grey45\"              \n[307] \"grey46\"               \"grey47\"               \"grey48\"              \n[310] \"grey49\"               \"grey50\"               \"grey51\"              \n[313] \"grey52\"               \"grey53\"               \"grey54\"              \n[316] \"grey55\"               \"grey56\"               \"grey57\"              \n[319] \"grey58\"               \"grey59\"               \"grey60\"              \n[322] \"grey61\"               \"grey62\"               \"grey63\"              \n[325] \"grey64\"               \"grey65\"               \"grey66\"              \n[328] \"grey67\"               \"grey68\"               \"grey69\"              \n[331] \"grey70\"               \"grey71\"               \"grey72\"              \n[334] \"grey73\"               \"grey74\"               \"grey75\"              \n[337] \"grey76\"               \"grey77\"               \"grey78\"              \n[340] \"grey79\"               \"grey80\"               \"grey81\"              \n[343] \"grey82\"               \"grey83\"               \"grey84\"              \n[346] \"grey85\"               \"grey86\"               \"grey87\"              \n[349] \"grey88\"               \"grey89\"               \"grey90\"              \n[352] \"grey91\"               \"grey92\"               \"grey93\"              \n[355] \"grey94\"               \"grey95\"               \"grey96\"              \n[358] \"grey97\"               \"grey98\"               \"grey99\"              \n[361] \"grey100\"              \"honeydew\"             \"honeydew1\"           \n[364] \"honeydew2\"            \"honeydew3\"            \"honeydew4\"           \n[367] \"hotpink\"              \"hotpink1\"             \"hotpink2\"            \n[370] \"hotpink3\"             \"hotpink4\"             \"indianred\"           \n[373] \"indianred1\"           \"indianred2\"           \"indianred3\"          \n[376] \"indianred4\"           \"ivory\"                \"ivory1\"              \n[379] \"ivory2\"               \"ivory3\"               \"ivory4\"              \n[382] \"khaki\"                \"khaki1\"               \"khaki2\"              \n[385] \"khaki3\"               \"khaki4\"               \"lavender\"            \n[388] \"lavenderblush\"        \"lavenderblush1\"       \"lavenderblush2\"      \n[391] \"lavenderblush3\"       \"lavenderblush4\"       \"lawngreen\"           \n[394] \"lemonchiffon\"         \"lemonchiffon1\"        \"lemonchiffon2\"       \n[397] \"lemonchiffon3\"        \"lemonchiffon4\"        \"lightblue\"           \n[400] \"lightblue1\"           \"lightblue2\"           \"lightblue3\"          \n[403] \"lightblue4\"           \"lightcoral\"           \"lightcyan\"           \n[406] \"lightcyan1\"           \"lightcyan2\"           \"lightcyan3\"          \n[409] \"lightcyan4\"           \"lightgoldenrod\"       \"lightgoldenrod1\"     \n[412] \"lightgoldenrod2\"      \"lightgoldenrod3\"      \"lightgoldenrod4\"     \n[415] \"lightgoldenrodyellow\" \"lightgray\"            \"lightgreen\"          \n[418] \"lightgrey\"            \"lightpink\"            \"lightpink1\"          \n[421] \"lightpink2\"           \"lightpink3\"           \"lightpink4\"          \n[424] \"lightsalmon\"          \"lightsalmon1\"         \"lightsalmon2\"        \n[427] \"lightsalmon3\"         \"lightsalmon4\"         \"lightseagreen\"       \n[430] \"lightskyblue\"         \"lightskyblue1\"        \"lightskyblue2\"       \n[433] \"lightskyblue3\"        \"lightskyblue4\"        \"lightslateblue\"      \n[436] \"lightslategray\"       \"lightslategrey\"       \"lightsteelblue\"      \n[439] \"lightsteelblue1\"      \"lightsteelblue2\"      \"lightsteelblue3\"     \n[442] \"lightsteelblue4\"      \"lightyellow\"          \"lightyellow1\"        \n[445] \"lightyellow2\"         \"lightyellow3\"         \"lightyellow4\"        \n[448] \"limegreen\"            \"linen\"                \"magenta\"             \n[451] \"magenta1\"             \"magenta2\"             \"magenta3\"            \n[454] \"magenta4\"             \"maroon\"               \"maroon1\"             \n[457] \"maroon2\"              \"maroon3\"              \"maroon4\"             \n[460] \"mediumaquamarine\"     \"mediumblue\"           \"mediumorchid\"        \n[463] \"mediumorchid1\"        \"mediumorchid2\"        \"mediumorchid3\"       \n[466] \"mediumorchid4\"        \"mediumpurple\"         \"mediumpurple1\"       \n[469] \"mediumpurple2\"        \"mediumpurple3\"        \"mediumpurple4\"       \n[472] \"mediumseagreen\"       \"mediumslateblue\"      \"mediumspringgreen\"   \n[475] \"mediumturquoise\"      \"mediumvioletred\"      \"midnightblue\"        \n[478] \"mintcream\"            \"mistyrose\"            \"mistyrose1\"          \n[481] \"mistyrose2\"           \"mistyrose3\"           \"mistyrose4\"          \n[484] \"moccasin\"             \"navajowhite\"          \"navajowhite1\"        \n[487] \"navajowhite2\"         \"navajowhite3\"         \"navajowhite4\"        \n[490] \"navy\"                 \"navyblue\"             \"oldlace\"             \n[493] \"olivedrab\"            \"olivedrab1\"           \"olivedrab2\"          \n[496] \"olivedrab3\"           \"olivedrab4\"           \"orange\"              \n[499] \"orange1\"              \"orange2\"              \"orange3\"             \n[502] \"orange4\"              \"orangered\"            \"orangered1\"          \n[505] \"orangered2\"           \"orangered3\"           \"orangered4\"          \n[508] \"orchid\"               \"orchid1\"              \"orchid2\"             \n[511] \"orchid3\"              \"orchid4\"              \"palegoldenrod\"       \n[514] \"palegreen\"            \"palegreen1\"           \"palegreen2\"          \n[517] \"palegreen3\"           \"palegreen4\"           \"paleturquoise\"       \n[520] \"paleturquoise1\"       \"paleturquoise2\"       \"paleturquoise3\"      \n[523] \"paleturquoise4\"       \"palevioletred\"        \"palevioletred1\"      \n[526] \"palevioletred2\"       \"palevioletred3\"       \"palevioletred4\"      \n[529] \"papayawhip\"           \"peachpuff\"            \"peachpuff1\"          \n[532] \"peachpuff2\"           \"peachpuff3\"           \"peachpuff4\"          \n[535] \"peru\"                 \"pink\"                 \"pink1\"               \n[538] \"pink2\"                \"pink3\"                \"pink4\"               \n[541] \"plum\"                 \"plum1\"                \"plum2\"               \n[544] \"plum3\"                \"plum4\"                \"powderblue\"          \n[547] \"purple\"               \"purple1\"              \"purple2\"             \n[550] \"purple3\"              \"purple4\"              \"red\"                 \n[553] \"red1\"                 \"red2\"                 \"red3\"                \n[556] \"red4\"                 \"rosybrown\"            \"rosybrown1\"          \n[559] \"rosybrown2\"           \"rosybrown3\"           \"rosybrown4\"          \n[562] \"royalblue\"            \"royalblue1\"           \"royalblue2\"          \n[565] \"royalblue3\"           \"royalblue4\"           \"saddlebrown\"         \n[568] \"salmon\"               \"salmon1\"              \"salmon2\"             \n[571] \"salmon3\"              \"salmon4\"              \"sandybrown\"          \n[574] \"seagreen\"             \"seagreen1\"            \"seagreen2\"           \n[577] \"seagreen3\"            \"seagreen4\"            \"seashell\"            \n[580] \"seashell1\"            \"seashell2\"            \"seashell3\"           \n[583] \"seashell4\"            \"sienna\"               \"sienna1\"             \n[586] \"sienna2\"              \"sienna3\"              \"sienna4\"             \n[589] \"skyblue\"              \"skyblue1\"             \"skyblue2\"            \n[592] \"skyblue3\"             \"skyblue4\"             \"slateblue\"           \n[595] \"slateblue1\"           \"slateblue2\"           \"slateblue3\"          \n[598] \"slateblue4\"           \"slategray\"            \"slategray1\"          \n[601] \"slategray2\"           \"slategray3\"           \"slategray4\"          \n[604] \"slategrey\"            \"snow\"                 \"snow1\"               \n[607] \"snow2\"                \"snow3\"                \"snow4\"               \n[610] \"springgreen\"          \"springgreen1\"         \"springgreen2\"        \n[613] \"springgreen3\"         \"springgreen4\"         \"steelblue\"           \n[616] \"steelblue1\"           \"steelblue2\"           \"steelblue3\"          \n[619] \"steelblue4\"           \"tan\"                  \"tan1\"                \n[622] \"tan2\"                 \"tan3\"                 \"tan4\"                \n[625] \"thistle\"              \"thistle1\"             \"thistle2\"            \n[628] \"thistle3\"             \"thistle4\"             \"tomato\"              \n[631] \"tomato1\"              \"tomato2\"              \"tomato3\"             \n[634] \"tomato4\"              \"turquoise\"            \"turquoise1\"          \n[637] \"turquoise2\"           \"turquoise3\"           \"turquoise4\"          \n[640] \"violet\"               \"violetred\"            \"violetred1\"          \n[643] \"violetred2\"           \"violetred3\"           \"violetred4\"          \n[646] \"wheat\"                \"wheat1\"               \"wheat2\"              \n[649] \"wheat3\"               \"wheat4\"               \"whitesmoke\"          \n[652] \"yellow\"               \"yellow1\"              \"yellow2\"             \n[655] \"yellow3\"              \"yellow4\"              \"yellowgreen\"         \n\n\nOverlay a Density Curve\nStatisticians often like seeing a histogram (for the frequencies or probability of each value for the variable in the dataset) with an overlaid density curve (which is “smoothed” line for these probabilities). Statistical software like SAS and SPSS make this really easy. However, in R, we need to think through the process to get this to work.\n\nFirst, we need to make the histogram using probabilities for the “bars” in the histogram instead of frequency counts.\nSecond, we need to add a density line curve over the histogram “bars”.\n\nSee these online examples:\n\nhttps://r-charts.com/distribution/histogram-curves/\nhttps://www.datacamp.com/doc/r/histograms-and-density\nhttps://www.r-bloggers.com/2012/09/histogram-density-plot-combo-in-r/\n\n\n# make histogram as we did above\n# add freq = FALSE\nhist(mydata$Height,\n     freq = FALSE,\n     xlab = \"Height (in decimal inches)\",\n     col = \"lightblue\",\n     border = \"black\",\n     main = \"Histogram of Heights\",\n     sub = \"Hypothetical Madeup mydata Dataset\")\n\n# add density curve line\n# add na.rm=TRUE to remove \n# the missing values in Height\nlines(density(mydata$Height, na.rm=TRUE),\n      col = \"black\")\n\n\n\n\n\n\n\nBase R - Barchart\nBase R - Boxplot",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#ggplot2-package",
    "href": "module133_DataVis.html#ggplot2-package",
    "title": "1.3.3: Data Visualization",
    "section": "2. ggplot2 package",
    "text": "2. ggplot2 package\nThe ggplot2 package name starts with gg which stands for the “grammar of graphics” which is explained in the “ggplot2: Elegant Graphics for Data Analysis (3e)” Book.\n\n\n\n\n\n\nWhy is the package ggplot2 and not ggplot?\n\n\n\nMany people often ask Hadley Wickham (the developer of ggplot2) what happened to the first ggplot? Technically, there was a ggplot package and you can still view the ggplot archived package versions on CRAN which date back to 2006 with the last version posted in 2008. However, in 2007, Hadley redesigned the package and published the first version of ggplot2 (version 0.5.1) was posted on CRAN. So, ggplot2 is the package that has stayed in production and actively maintained for nearly 20 years!!\n\n\nGiven that ggplot2 has been actively maintained for nearly 20 years, it has become almost the defacto graphical standard for R graphics. If you take a look at the list of packages on CRAN that start with the letter “G”, as of this morning 01/28/2025 at 8:23 am EST, USA, there are 230 packages that start with gg - nearly all of these are compatible packages that extend the functionality or work in concert with the ggplot2 package.\n\nggplot2 - Scatterplot\n\nggplot2 - Histogram\n\nggplot2 - Barchart\n\nggplot2 - Boxplot",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#get-boilerplate-code-to-start",
    "href": "module133_DataVis.html#get-boilerplate-code-to-start",
    "title": "1.3.3: Data Visualization",
    "section": "3. Get boilerplate code to start",
    "text": "3. Get boilerplate code to start\nR Gallery\nR Graphics Cookbook",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#references",
    "href": "module133_DataVis.html#references",
    "title": "1.3.3: Data Visualization",
    "section": "References",
    "text": "References\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module133_DataVis.html#other-helpful-resources",
    "href": "module133_DataVis.html#other-helpful-resources",
    "title": "1.3.3: Data Visualization",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "1.3.3: Data Visualization"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html",
    "href": "module131_IntroRRStudio.html",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "",
    "text": "Get acquainted with R and R Studio\nWrite simple R code in Console\nCreate your first R script\nInstall and load R packages (understanding your R session)\nCreate your first R Markdown report and produce output files in different formats (HTML, PDF, or DOCX)",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#session-objectives",
    "href": "module131_IntroRRStudio.html#session-objectives",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "",
    "text": "Get acquainted with R and R Studio\nWrite simple R code in Console\nCreate your first R script\nInstall and load R packages (understanding your R session)\nCreate your first R Markdown report and produce output files in different formats (HTML, PDF, or DOCX)",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#prework---before-you-begin",
    "href": "module131_IntroRRStudio.html#prework---before-you-begin",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\n\n\n\n\n\n\nR versus RStudio\n\n\n\nNote: R is the name of the programming language itself and RStudio is an integrated development environment (IDE) which is an enhanced interface for better organization, files management and analysis workflows.\n\n\nSoftware and Applications to Download\n\nFIRST, Download and install R onto your computer from https://cran.r-project.org/.\nNEXT, After installing R, download and install RStudio Desktop onto your computer from https://posit.co/download/rstudio-desktop/.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#get-aquainted-with-r-and-r-studio",
    "href": "module131_IntroRRStudio.html#get-aquainted-with-r-and-r-studio",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "1. Get aquainted with R and R Studio",
    "text": "1. Get aquainted with R and R Studio\nBasic R\nWhen you download R from CRAN and install it on your computer, there is an R application that you can run. However, it is very bare bones. Here is a screenshot of what it looks like on my computer (Windows 11 operating system).\n\nYou can type commands in the console window at the prompt “&gt;” but this is slow and tedious. You can also write and execute scripts from inside this application and see the output back in the console window as well as creating plots. But managing large projects using this interface is not efficient.\n\n\nRStudio IDE\n\n\n\n\n\n\nRStudio Desktop Software vs Posit the company\n\n\n\nRStudio was founded in 2009 https://posit.co/about/ when it published the “free and open source” RStudio software. But over time, the RStudio application has expanded beyond just being used for the R programming language. You can now use RStudio for writing and managing projects with Python code, Markdown, LaTeX, Cascading Style Sheets and more.\nSo, in 2022, RStudio the company became Posit https://posit.co/blog/rstudio-is-becoming-posit/ to encompass the broader computing community.\n\n\nThe RStudio Integrated Development Environment (IDE) application provides much better tools for managing files within a given “project”. This biggest advantage of working in an IDE is everything is contained and managed within a given project, which is linked to a specific folder (container) on your computer (or cloud drive you may have access to).\nHowever, you will still need to write and execute code using scripts and related files. An IDE is NOT a GUI (graphical user interface) which is the “point and click” workflow you may have experience with if you’ve used other analysis software applications such as SPSS, SAS Studio, Excel and similar.\n\nThe interface is usually arranged with the following 4 “window panes”:\n\nConsole\nSource\nEnvironment\nFiles\n\n\nThe typical arrangement, usually has the “Console” window pane at the bottom left. This window also usually has TABs for the “Terminal” and any “Background Jobs” that might be running.\n\n\nThe “Source” window pane is usually at the top left. This is where you will do most of your editing of your R program scripts (*.R) or Rmarkdown files (*.Rmd). This is also where the data viewer window will open. You can also open and edit other kinds of files here as well (*.tex, *.css, *.txt, and more).\n\n\nThe top right window pane should always have your “Environment”, “History” and “Tutorial” TABs but may also have TABs for “Build” and “Git” and others depending on your project type and options selected.\n\n\nThe bottom right window pane has TABs for your:\n\n“Files” directory\n“Plots” window for graphical output\n“Packages” - which lists all add-on R packages installed on your computer\n“Help” window\nas well as other TABs for “Viewer” and “Presentation” for viewing other kinds of output.\n\n\n\nCustomizing your RStudio interface\nYou also have the option to rearrange your window panes as well as change the look and feel of your programming interface and much more. To explore all of your options, click on the menu at the top for “Tools/Global Options”:\n\n\nTake a look at the left side for the list of all of the options. Some of the most useful options to be aware of are:\n\nGeneral\nAppearance, and\nPane Layout\n\n\nIn the “General” TAB is where you can see and confirm that R is installed and where the R programming language executable is installed on your computer.\n\n\nYou will probably want to explore fine-tuning these parameters to customize the appearance of your RStudio preferences. For example, you can change the ZOOM level to improve readability. You may also want to change the FONT sizes for the Editor and Help windows as needed.\n\n\n\n\n\n\nZOOM + FONT\n\n\n\nWhen making changes to your RStudio interface appearance, be aware that ZOOM and FONT size settings work together, so you may need to play around with the settings that work best for your monitor or device you are using.\n\n\nI also encourage you to try out different “Editor Themes” which will change the colors of the R code as well as background colors (light or dark).\n\n\nThe default “Editor Theme” is textmate.\n\n\nBut here is an example of changing the theme to “Tomorrow Night Blue”.\n\n\nI would also suggest NOT changing the layout of the window panes until you are very familiar with the default settings. But in “Pane Layout” is where you can see what the default layout settings are and what other options are available to you.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#write-simple-r-code-in-console",
    "href": "module131_IntroRRStudio.html#write-simple-r-code-in-console",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "2. Write simple R code in Console",
    "text": "2. Write simple R code in Console\nSimple math\nSo, let’s start with some simple R code using the Console window and typing commands at the &gt; prompt (which is the greater than symbol).\nYou can write simple math expressions like 5 + 5.\n\n5 + 5\n\n[1] 10\n\n\n\nNotice that the output shows the number 1 enclosed in square brackets [] followed by the answer (or output) of 10.\nThis is because R performed the addition operation using the + operator and then “saved” the output in temporary memory as a scalar object with 1 element, which is the number 10.\nYou can actually see this temporary object by typing .Last.value - which is only temporary and will be overwritten after the execution of the next R command.\n.Last.value\n[1] 10\n\nHowever, if we look at our current computing environment (see “Global Environment” upper right window pane), it is still showing as empty.\n\nThis is because we have not yet “saved” the output into an object that we created. Let’s save the output from 5 + 5 into an object called ten.\nTo do this we need to do 2 things:\n\nCreate the object called ten by\nUsing the “assign” operator &lt;- to take the result of 5 + 5 and move it (save it or pipe it) into the object ten.\n\n\nten &lt;- 5 + 5\n\n\n\n\n\n\n\n\n\nTL;DR What is the Assign Operator &lt;-?\n\n\n\nThe “R” language is actually a derivative of the original “S” language which stood for the “language of statistics” - it was written by statisticians for statisticians (and now data scientists). The original S language was written in the mid-1970’s by programmers/statisticians at Bell Labs/AT&T.\nThe &lt;- actually comes from the physical key on their “APL” keyboards, for the APL programming language they were using at Bell Labs.\nA Nice Blog Post on the History of &lt;-\n\n\nTo “see” the output of this object called ten - you can either see it now in your Global Environment or type the object name in the Console to view it.\n\nten\n\n[1] 10\n\n\n\n\nIt is important to remember that R is an “object-oriented” programming language - everything in R is an object.\n\nBuilt in constants\nThere are several built in “constants” in R. Try typing these in at the Console to see the results.\n\npi\n\n[1] 3.141593\n\nletters\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n\n\n\n\n\n\n\n\nR is Case Sensitive!\n\n\n\nA pro and con of the R language is that it is case sensitive. Lower case x and uppercase X are different objects. As seen above, the lowercase letters object is a vector of the 26 lowercase letters, whereas LETTERS is a different object vector of the 26 uppercase letters. Be on the lookout for case sensitive spelling and formatting of object, package and function names in R.\n\n\nFor the constants like letters you get a list of the 26 lower case letters in the alphabet. Notice that the number in [square brackets] updates for each new line printed out. This allows you to keep track of the number of elements in the output object. letters is an “character” array (or vector) with 26 elements.\nTo confirm these details, we can use the class() function to determine that the letters object has all “character” elements. The length() function will let you know that there are 26 elements.\n\nclass(letters)\n\n[1] \"character\"\n\nlength(letters)\n\n[1] 26\n\n\n\n\nGetting help\nIf you would like to learn more about these built-in “constants”, you can get help in one of two ways. You can either type help(pi) in the “Console” (lower left) or type pi in the “Help” window (lower right).\n\nhelp(pi)\n\n\n\nThe help() function defaults to searching for a built-in object, function or dataset by default in the base R package. But some functions may exist in multiple packages, so it is always a good idea to add the package when running the help() function if possible.\nSince pi is in the base R package, it would be better to run:\n\nhelp(pi, package = \"base\")\n\nIf you have no idea what package a function may be in, you can use the ?? search operator. For example, many packages include a plotting related function. If you want to see how many R packages currently installed on your computer have a plot related function, type the following:\n\n??plot\n\n\nTry out a built-in R function\nThe majority of the R programming language is driven by functions. Technically the + operator is actually a function that performs a sum.\nYou can even get help on these operators, by typing help(\"+\"). We have to add the quotes \"\" so that R knows we are looking for this operator and not trying to perform an addition operation inside the function call.\n\nhelp(\"+\")\n\n\nBut let’s try a function to create a sequence of numbers - for example, let’s use the seq() function to create a sequence of numbers from 1 to 10.\n\nseq(10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nAnd let’s look at the help page for the seq() function.\n\nR allows for what is called “lazy” coding. This basically means you can provide very minimal input and R will try to figure out what you want using the default settings for a given function. In the case of seq() the function begins by default at 1 and creates the output in steps of 1 up to the value of 10.\nWhile “lazy” coding practices are common with R, it would actually be better to explicitly define each argument to make sure you get the exact output you want. To do this, inside the parentheses () we should assign a value to each argument.\nNotice in the “Help” page for seq() shown above that the first 3 arguments are: from, to and by. Each of these can be defined inside the () by using the syntax of the name of the argument, an equals sign =, and then the value (or object) you want to assign:\n\\[argument = value\\]\nFor example, the explicit function call should be:\n\nseq(from = 1,\n    to = 10,\n    by = 1)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nYou could easily change these values to get a sequence from 0 to 5 in increments of 0.1 as follows:\n\nseq(from = 0,\n    to = 5,\n    by = 0.1)\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8\n[20] 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7\n[39] 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5.0\n\n\n\nNotice the incremental counter [#] on the left to help you keep track of how many elements are in the resulting numeric vector that was the “result” or “output” from the seq() function.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#create-your-first-r-script",
    "href": "module131_IntroRRStudio.html#create-your-first-r-script",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "3. Create your first R script",
    "text": "3. Create your first R script\nSave your code in a new script\nSo, as you can tell, the R Console is useful but slow and tedious. Let’s create an R script to save all of these commands in a file so that we can easily access everything we’ve done so far and re-run these commands as needed.\n\n\n\n\n\n\nGood Reproducible Research Coding Practice\n\n\n\nIt is a good coding practice to create R code (saved in *.R script files or *.Rmd Rmarkdown files) for every step in your data preparation and analysis so that:\n\nyou have a record of everything you’ve done and why\nother people on your team (including yourself in a few weeks) will know what you did and why\nyou can share your code with others so they will understand what you did and why (and to publish your code and data with your research articles - YES you can get a DOI citation to add to your CV for data and code as well as for the article)!\n\n\n\nIn RStudio go to the top menu “File/New File/R Script”:\n\nOnce the R Script file is created, type in some of the commands we did above in the Console and put one command on each line.\nJust select each line and click “Run”.\n\nThen you can save the file on your computer as “myscript.R”, for example.\nYou can also select all of the rows and click run to execute all of the code in sequence and see the output in the “Console” Window.\n\nHere is the code and output:\n\n4 + 4\n\n[1] 8\n\nsqrt(25)\n\n[1] 5\n\npi\n\n[1] 3.141593\n\nseq(from=1, to=10, by=0.5)\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\n\n\n\nCreate R objects and Use Them\nLet’s try out some more built-in R functions, save the output in objects in your “Global Environment” and then use them in other functions and subsequent analysis steps.\nCreate a sequence of numbers and save them as an object called x. I also added a comment in the R code block below. Everything after the # hashtag is a comment which R will ignore. It is a good idea to add comments in your code to make sure that you and others understand what each part of your code does (including yourself in a few weeks when you’ve forgotten why you wrote that code step).\n\n# save sequence of numbers \n# from 1 to 10 in steps of 0.5\n# in an object named x\nx &lt;- seq(from=1, to=10, by=0.5)\n\n# Type x to view the contents\nx\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\n\n\nAlso take a look at the “Global Environment” to see the new object x.\n\n\n# use x to create new object y\ny &lt;- x*x\n\n\n\nOnce the object y is created, we can make a simple 2-dimensional scatterplot using the built-in plot() base R function.\n\n# plot x and y\nplot(x,y)\n\n\n\n\n\n\n\n\nThe plot is shown above, but if you are running this code interactively in the RStudio desktop, check the “Plots” window pane (lower right).\n\nOn your own\nDownload Rscript_01.R (right click the linked file and “Save As” the file on your computer), then open it in your RStudio and run through the code. Try out new variations on your own.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#install-and-load-r-packages-understanding-your-r-session",
    "href": "module131_IntroRRStudio.html#install-and-load-r-packages-understanding-your-r-session",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "4. Install and load R packages (understanding your R session)\n",
    "text": "4. Install and load R packages (understanding your R session)\n\nStatus of your current computing R session with sessionInfo()\n\nWhile the base installation of R is pretty powerful on it’s own, the beauty of R and the R programming community is that there are literally hundreds of thousands if not millions of people programming in R and creating new functions everyday.\nIn order to use these new functions, the developers put them together in packages that we can install to extend the functionality of R.\nBut first, let’s take a look at the packages that are part of the basic installation of R. One way to see which packages are currently installed and loaded into your current R computing session, is by running the command sessionInfo().\n\n\n\n\n\n\nWatch spelling - R is case sensitive!\n\n\n\nNotice: This function name is all lowercase except for the capital “I” in the middle. Be sure you are typing sessionInfo() and not sessioninfo().\n\n\nYou will also notice that the sessionInfo() command also lists the version of R I’m currently running (4.4.2), my operating system (Windows 11) and and my locale (USA, East Coast). These details can sometimes be helpful for collaborating with others who may be working with different system settings and for debugging errors.\n\nsessionInfo()\n\n\n7 Base R Packages\nThe basic installation of R includes 7 packages:\n\nstats\ngraphics\ngrDevices\nutils\ndatasets\nmethods\nbase\n\nTo learn more click on the “Packages” TAB in the lower right window pane to see the list of packages installed on your computer. I have a lot of packages on my computer, but here is a screenshot of the base R packages.\nSee the packages listed under “System Library” which are the ones that were installed with base R. You’ll notice that only some of these have checkmarks next to them. The checkmark means those are also loaded into your R session. Only some of them are loaded into memory by default to minimize the use of your computer’s memory.\n\n\nInstall a Package and Load it into R session memory\nLet’s install a “new” R package, like ggplot2.\nGo to the RStudio menu “Tools/Install” Packages\n\n\nThis will then open up a window where you can type in the name of the package you want. As soon as we start typing ggplot2 the menu begins listing all packages with that partial spelling…\n\n\nYou’ll notice that there are 3 parts to the installation:\n\nWhere you want to get the package from (i.e., which repository - more on repositories below).\nThe name of the package. You can actually type more than one package name at a time separated by commas if you want to install several packages at once.\nThe file location on your computer where the new package is installed - your file location may be different than mine. But this is useful to know in case something goes wrong. I would suggest keeping the default settings.\n\n\n\n\nWhere to get packages - CRAN Repository\nUsing the “Tools/Install” Packages menu from within RStudio automatically links to CRAN, which is the “The Comprehensive R Archive Network”. You’ve already been here once to download and install the R programming language application.\nHere is a screenshot of the CRAN homepage.\n\n\nNext click on “Packages” at the left to see the full list of packages currently available. As of right now (01/10/2025 at 5:12pm EST) there are 21,872 packages. This number increases every day as people create, validate and publish their packages on CRAN. You can get a list of all of the packages or if you have no idea what package you need, you can also look at the “Task Views” to see groupings of packages.\n\n\nHere is what the list of Packages looks like sorted by name:\n\n\nHowever, you can also browse Packages by “Task View”:\n\n\nFor example, suppose you are interested in survival analysis, here is a screenshot of the Survival Task View.\nAs you can see each Task View has a person(s) listed who help to maintain these collections. As you scroll through the webpage, you’ll see links to packages they recommend along with a description of what the packages do. For example, see the links below to the survival and rms packages.\n\n\nWhere to get packages - Bioconductor Repository\nWhile the list of R packages on CRAN is impressive, if you plan to do analyses of biological data, there is a good chance you will need a package from Bioconductor.org.\nAs of right now (01/10/2025 at 6:45pm EST) there are 2289 packages on Bioconductor. Similar to CRAN, Bioconductor requires each package to meet certain validation criteria and code testing requirements but these criteria are even more stringent on Bioconductor than on CRAN. You’ll notice that you can search for packages under the biocViews (left side column) or you can sort them alphabetically or search for individual packages in the section on the right side.\n\nThe one disadvantage of R packages from Bioconductor is that you cannot install them directly using the RStudio menu for “Tools/Install” Packages because you cannot “see” the Bioconductor repository from inside RStudio. Instead you have to install Bioconductor packages using R code.\n\nFor example, here is what you need to do to install the phyloseq package which “… provides a set of classes and tools to facilitate the import, storage, analysis, and graphical display of microbiome census data”.\nTo install phyloseq you need to (see the black box of code in the screenshot shown below):\n\nInstall BiocManager from CRAN - this package you can install from the RStudio menu for “Tools/Install Packages” - or you can run the code shown below for install.packages().\n\n\ninstall.packages(\"BiocManager\")\n\n\nThen go to the Console or open an R script and run:\n\n\nBiocManager::install(\"phyloseq\")\n\n\n\nWhere to get packages - Github, friends, teammates, …\nIn addition to the CRAN and Bioconductor repositories, you can also get packages from Github (and other cloud-based repositories), friends, teammates or write your own.\nTo get an idea of how many packages may be currently on Github, we can “search” for “R package” at https://github.com/search?q=R+package&type=repositories and as you can see this is well over 118,000+ packages.\n\n\nWhile you can find packages on Github that have not (yet) been published on CRAN or Bioconductor, the developers of packages currently on CRAN and Bioconductor also often publish their development version (think of these as in “beta” and still undergoing testing) on Github.\nFor example, the current published version of the data wrangling R package dplyr on CRAN was last updated on 11/17/2023 (over a year ago).\n\n\nHowever, the development version of dplyr on Github was last updated 5 months ago in August 2024. So, there is probably a new version of dplyr coming soon for CRAN.\n\nWhile the developers haven’t published this “Github” version of dplyr yet on CRAN, if you want to test out new dplyr functions and updates under development, you can go to the R Console or write an R script to install the development version using these commands (see below) which is explained on the dplyr on Github website.\n\n# install.packages(\"pak\")\npak::pak(\"tidyverse/dplyr\")\n\n\nFinding and vetting R packages\nSo, as you have seen there are numerous ways to find R packages and there are hundreds of thousands of them out there. Your company or team may also have their own custom R package tailored for your specific research areas and data analysis workflows.\nFinding R packages is similar to finding new questionnaires, surveys or instruments for your research. For example, if you want to measure someone’s depression levels, you should use a validated instrument like the Center for Epidemiological Studies-Depression (CESD) or the Beck Depression Index (BDI). These measurement instruments have both been well published and are well established for depression research.\nFinding R packages is similar - do your research! Make sure that the R package has been published and is well established to do the analysis you want. In terms of reliability, getting packages from CRAN or Bioconductor are the best followed by Github or other individuals. The best suggestion is look to see which R packages are being used by other people in your field.\n\n\n\n\n\n\nNo oversight company or agency\n\n\n\nWhile it may seem worrisome that there is no governing company or organization that verifies and validates and certifies all R packages, the good news is that the R community is a vast Global community. The development of R is not controlled by a limited number of people hired within a single company - instead there are literally millions of R programmers across the Globe testing and providing feedback on a 24/7 basis. If there is a problem with a package or function, there will be people posting about these issues - see Additional Resources.\nThis is the power of Open Source computing!!\n\n\n\nPopularity of R Packages\nTo get an idea of how long a package has been in use and if it is still being actively supported and how it relates to other similar packages, check out this interactive Shiny app website for package downloads from CRAN https://hadley.shinyapps.io/cran-downloads/. Type in the packages you want (separated by commas) to compare and put in the date range of interest.\nHere is an example comparing the arsenal, gtsummary, and tableone packages all of which are useful for making tables of summary statistics (aka, “Table 1”) - showing the number of downloads since the beginning of Jan 1, 2024.\nAs you can see the most downloaded of these 3 packages is gtsummary followed by tableone and then arsenal having the fewest downloads. This does NOT necessarily imply quality, but it does give you some insight into the popularity of these packages. I actually prefer the arsenal table package but tableone has been around longer and gtsummary is written by members of the RStudio/Posit development community and is more well known and popular. All 3 of these packages can be found in use in current research literature.\nYou will see examples of all 3 of these table-making packages in Module lesson 1.3.2\n\n\nHere is an example of two specific packages I like. The rggobi package which was great for visualizing multiple dimensions of data simultaneously but which is no longer supported. But there is now a newer tourr package which was written by the same developers to replace the rggobi package. You can see that in the middle of 2020, the number of downloads for rggobi dropped almost to 0 and the tourr package downloads started to rise - this is about when rggobi was archived on CRAN and they switched over to maintaining the newer tourr package.\n\n\nrggobi on CRAN moved to archived status in July 2020, but\n\ntourr on CRAN was last updated in April 2024.\n\n\nIn summary:\n\ndo your homework,\ncheck to see when the package was last updated,\nresearch who maintains it and\nreview how good their documentation is for the package and what it does, and\nsee if the package has been used by others in your research area.\n\n\nLoad the new R package into your R session\nAfter you’ve decided what package you want and have installed it onto your computer, you must load it into memory for EVERY new R session for which you want those functions available.\n\n\n\n\n\n\nPackages - install once, (re)-load every R session\n\n\n\nUnless you upgrade R or change computers, you only need to install a given R package once. But you do need to (re)-load the package into your current R session every time you (re)-start R (or RStudio).\n\n\nFor example, suppose I want to make a plot using the ggplot2 package. Before I can use the ggplot() function, I have to load that package into my computing session.\nHere is my current R session status BEFORE I load the ggplot2 package.\n\n# show current sessionInfo\nsessionInfo()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 22000)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.1.1     cli_3.6.3        \n [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.15.0 yaml_2.3.8       \n [9] rmarkdown_2.26    knitr_1.49        jsonlite_1.8.8    xfun_0.49        \n[13] digest_0.6.35     rlang_1.1.4       evaluate_0.23    \n\n\nSince I have not yet loaded the ggplot2 package into the session, I will get an error.\n\n# I have not yet loaded ggplot2 into the session\n# try the ggplot() function with the\n# built-in pressure dataset to see error\nggplot(pressure, aes(temperature, pressure)) +\n  geom_point()\n\nError in ggplot(pressure, aes(temperature, pressure)): could not find function \"ggplot\"\n\n\n\nThe code above generates an error since these functions are not yet available in our session.\n\nTo fix this error, we need to use the library() function to load the ggplot2 functions into current working memory.\n\n# load ggplot2 package\nlibrary(ggplot2)\n\n# look at sessionInfo again\nsessionInfo()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 22000)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.5.1\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.5       cli_3.6.3         knitr_1.49        rlang_1.1.4      \n [5] xfun_0.49         generics_0.1.3    jsonlite_1.8.8    glue_1.8.0       \n [9] colorspace_2.1-0  htmltools_0.5.8.1 scales_1.3.0      fansi_1.0.6      \n[13] rmarkdown_2.26    grid_4.4.2        evaluate_0.23     munsell_0.5.0    \n[17] tibble_3.2.1      fastmap_1.1.1     yaml_2.3.8        lifecycle_1.0.4  \n[21] compiler_4.4.2    dplyr_1.1.4       htmlwidgets_1.6.4 pkgconfig_2.0.3  \n[25] rstudioapi_0.15.0 digest_0.6.35     R6_2.5.1          tidyselect_1.2.1 \n[29] utf8_1.2.4        pillar_1.9.0      magrittr_2.0.3    withr_3.0.2      \n[33] tools_4.4.2       gtable_0.3.6     \n\n\n\nNotice that under other attached packages we can now see ggplot2_3.5.1 indicating that yes ggplot2 is installed and in memory and that version 3.5.1 is the version I am currently using.\n\nLet’s try the plot again with the ggplot() function from the ggplot2 package.\n\n# try the plot again\nggplot(pressure, aes(temperature, pressure)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReload packages for every new R session\n\n\n\nEverything you close out your R/RStudio computing session (or restart your R session) you will need to load all of your package again. I know this seems like a HUGE pain, but there is a rationale for this.\n\nYou may not need the same packages for every new computing session - so R begins with the minimum loaded to save computing memory.\nThe GOOD NEWS is you do not have to re-install the packages - these are already saved on your computer. You only have to re-load them into memory using the library() function.\nThis workflow forces you to document (in your code) which packages you need for your computing sessions and why you are using them.\n\nBUT … If you do have a core set of packages that you would like to make sure get loaded into memory every time you start R/RStudio, see these helpful posts on customizing your startup:\n\nhttps://www.datacamp.com/doc/r/customizing\nhttps://www.r-bloggers.com/2014/09/fun-with-rprofile-and-customizing-r-startup/",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#create-your-first-r-markdown-report-and-produce-output-files-in-different-formats-html-pdf-or-docx",
    "href": "module131_IntroRRStudio.html#create-your-first-r-markdown-report-and-produce-output-files-in-different-formats-html-pdf-or-docx",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "5. Create your first R Markdown report and produce output files in different formats (HTML, PDF, or DOCX)",
    "text": "5. Create your first R Markdown report and produce output files in different formats (HTML, PDF, or DOCX)\nCreate a new Rmarkdown File\nWe will do more in the later lesson 1.3.6: Putting reproducible research principles into practice, but let’s take a look at an Rmarkdown file and how we can use it to create a report that combines together data + code + documentation to produce a seamless report.\nGo to the RStudio menu and click “File/New File/R Markdown”:\n\n\nType in a title, your name, the date and choose the format you’d like to create. For your first document I encourage you to try HTML. But you can create WORD (DOC) documents and even PDFs. In addition to documents, you can also create slide deck presentations, Shiny apps and other custom products like R packages, websites, books, dashboards and many more.\n\n\n\n\n\n\nRmarkdown ideas and inspiration\n\n\n\n\nRmarkdown Gallery\nRmarkdown Formats\nRmarkdown Cookbook\n\n\n\nTo get started, use the built-in template:\n\nType in a title\nType in your name as author\nChoose and output document format\n\nHTML is always a good place to start - only need a browser to read the output *.html file.\n\nDOC usually works OK - but you need MS Word or Open Office installed on your computer.\n\nPDF NOTE: You need a TEX compiler on your computer - Learn about installing the tinytex https://yihui.org/tinytex/ R package to create PDFs.\n\n\n\n\n\nRmarkdown sections\nHere is the Example RMarkdown Template provided by RStudio to help you get started with your first Rmarkdown document.\n\n\nThis document consists of the following 3 key sections:\n\nYAML (yet another markup language) - this is essentially the metadata for your document and defines elements like the title, author, date and type of output document to be created (HTML in this example).\n\n\n\n\nR code blocks - the goal is to “interweave” code and documentation so these 2 elements live together. That way the analysis output and any associated tables or figures are updated automatically without having to cut-and-paste from other applications into your document - which is time consuming and prone to human errors.\n\nNotice that the code block starts and ends with 3 backticks ``` and includes the {r} Rlanguage designation inside the curly braces.\n\n\n\n\n\n\nRmarkdown\n\n\n\nRmarkdown can be used for many different programming languages including python, sas, and more, see rmarkdown - language-engines.\n\n\n\n\n\nAlong with the R code blocks, we can also create our document with “marked up (or marked down)” text. Rmarkdown is a version of “markdown” which is a simplified set of tags that tell the computer how you want a piece of text formatted.\n\nFor example putting 2 asterisks ** before and after a word will make it bold, putting one _ underscore before and after a word will make the word italics; one or more hashtags # indicate a header at certain levels, e.g. 2 hashtags ## indicate a header level 2.\n\n\n\n\n\n\nRmarkdown Tutorial\n\n\n\nI encourage you to go through the step by step tutorial at https://rmarkdown.rstudio.com/lesson-1.html.\n\n\n\n\nHere are all 3 sections outlined.\n\n\nAt the top of the page you’ll notice a little blue button that says “knit” - this will “knit” (or combine) the output from the R code chunks and format the text as “marked up” and produce this HTML file (which will open in a browser window):",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#references",
    "href": "module131_IntroRRStudio.html#references",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "References",
    "text": "References\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "module131_IntroRRStudio.html#other-helpful-resources",
    "href": "module131_IntroRRStudio.html#other-helpful-resources",
    "title": "1.3.1: Introduction to R and R Studio",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "1.3.1: Introduction to R and R Studio"
    ]
  },
  {
    "objectID": "additionalResources.html",
    "href": "additionalResources.html",
    "title": "Additional Help and Resources",
    "section": "",
    "text": "Download: R from CRAN\n\nThis is where you can download the R language software for FREE for your own computer.\nChoose your operating system (Mac OS or Windows or Linux/Unix)\nNOTE: For Windows, you should also download and install Rtools - this is technically optional, but is useful to have. Make sure to download the one for your R version.\n\nR Cookbook\n\n\n\n\n\nDownload: RStudio IDE Desktop\n\nNote: Windows is listed at the top - just scroll down to see the installer for the Mac OS as well. There are also installers for the versions of Linux/Unix.\n\nRStudio Education\nRStudio Cloud Tutorials\n** Quick-R **",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#r-and-rstudio-resources",
    "href": "additionalResources.html#r-and-rstudio-resources",
    "title": "Additional Help and Resources",
    "section": "",
    "text": "Download: R from CRAN\n\nThis is where you can download the R language software for FREE for your own computer.\nChoose your operating system (Mac OS or Windows or Linux/Unix)\nNOTE: For Windows, you should also download and install Rtools - this is technically optional, but is useful to have. Make sure to download the one for your R version.\n\nR Cookbook\n\n\n\n\n\nDownload: RStudio IDE Desktop\n\nNote: Windows is listed at the top - just scroll down to see the installer for the Mac OS as well. There are also installers for the versions of Linux/Unix.\n\nRStudio Education\nRStudio Cloud Tutorials\n** Quick-R **",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#rmarkdown-resources",
    "href": "additionalResources.html#rmarkdown-resources",
    "title": "Additional Help and Resources",
    "section": "RMarkdown Resources",
    "text": "RMarkdown Resources\n\nRmarkdown Tutorial\nBook: R Markdown: The Definitive Guide\nBook: R Markdown Cookbook",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#r-graphics",
    "href": "additionalResources.html#r-graphics",
    "title": "Additional Help and Resources",
    "section": "R Graphics",
    "text": "R Graphics\n\nBook: ggplot2\nR Graphics Cookbook - online book\nCookbook for R (graphics) - earlier version\nR Graph Gallery\nBook: R Graphics, 3rd edition by Paul Murrell and accompanying R Graphics - book website",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#online-training-and-courses",
    "href": "additionalResources.html#online-training-and-courses",
    "title": "Additional Help and Resources",
    "section": "Online Training and Courses",
    "text": "Online Training and Courses\n\nCode Academy\nSoftware Carpentry\nswiRl - Learn R in R\nDatacamp\n\nR for SAS Users - My Datacamp Course\n\nCoursera\n\nReproducible Templates for Analysis and Dissemination - My Coursera Course\n\nMy Courses at Emory:\n\nEmory N741\nEmory N736",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#more-helpful-online-books-on-r-and-statistics-with-r",
    "href": "additionalResources.html#more-helpful-online-books-on-r-and-statistics-with-r",
    "title": "Additional Help and Resources",
    "section": "More Helpful Online Books on R and Statistics with R",
    "text": "More Helpful Online Books on R and Statistics with R\n\nBook: Statistical Inference via Data Science\nBook: The Epidemiologist R Handbook\nBook/Course: Stat 545\nBook: Statistical Inference via Data Science: A ModernDive into R and the Tidyverse\nOpenIntro Statistics\nMastering Software Development in R",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#other-places-to-get-help",
    "href": "additionalResources.html#other-places-to-get-help",
    "title": "Additional Help and Resources",
    "section": "Other places to get HELP",
    "text": "Other places to get HELP\n\nStackOverflow\n\nI encourage you to create an account so you can post questions. But even without an account you can search for and find answers to your questions and error messages.\n\nGoogle\n\nYou can often cut and paste error messages in Google to find answers - most likely will redirect you to Stack Overflow.\n\nPackage vignettes for packages on CRAN\n\nHere is one vignette for dplyr\nThese will often help you get started.\n\nGithub package issues\n\nMany packages will host their code on Github which includes an “issues” tab. This can be a good place to see what other problems people may be having with a given package.\ndplyr issues on Github\n\nCRAN package site\n\ndplyr on CRAN - spend time looking at:\n\nthe README for the package or\nbug reports or\nNEWS which will detail the changes for each version updates\n\n\nR Bloggers\n\nThis is a really good website which curates thousands of people who are R developers, users and programmers who post articles about R.\n\nSTHDA Website for “Statistical tools for high-throughput data analysis”\n\nThis website will often come up when “Googling” for answers. It has a lot of ads but often has very helpful examples.",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "additionalResources.html#r-packages-used-in-tidal-modules",
    "href": "additionalResources.html#r-packages-used-in-tidal-modules",
    "title": "Additional Help and Resources",
    "section": "R Packages Used in TIDAL Modules",
    "text": "R Packages Used in TIDAL Modules\n\ntidyverse",
    "crumbs": [
      "Additional Help and Resources"
    ]
  },
  {
    "objectID": "PRAMS.html",
    "href": "PRAMS.html",
    "title": "PRAMS Data Analysis",
    "section": "",
    "text": "PRAMS is the Pregnancy Risk Assessment Monitoring System (PRAMS). According to the CDC’s website for About PRAMS:\n\n\n\n\n\n\nWhat is PRAMS?\n\n\n\nPRAMS is the Pregnancy Risk Assessment Monitoring System. It is a joint surveillance project between state, territorial, or local health departments and CDC’s Division of Reproductive Health. PRAMS was developed in 1987 to reduce infant morbidity and mortality by influencing maternal behaviors before, during, and immediately after live birth.\n\n\n\n\n\n\n\n\nWhat is the purpose of PRAMS?\n\n\n\nThe purpose of PRAMS is to find out why some infants are born healthy and others are not. The survey asks new mothers questions about their pregnancy and their new infant. The questions give us important information about the mother and the infant and help us learn more about the impacts of health and behaviors.\n\n\n\n\nYou can request the PRAMS Data from the CDC.\nOnce granted access, follow the instructions from the CDC to download the data and sign the data sharing agreement.\nFor the purposes of the TIDAL R training session, we will be working with PRAMS Phase 8 ARF (Automated Research File) dataset.\n\n\nSee the details on the PRAMS Questionnaires.\nLearn more about the PRAMS Data Methodology including details on how the samples are weighted.\n\nDownload and Read this helpful paper on PRAMS design and methodology (Shulman, D’Angelo, Harrison, Smith, and Warner, 2018).\nThere are also helpful tutorial videos on working with PRAMS data by ASSOCIATION OF STATE AND TERRITORIAL HEALTH OFFICIALS (ASTHO.org).",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#prams-data",
    "href": "PRAMS.html#prams-data",
    "title": "PRAMS Data Analysis",
    "section": "",
    "text": "PRAMS is the Pregnancy Risk Assessment Monitoring System (PRAMS). According to the CDC’s website for About PRAMS:\n\n\n\n\n\n\nWhat is PRAMS?\n\n\n\nPRAMS is the Pregnancy Risk Assessment Monitoring System. It is a joint surveillance project between state, territorial, or local health departments and CDC’s Division of Reproductive Health. PRAMS was developed in 1987 to reduce infant morbidity and mortality by influencing maternal behaviors before, during, and immediately after live birth.\n\n\n\n\n\n\n\n\nWhat is the purpose of PRAMS?\n\n\n\nThe purpose of PRAMS is to find out why some infants are born healthy and others are not. The survey asks new mothers questions about their pregnancy and their new infant. The questions give us important information about the mother and the infant and help us learn more about the impacts of health and behaviors.\n\n\n\n\nYou can request the PRAMS Data from the CDC.\nOnce granted access, follow the instructions from the CDC to download the data and sign the data sharing agreement.\nFor the purposes of the TIDAL R training session, we will be working with PRAMS Phase 8 ARF (Automated Research File) dataset.\n\n\nSee the details on the PRAMS Questionnaires.\nLearn more about the PRAMS Data Methodology including details on how the samples are weighted.\n\nDownload and Read this helpful paper on PRAMS design and methodology (Shulman, D’Angelo, Harrison, Smith, and Warner, 2018).\nThere are also helpful tutorial videos on working with PRAMS data by ASSOCIATION OF STATE AND TERRITORIAL HEALTH OFFICIALS (ASTHO.org).",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#prework---before-you-begin",
    "href": "PRAMS.html#prework---before-you-begin",
    "title": "PRAMS Data Analysis",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\nInstall R Packages\nBefore you begin, please go ahead and install (or make sure these are already installed) on your computer for these following packages - these are all on CRAN, so you can install them using the RStudio Menu Tools/Install Packages interface:\n\nhaven\ndplyr\nsurvey\n\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(survey)\n\nCreate a NEW RStudio Project\nBEFORE you being any new analysis project, it is ALWAYS a good idea to begin with the NEW RStudio project.\nGo to the RStudio menu “File/New Project” and create your new project (ideally in a NEW directory, but it is also ok to use an exisiting directory/folder on your computer).\nThis new directory (or folder) will be where all of your files will “live” for your current analysis project.\nSee the step-by-step instructions for creating a new RStudio project in Module 1.3.2.",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#get-prams-data-and-select-subset-for-analysis",
    "href": "PRAMS.html#get-prams-data-and-select-subset-for-analysis",
    "title": "PRAMS Data Analysis",
    "section": "1. Get PRAMS Data and Select Subset for Analysis",
    "text": "1. Get PRAMS Data and Select Subset for Analysis\nA. Read-in the PRAMS Phase 8 2016-2021 combined dataset\nThe PRAMS data provided by the CDC will be in SAS format (*.sas7bdat). We can read the native SAS file into R using the haven package and the read_sas() function.\n\n\n\n\n\n\nMemory Warning\n\n\n\nThe size of the phase8_arf_2016_2021.sas7bdat dataset is a little over 1GB. So, make sure your computer has enough available memory to fully load this dataset. I will provide some more details below on how we can reduce the size of the dataset and improve the memory issues below.\n\n\nYou can check your available memory, by checking your “Global Environment” TAB (upper right window pane) click on the down arrow next to the icon with “XX MiB” just to the left of the little broom:\n\n\n\nClick on the “Memory Usage Report” to see a detailed breakdown. This window will show:\n\nMemory used by R objects (in your “Global Environment”)\nMemory used on your computer by your current R Session\nMemory currently in use for everything currently running on your computer (all apps running - active and in background) - you can compare this to your “task manager” memory viewer.\nFree System Memory - when this gets low the “XX MiB” graphic will change color from green - to yellow - to orange - to red. Once you get to red, your R session will most likely crash since there is not enough memory to perfom operations or run analyses.\n\nThis is a screen shot of my computer (yours will look different) BEFORE I load the PRAMS dataset.\n\n\n\nRun the following R code to load the PRAMS Phase 8 dataset into your R Session and check the “Global Environment”.\n\nlibrary(haven)\nprams &lt;- \n  read_sas(\"phase8_arf_2016_2021.sas7bdat\")\n\nHere is my memory AFTER loading the PRAMS dataset into my “Global Environment”.\n\n\n\n\n\nB. Save the data as a *.RData binary file for use in later analyses\nOne way to reduce the size of the PRAMS dataset is to save it as a native *.RData binary file format. So, let’s save the PRAMS dataset in this format on your computer.\n\n# save the whole dataset as *.RData format\nsave(prams, \n     file = \"prams.RData\")\n\nOn my computer, here is a comparison of the size of these 2 files:\n\n\nphase8_arf_2016_2021.sas7bdat is 1,095,499,776 bytes (which is 1.02 GB)\n\nprams.RData is only 34,713,319 (which is only 0.0323 GB)\n\nThis is a file size reduction of 96.83%!!\n\n\n\nNow that we’ve reduced the file size of the dataset on your computer’s hard drive (or cloud storage), let’s also clear up the “Global Environment” back in your current RStudio computing session.\nC. Clean up files to save memory\nNow that we’ve saved the data, let’s remove the PRAMS data object from the RStudio session.\n\nFor now we can simply remove everything using the rm(list=ls()).\nHowever, if you have other objects you want to keep, you can specifically only remove the PRAMS dataset using rm(prams).\n\n\n# remove all objects from Global Environment\nrm(list=ls())\n\n# confirm Global Environment is empty\n# list all objects\nls()\n\ncharacter(0)\n\n# and free any currently unused memory\ngc()\n\n          used  (Mb) gc trigger   (Mb)  max used  (Mb)\nNcells 2107864 112.6    4135961  220.9   4135961 220.9\nVcells 3850988  29.4  153275530 1169.4 112103527 855.3\n\n\nAfter we remove everything, let’s look at the session memory again.\n\n\n\nNow let’s read the PRAMS data back in, but this time read in the prams.RData binary R data formatted file. We will use the built-in load() function.\n\n# load back only the prams dataset\nload(file = \"prams.RData\")\n\nLet’s check the R session memory again:\n\n\n\nI know this didn’t make a large difference for the R session available memory, but by doing this process:\n\nThe PRAMS dataset now takes up less memory on your computer’s file storage, and\nThe load() function for the prams.RData file should run faster when beginning your R computing session instead of having to use the haven package to read in the SAS formatted file everytime.\n\nAs a quick comparison on my computer (Windows 11), the time to read in the SAS formatted file was about 14 sec:\n&gt; system.time(\n+   prams &lt;- \n+     read_sas(\"phase8_arf_2016_2021.sas7bdat\")\n+ )\n   user  system elapsed \n  13.44    0.47   13.96\nAnd the time to read in the prams.RData file was only about 1.5 sec.\n&gt; system.time(\n+   load(\"prams.RData\")\n+ )\n   user  system elapsed \n   1.45    0.08    1.54",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#getting-started-with-prams-data",
    "href": "PRAMS.html#getting-started-with-prams-data",
    "title": "PRAMS Data Analysis",
    "section": "2. Getting started with PRAMS Data",
    "text": "2. Getting started with PRAMS Data\nBreastfeeding summary - UNWEIGHTED data\nLet’s look at whether the mother ever breastfed her baby - this is variable BF5EVER, where 1 = “NO” and 2 = “YES”.\nPRAMS Phase 8 Codebook\n\n# create a factor variable\n# and add labels\nprams$BF5EVER.f &lt;- factor(\n  prams$BF5EVER,\n  levels = c(1, 2),\n  labels = c(\"NO\", \"YES\")\n)\n\nFor the UNWEIGHTED data, let’s get a simple table of breastfeeding by STATE (variable STATE) and YEAR (variable NEST_YR).\nAs we can see below, in 2017 for the state of GA, 919 women responded to this question:\n\n919 women responded\n\n170 said NO\n749 said YES\n\n\n36 were missing a response (indicated by &lt;NA&gt;)\n\n\nprams %&gt;%\n  filter(NEST_YR == 2017) %&gt;% \n  with(., table(STATE, BF5EVER.f, \n                useNA = \"ifany\"))\n\n     BF5EVER.f\nSTATE   NO  YES &lt;NA&gt;\n   AK   71  927   47\n   AL  181  659   42\n   CO   73 1037   18\n   DE  126  728   37\n   GA  170  749   36\n   IA  136  867   30\n   IL  140 1048   36\n   KS   81  856   58\n   KY  139  536   27\n   LA  285  586   23\n   MA  115 1268   40\n   MD   97  928   35\n   ME   88  754   30\n   MI  290 1532   75\n   MO  166  908   37\n   MT   66  851   20\n   ND  102  472   17\n   NH   42  523   15\n   NJ  125 1102   31\n   NM  123 1038   19\n   NY  109  706   33\n   PA  164 1023   42\n   PR   81  928   23\n   RI  105  960   37\n   SD  150  946   35\n   UT   93 1305   49\n   VA   88  969   26\n   VT   54  780   14\n   WA   69 1138   31\n   WI  221 1051   74\n   WV  186  475   38\n   WY   49  438   16\n   YC   99 1125   69\n\n\nThis aligns with the CDC PRAMS Indicators Report for GA in 2020 - scroll to the bottom to see the RAW count of 919 women who responded to “Ever Breastfed” in GA in 2017.\nBreastfeeding summary - WEIGHTED data\nIn the CDC PRAMS Indicators Report for GA in 2020 the columns that have the 95% CI (confidence intervals) for the percentages are the population weighted percentage estimates for the Stats of GA during that year.\nTo get the estimated percentage of women in the stats of GA who had “ever breastfed” in 2017, we need to use the survey package and apply the proper sample weighting to get these estimates.\nFrom this we can see that the population estimates for 2017 are:\n\nBreastfed ever = NO: 17639.96 +/- 2045.415\nBreastfed ever = YES: 101686.10 +/- 2271.075\n\nThis leads to a percentage of YES estimate of 101686.10 * 100 / (101686.10 + 17639.96) = 85.2170096% which should match pretty closely to what is in the CDC PRAMS Indicators Report for GA in 2020.\nWe can also get the percentage of overall breastfeeding YES for the USA for the 40 “states” (technically 38 states, Puerto Rico, and New York City) that were included in the PRAMS dataset in 2020 (see the last column in the CDC report), using the following R code. Note: 2 “states” did not have data in 2020: Connecticut and Florida.\nFrom this we can see that the population estimates for the “whole USA” for 2020 were:\n\nBreastfed ever = NO: 225560.3 +/- 4884.871\nBreastfed ever = YES: 1609464 +/- 5540.240\n\nThis leads to a percentage of YES estimate of 1609464 * 100 / (1609464 + 225560.3) = 87.7080483% which is pretty close to what is in the CDC PRAMS Indicators Report for GA in 2020 - with some numerical precision variation due to software algorithms.\nCongratulations on getting started with the PRAMS Dataset\n3. Data Wrangling with PRAMS\nExamples will be posted here working with the PRAMS Dataset for recoding, creating or modifying variables.\n4. Visualizing PRAMS Data\nExamples will be posted here for making graphs and figures with suggestions on handling very large datasets.\n5. Missing Data in PRAMS\nExamples will be posted here for summarizing and visualizing missing data in PRAMS.\n6. PRAMS Statistical Tests and Models\nExamples will be posted here for statistical tests and models (such as linear and logistic regression) for both the unweighted and weighted data approaches.\n7. PRAMS Reproducible Research Report\nA Rmarkdown analysis report will be provided here as a template to “kick start” your research with the PRAMS dataset.",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#references",
    "href": "PRAMS.html#references",
    "title": "PRAMS Data Analysis",
    "section": "References",
    "text": "References\n\n\nBates, Douglas, Martin Maechler, and Mikael Jagan. 2024. Matrix: Sparse and Dense Matrix Classes and Methods. https://Matrix.R-forge.R-project.org.\n\n\nBoettiger, Carl. 2021. Knitcitations: Citations for Knitr Markdown Files. https://github.com/cboettig/knitcitations.\n\n\nLumley, Thomas. 2004. “Analysis of Complex Survey Samples.” Journal of Statistical Software 9 (1): 1–19.\n\n\n———. 2010. Complex Surveys: A Guide to Analysis Using r: A Guide to Analysis Using r. John Wiley; Sons.\n\n\nLumley, Thomas, Peter Gao, and Ben Schneider. 2024. Survey: Analysis of Complex Survey Samples. http://r-survey.r-forge.r-project.org/survey/.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nShulman, Holly B., Denise V. D’Angelo, Leslie Harrison, Ruben A. Smith, and Lee Warner. 2018. “The Pregnancy Risk Assessment Monitoring System (PRAMS): Overview of Design and Methodology.” American Journal of Public Health 108 (10): 1305–13. https://doi.org/10.2105/ajph.2018.304563.\n\n\nTerry M. Therneau, and Patricia M. Grambsch. 2000. Modeling Survival Data: Extending the Cox Model. New York: Springer.\n\n\nTherneau, Terry M. 2024. Survival: Survival Analysis. https://github.com/therneau/survival.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. Haven: Import and Export SPSS, Stata and SAS Files. https://haven.tidyverse.org.",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "PRAMS.html#other-helpful-resources",
    "href": "PRAMS.html#other-helpful-resources",
    "title": "PRAMS Data Analysis",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "PRAMS Data Analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theme 1. Measurement and Prediction of IPV",
    "section": "",
    "text": "Presented by Melinda Higgins, PhD\nModule 1.3 will introduce the learner to writing code in the R language and utilizing the RStudio IDE (integrated development environment). This module will include 6 sessions - the first 3 will be asynchronous-online (AO) and the last 3 will be in-person (IP).\nThe 6 sessions are:\n\n1.3.1: Introduction to R and R Studio (AO)\n1.3.2: Data Wrangling (AO)\n1.3.3: Data Visualization (AO)\n1.3.4: Missing data and sampling weight (IP)\n1.3.5: Statistical tests and models (IP)\n1.3.6: Putting reproducible research principles into practice (IP)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#module-1.3-data-analytics-using-r",
    "href": "index.html#module-1.3-data-analytics-using-r",
    "title": "Theme 1. Measurement and Prediction of IPV",
    "section": "",
    "text": "Presented by Melinda Higgins, PhD\nModule 1.3 will introduce the learner to writing code in the R language and utilizing the RStudio IDE (integrated development environment). This module will include 6 sessions - the first 3 will be asynchronous-online (AO) and the last 3 will be in-person (IP).\nThe 6 sessions are:\n\n1.3.1: Introduction to R and R Studio (AO)\n1.3.2: Data Wrangling (AO)\n1.3.3: Data Visualization (AO)\n1.3.4: Missing data and sampling weight (IP)\n1.3.5: Statistical tests and models (IP)\n1.3.6: Putting reproducible research principles into practice (IP)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#project-tidal",
    "href": "index.html#project-tidal",
    "title": "Theme 1. Measurement and Prediction of IPV",
    "section": "Project TIDAL",
    "text": "Project TIDAL\nProject TIDAL is an NIH-funded R25 study (1 R25 NR021324-01 ), co-led by Drs. Sangmi Kim and Ran Xiao from Nell Hodgson Woodruff School of Nursing at Emory University.\nThe objective of the study is to develop a short course titled “Trauma-Informed Data Science and Digital Health Technologies to Prevent Intimate Partner Violence (IPV) among Pregnant/Postpartum Women” targeting early-career researchers (predoctoral, postdoctoral, and junior faculty) from diverse backgrounds across the U.S.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html",
    "href": "module132_DataWrangling.html",
    "title": "1.3.2: Data Wrangling",
    "section": "",
    "text": "To read in data.\nTo view the Data.\nTo subset the data - select and filter.\nTo create and modify variables.\nTo get data summary and descriptive statistics\nExporting/Saving Data",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#session-objectives",
    "href": "module132_DataWrangling.html#session-objectives",
    "title": "1.3.2: Data Wrangling",
    "section": "",
    "text": "To read in data.\nTo view the Data.\nTo subset the data - select and filter.\nTo create and modify variables.\nTo get data summary and descriptive statistics\nExporting/Saving Data",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#prework---before-you-begin",
    "href": "module132_DataWrangling.html#prework---before-you-begin",
    "title": "1.3.2: Data Wrangling",
    "section": "0. Prework - Before You Begin",
    "text": "0. Prework - Before You Begin\nInstall Packages\nBefore you begin, please go ahead and install the following packages - these are all on CRAN, so you can install them using the RStudio Menu “Tools/Install” Packages interface:\n\n\nreadr on CRAN and readr package website\n\n\nreadxl on CRAN and `readxl`` package website\n\n\nhaven on CRAN and haven package website\n\n\ndplyr on CRAN and dplyr package website\n\n\nHmisc on CRAN and Hmiscpackage website\n\n\npsych on CRAN and psych package website\n\n\narsenal on CRAN and arsenal package website\n\n\ngtsummary on CRAN and gtsummary package website\n\ntableone on CRAN\ngmodels on CRAN\n\nSee Module 1.3.1 on Installing Packages",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-read-in-data.",
    "href": "module132_DataWrangling.html#to-read-in-data.",
    "title": "1.3.2: Data Wrangling",
    "section": "1. To read in data.",
    "text": "1. To read in data.\nBegin with a NEW RStudio Project\nLet’s begin with a new RStudio Project.\n\nFirst click on the menu at top for “File/New Project”:\n\n\n\n\nNext choose either an “Existing Directory” or “New Directory” depending on whether you want to use a folder that already exists on your computer or you want to create a new folder.\n\n\n\nFor now, let’s choose a “New Directory” and then select “New Project”\n\n\n\n\nWhen the next window opens, as an example, I’m creating a new project folder called myfirstRproject for my RStudio project under my parent directory, C:\\MyGithub. Your folder names and directories will most likely be different than mine.\n\n\n\n\nSo, if I look back on my computer in my file manager (I’m on a computer with Windows 11 operating system) - I can now see this new folder on my computer for C:\\MyGithub\\myfirstRproject.\n\n\n\n\nNow let’s put some data into this folder. Feel free to move datasets of your own into this new RStudio project directory. But here are some test datasets you can download and place into this new directory on your computer - choose at least one to try out - right click on each link and use “Save As” to save the file on your computer in your new project folder.\n\n\nmydata.csv - CSV (comma separated value) formatted data\nmydata.xlsx - EXCEL file\nmydata.sav - SPSS Dataset\nmydata.sas7bdat - SAS Dataset\nMydata_Codebook.pdf - Codebook Details on “mydata” dataset\n\n\nAfter putting these files into your new RStudio project folder, you should see something like this now in your RStudio Files Listing (bottom right window pane):\n\n\n\nImporting Data\nNow that you’ve got some data in your RStudio project folder, let’s look at options for importing these datasets into your RStudio computing session.\nClick on “File/Import Dataset” - and then choose the file format you want.\nImport a CSV file\n\n\n\n\n\n\nWhat is a CSV file?\n\n\n\nCSV stands for “comma separated value” format. This format is what you would think - each value for a different column (or variable) is separated by a column and each new row represents a new record in the dataset.\nCSV is widely accepted as a “universal” standard as a data format for easy exchange between different software and databases.\n\nWikipedia Page on CSV\nLibrary of Congress Page on CSV\nThere is even a conference on CSV\n\n\n\n\nHere is an example of importing the mydata.csv - CSV formatted data. Let’s use the From Text (readr) option.\n\n\nWhy should we use the “from text” option? Why do I not see a CSV option?\nTechnically the CSV format is TEXT. You can open a CSV file in a text editor and easily read it - even if you do not have proprietary software like Excel, Access, SPSS, SAS, etc. Here is a screen shot of what the “mydata.csv” file looks like in my text editor “Notepad” on my Windows 11 computer:\nNotice that:\n\nThe first row has text labels for the “variables” (columns) in the dataset - there are 14 column labels with each value separated by a , comma.\nThe remaining rows are the “data” for the dataset.\nAfter the 1st row of labels, there are 21 rows of data.\nTake a minute and notice there are some odd values, and odd patterns of missing data (two commas ,, together indicate that value is missing for that column (variable)). We’ll explore these issues further in later lesson modules.\n\n\n\n\nOnce the “File/Import Data/From Text (readr)” opens, click on “Browse” and choose the mydata.csv file. Assuming all goes well, this window will read the top of the datafile and show you a quick “Data Preview” to check that the import will work.\nAnd on the bottom right, the “Code Preview” shows you the R code commands needed to import this dataset. You can then click on the little “clipboard” on the bottom right to copy this R code to your “clipboard”, (the R code option will be explained below).\nOR You can also just click “Import” and the R code will be executed for you and the dataset brought into your R computing session (but this is NOT a good practice for reproducible research!).\n\nThe better way is to save the R code commands to import the data so you will be able to reproduce all steps in your data analysis workflow using code as opposed to non-reproducible point-and-click steps.\nOnce you copied the R code above to your clipboard, go to “File/New File/R Script” to open a script programming window:\n\nAnd then “paste” your R code into this window.\nAs you can see importing the mydata.csv dataset, involves 2 steps:\n\nLoading the readr package into your RStudio computing session, by running library(readr)\n\nRunning the read_csv() function from the readr package and then assigning &lt;- this output into a new R data object called mydata.\n\n\nTo import the dataset, select these 2 lines of code and then click “Run” to run the R code. And be sure to click “Save” to save your first R program - for example “importdata.R”.\n\n\nAfter running these 2 lines of code, you should see something like this - the code messages in the bottom left “Console” window pane and a new R data object “mydata” in the top right “Global Environment” window pane.\n\n\nImport an EXCEL file\nLet’s try another format. While you will probably encounter CSV (comma separated value) data files often (since nearly all data collection platforms, databases and software will be able to export this simple non-proprietary format), many people natively open/read CSV files in the EXCEL software. So you will probably also encounter EXCEL (*.XLS or *.XLSX) formatted data files.\nIn addition to an EXCEL file using a Microsoft proprietary format, EXCEL files can have formatting (font sizes, colors, borders) and can have multiple TABs (or SHEETs). Here are some screen shots of the mydata.xlsx - EXCEL file file.\nThe first “Data” TAB:\n\n\nThe second “Codebook” TAB:\n\n\nTo import an EXCEL file into R, we will use the same process as above, but this time we will select “File/Import Dataset/From Excel”:\n\nThis process uses the read_excel() function from the readxl package.\nWith the read_excel() function, we can specify several options including:\n\nWhich TAB do you want to import (for now we are only importing one data TAB at a time). We are selecting the “Data” TAB.\nI’m leaving all of the rest as their defaults which include:\n\nnot changing the “Range”,\nleaving “Max Rows” blank,\nand leaving rows to “Skip” as 0, which can be useful if you receive files with a lot of “header” information at the top,,\nleaving the “NA” box blank - but you could put in a value like “99” if you want all 99’s treated as missing - but this is applied to the ENTIRE dataset. We will look at these issues for individual variables below.\n\n\nAlso notice that the checkboxes are selected for “First Row as Names” (which is the usual convention) and “Open Data Viewer”, which creates the View(mydata) in the “Code Preview” window to the right. You can skip this if you like.\n\nSo in the “Code Preview” window to the right, we have specified the name of the data file \"mydata.xlsx\" and the “Data” TAB using the option sheet = \"Data\". Remember to copy this code to the clipboard and save it in a *.R program script.\n\n\nHere is the importdata.R program script we have so far for reading in the \"mydata.csv\" and \"mydata.xlsx\" data files. At the moment, the second time we “create” the mydata R data object we are overwriting the previous one in the sequential code steps below.\nAlso notice I have added some comments which start with a # hashtag. Any text following a # will be ignored by R and not executed.\n\n\nImport SPSS data\nFor data files from other “common” statistics software like SPSS, SAS and Stata, we can use the “File/Import Dataset/From SPSS (or From SAS or From Stata)”. All of these use read_xxx() functions from the haven package.\n\nHere is the code generated to import a SPSS datafile:\n\n\nImport SAS data\nImporting a *.sas7bdat SAS datafile, is similar to SPSS - here is that code.\nNotice that in addition to the datafile \"mydata.sas7bdat\", the read_sas() function also shows NULL. When reading in a SAS file, you can also add arguments for the catalog file and encoding specifics. You can read more on the Help pages for the haven::read_sas() function.\n\n\n\nHere is a quick summary of all of the data import codes shown above importdata.R:\n\n\n\n\n\n\nUsing = equals for parameter options inside a function\n\n\n\nNotice that we used sheet = \"Data\" inside the readxl::read_excel() function. The single = equals sign is used to assign a value to a parameter or option inside a function.\n\n\n\n# Import the CSV file\nlibrary(readr)\nmydata &lt;- read_csv(\"mydata.csv\")\n\n# Import the EXCEL file\n# Choose the \"Data\" TAB\nlibrary(readxl)\nmydata &lt;- read_excel(\"mydata.xlsx\", sheet = \"Data\")\n\n# Import a SPSS file\nlibrary(haven)\nmydata &lt;- read_sav(\"mydata.sav\")\n\n# Import a SAS file\nlibrary(haven)\nmydata &lt;- read_sas(\"mydata.sas7bdat\", NULL)\n\n\n\n\n\n\n\nhaven and foreign packages\n\n\n\nIn addition to the haven package which is part of tidyverse and has been around since 2015, there is also another useful package for importing and exporting other statistical software formats that has been around since 1999 and it still being maintained - the foreign package.\nIn addition to SPSS and Stata, the foreign package also can read in other formats like DBF, EPI INFO, Minitab, Octave, SSD (SAS Permanent Datasets via XPORT) SYSTAT, and ARFF.\nCompare current downloads of these 2 packages at https://hadley.shinyapps.io/cran-downloads/.\nWe can also review the history of these 2 packages using the pkgsearch package and the cran_package_history() function.\n\n# optionally install pkgsearch\n# install.packages(\"pkgsearch\")\nlibrary(pkgsearch)\n\n# get history of haven package\nhavenhistory &lt;- cran_package_history(\"haven\")\n\n# get history of foreign package\nforeignhistory &lt;- cran_package_history(\"foreign\")\n\n# display the earliest date on CRAN\n# for these 2 packages\nhavenhistory$date[1]\n\n[1] \"2015-03-01T08:18:16+00:00\"\n\nforeignhistory$date[1]\n\n[1] \"1999-12-17T02:05:13+00:00\"\n\n\n\n\n\nExploring Built-in Datasets\nIf you are looking for other datasets to test out functions or just need some data to play around with, the base R packages and other R packages (like palmerpenguins) have data built-in to them. You can use these datasets.\nWe can take a look at what datasets are available using the data() function:\n\n# take a look at the datasets available in the\n# \"datasets\" base R package\ndata()\n\nThis will open a viewer window (top left) - also notice that if you search for “Help” on the pressure dataset, you get a description of the dataset and the original source and citation. Notice in the “Help” window, the word pressure is followed by curly brackets indicating that the pressure dataset is in the built-in R package {datasets}.\n\n\nWe can see the pressure dataset is indeed in the datasets package if we keep scrolling down in the viewer window - also notice the mtcars dataset which you will often find in R tutorials and coding examples.\n\n\nOnce you know where to look, you can then explore lots of these datasets. For example, we can take a look at the built-in pressure dataset, which includes 19 values showing the relationship between temperature in degrees Celsius and pressure in mm (or mercury). To “see” this built-in data object, just type the name pressure to see (or print out) the object.\n\npressure\n\n   temperature pressure\n1            0   0.0002\n2           20   0.0012\n3           40   0.0060\n4           60   0.0300\n5           80   0.0900\n6          100   0.2700\n7          120   0.7500\n8          140   1.8500\n9          160   4.2000\n10         180   8.8000\n11         200  17.3000\n12         220  32.1000\n13         240  57.0000\n14         260  96.0000\n15         280 157.0000\n16         300 247.0000\n17         320 376.0000\n18         340 558.0000\n19         360 806.0000\n\n\nNormally most datasets are much larger than this little dataset. So, I would not advise trying to view most datasets by printing them to the “Console” window pane. Instead you can either click on the object in your “Global Environment” to view it - or you can run the View() function to open the viewer window.\nYou can “load” the built-in pressure dataset using the data(pressure) function to load the pressure dataset to load into your “Global Environment”, which loads the dataset into your R session.\n\nIf we click on the little “Table icon” all the way to the right of the pressure dataset in the “Global Environment” window - or run View(pressure) - we can open the dataset in the Viewer window:\n\ndata(pressure)\nView(pressure)\n\n\n\n\n\n\n\n\nExplore Datasets in R Packages\n\n\n\nI encourage you to use the data(package = \"xxx\") function to see what, if any, datasets may be built-in to the various packages you may install and load during your R computing sessions.\n\n\n\nIf you are interested in seeing other datasets in other R packages, go ahead and install the palmerpenguins package and take a look at the penguins dataset included:\n\n# look at datasets included with the\n# palmerpenguins dataset\ndata(package = \"palmerpenguins\")\n\nYou can learn more about the penguins dataset, by opening up the “Help” page for the dataset. You can also load the palmerpenguins package and then load the penguins dataset using this code.\n\nhelp(penguins, package = \"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(penguins)\n\n\n\nAnd clicking the the little data table icon after loading the penguins dataset into the “Global Environment”, you can see the dataset in the viewer window.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-view-the-data.",
    "href": "module132_DataWrangling.html#to-view-the-data.",
    "title": "1.3.2: Data Wrangling",
    "section": "2. To view The Data.",
    "text": "2. To view The Data.\nLook at small data in Console\nLet’s work with the mydata dataset that we imported above using the readr::read_csv() function.\n\n# import the mydata.csv dataset\nmydata &lt;- readr::read_csv(\"mydata.csv\")\n\nThis is not a very large dataset - mydata has 21 rows (or observations) and 14 variables (or columns). So, we can view the whole thing by printing it to the “Console” window.\nYou’ll notice that depending on the size of your current “Console” window, font size, zoom settings and more, what you see may vary. Since we read this dataset in using the readr package, the data object is now a “tibble” dataframe which only shows the columns and rows that will reasonably show up in your “Console” window.\n\n\n\n\n\n\nWhat is a “tibble” tbl_df?\n\n\n\nAs stated on the homepage for the tibble package at https://tibble.tidyverse.org/, a “tibble” is\n\n“… a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not.”\n\nAlso a “tibble” has\n\n“… an enhanced print() method which makes them easier to use with large datasets containing complex objects.”\n\n\n\nAnd the output below also lists what kind of column each variable is. For example,\n\n\nAge is a &lt;dbl&gt; indicating it is a numeric variable saved using double-precision, whereas\n\nGenderSTR is &lt;chr&gt; indicating this is a text or character (or “string”) type variable.\n\n\n# print the dataset into the Console\nmydata\n\n# A tibble: 21 × 14\n   SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n 1         1    45        68        145    5.6     9 m                   1     4\n 2         2    50       167        166    5.4     2 f                   2     3\n 3         3    35       143        135    5.6     2 &lt;NA&gt;               NA     3\n 4         4    44       216        201    5.6     2 m                   1     4\n 5         5    32       243        223    6       2 m                   1     5\n 6         6    48       165        145    5.2     2 f                   2     2\n 7         8    50        60        132    3.3     2 m                   1     3\n 8         9    51       110        108    5.1     3 f                   2     1\n 9        12    46       167        158    5.5     2 F                   2     1\n10        14    35       190        200    5.8     1 Male                1     4\n# ℹ 11 more rows\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n\n\nLook the “structure” of the dataset\nYou can also view the different kinds of variables in the dataset using the str() or “structure” function - which lists the type of variable, the number of elements in each column [1:21] indicates each column has 21 elements (or 21 rows) and the other values are a quick “peek” at the data inside the dataset. For example, the first 3 people in this dataset are ages 45, 50 and 35.\n\nstr(mydata)\n\nspc_tbl_ [21 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ SubjectID  : num [1:21] 1 2 3 4 5 6 8 9 12 14 ...\n $ Age        : num [1:21] 45 50 35 44 32 48 50 51 46 35 ...\n $ WeightPRE  : num [1:21] 68 167 143 216 243 165 60 110 167 190 ...\n $ WeightPOST : num [1:21] 145 166 135 201 223 145 132 108 158 200 ...\n $ Height     : num [1:21] 5.6 5.4 5.6 5.6 6 5.2 3.3 5.1 5.5 5.8 ...\n $ SES        : num [1:21] 9 2 2 2 2 2 2 3 2 1 ...\n $ GenderSTR  : chr [1:21] \"m\" \"f\" NA \"m\" ...\n $ GenderCoded: num [1:21] 1 2 NA 1 1 2 1 2 2 1 ...\n $ q1         : num [1:21] 4 3 3 4 5 2 3 1 1 4 ...\n $ q2         : num [1:21] NA 4 4 2 3 5 NA 4 1 44 ...\n $ q3         : num [1:21] NA 1 2 2 5 5 4 1 5 1 ...\n $ q4         : num [1:21] 4 40 3 1 2 1 3 3 5 1 ...\n $ q5         : num [1:21] 4 3 5 1 4 4 9 1 1 4 ...\n $ q6         : num [1:21] 5 2 2 9 1 5 2 4 2 5 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   SubjectID = col_double(),\n  ..   Age = col_double(),\n  ..   WeightPRE = col_double(),\n  ..   WeightPOST = col_double(),\n  ..   Height = col_double(),\n  ..   SES = col_double(),\n  ..   GenderSTR = col_character(),\n  ..   GenderCoded = col_double(),\n  ..   q1 = col_double(),\n  ..   q2 = col_double(),\n  ..   q3 = col_double(),\n  ..   q4 = col_double(),\n  ..   q5 = col_double(),\n  ..   q6 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nYou can also interactively View the data by clicking on the data icon and you can also click the little “table” icon to the far right next to the dataset in the “Global Environment”to open the data viewer window on the left.\nYou can also click on the little blue circle to the left of the mydata dataset to change the arrow from facing right  to facing down  to see the “structure” of the data in the “Global Environment”.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-subset-the-data---select-and-filter.",
    "href": "module132_DataWrangling.html#to-subset-the-data---select-and-filter.",
    "title": "1.3.2: Data Wrangling",
    "section": "3. To subset the data - select and filter.",
    "text": "3. To subset the data - select and filter.\nUsing base R packages and functions\nView parts of the dataset\nNow let’s “explore” the data by viewing sections of it.\nUsing base R commands, we can use functions like head() and tail() with each showing either the top or bottom 6 rows of the dataset. We can add a number to the function call to see more or less rows if we wish.\n\n# look at top 6 rows of data\nhead(mydata)\n\n# A tibble: 6 × 14\n  SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1         1    45        68        145    5.6     9 m                   1     4\n2         2    50       167        166    5.4     2 f                   2     3\n3         3    35       143        135    5.6     2 &lt;NA&gt;               NA     3\n4         4    44       216        201    5.6     2 m                   1     4\n5         5    32       243        223    6       2 m                   1     5\n6         6    48       165        145    5.2     2 f                   2     2\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n# look at the bottom 10 rows of data\ntail(mydata, n=10)\n\n# A tibble: 10 × 14\n   SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n 1        19    40       200        195    6.1     1 f                   2     1\n 2        21    99       180        185    5.9     3 f                   2     2\n 3        22    52       240        220    6.5     2 m                   1     2\n 4        23    24       250        240    6.4     2 M                   1     5\n 5        24    35       175        174    5.8     2 F                   2     5\n 6        27    51       220        221    6.3     2 m                   1     4\n 7        28    43       230         98    2.6     2 m                   1    11\n 8        30    36       190        180    5.7     1 female              2     5\n 9        32    44       260        109    6.4     3 male                1     1\n10        NA    NA        NA         NA   NA      NA &lt;NA&gt;               NA    NA\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nWhat are these wierd NAs?\n\n\n\nThe NA letters that show up is how R stores missing data. If the dataset you import has a blank cell (for either numeric or character type data), then R interprets that as “not available” which is indicated by NA. NA is a reserved word in R specifically set aside for handling missing values. \nYou can learn more about NA by running:\n\nhelp(NA, package = \"base\")\n\n\n\nYou can also view different parts of the data by using square brackets [] to select specific rows and columns using [row, column] index indicators.\n\n# Select the values in rows 1-4\n# and in columns 1-3\nmydata[1:4, 1:3]\n\n# A tibble: 4 × 3\n  SubjectID   Age WeightPRE\n      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1         1    45        68\n2         2    50       167\n3         3    35       143\n4         4    44       216\n\n\nTo select all of a given row or column just leave that index blank.\n\n# show all of rows 1-2\nmydata[1:2, ]\n\n# A tibble: 2 × 14\n  SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1         1    45        68        145    5.6     9 m                   1     4\n2         2    50       167        166    5.4     2 f                   2     3\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n# show all of columns 3-4\nmydata[ ,3:4]\n\n# A tibble: 21 × 2\n   WeightPRE WeightPOST\n       &lt;dbl&gt;      &lt;dbl&gt;\n 1        68        145\n 2       167        166\n 3       143        135\n 4       216        201\n 5       243        223\n 6       165        145\n 7        60        132\n 8       110        108\n 9       167        158\n10       190        200\n# ℹ 11 more rows\n\n\n\nView variables in dataset by name\nWe can also select columns from a dataset using the variable (or column) name. To see the names of all of the variables in a dataset, use the names() function.\n\n# list variable names in mydata\nnames(mydata)\n\n [1] \"SubjectID\"   \"Age\"         \"WeightPRE\"   \"WeightPOST\"  \"Height\"     \n [6] \"SES\"         \"GenderSTR\"   \"GenderCoded\" \"q1\"          \"q2\"         \n[11] \"q3\"          \"q4\"          \"q5\"          \"q6\"         \n\n\nWe can use the $ “dollar sign” operator to “select” named variables out of a dataset. Let’s look at all of the ages in mydata.\n\n# look at all of the ages\n# of the 21 people in mydata\nmydata$Age\n\n [1] 45 50 35 44 32 48 50 51 46 35 36 40 99 52 24 35 51 43 36 44 NA\n\n\nWe can also use these variable names with the [] brackets in base R syntax. And we use the c() combine function to help us put a list together. Let’s look at the 2 weight columns in the dataset. Put the variable names inside \"\" double quotes.\n\n# show all rows for\n# the 2 weight variables in mydata\nmydata[ , c(\"WeightPRE\", \"WeightPOST\")]\n\n# A tibble: 21 × 2\n   WeightPRE WeightPOST\n       &lt;dbl&gt;      &lt;dbl&gt;\n 1        68        145\n 2       167        166\n 3       143        135\n 4       216        201\n 5       243        223\n 6       165        145\n 7        60        132\n 8       110        108\n 9       167        158\n10       190        200\n# ℹ 11 more rows\n\n\n\nUsing dplyr functions\nUsing tidyverse packages and functions\nAs you can see while base R is very powerful on it’s own, the syntax is less than intuitive. There is a whole suite of R packages that are designed to work together and use a different syntax that improves programming workflow and readability.\nLearn more about the suite of tidyverse packages. You’ve already used two of these, readr and haven are both part of tidyverse for importing datasets.\nAnother one of these tidyverse packages, dplyr is a very good package for “data wrangling”.\nPick columns using dplyr::select()\nInstead of using the base R $ selector, the dplyr package has a select() function where you simply choose variables using their name. Let’s look at Height and q1 from the mydata dataset.\n\n\n\n\n\n\nUsing package::function() syntax\n\n\n\nIt is good coding practice, especially when loading several packages at once into your computing session, to make sure you are calling the exact function you want from a specific package. So, I’m using the syntax of package::function() to help keep track of which package and which function is being used below.\n\n\n\n# load dplyr package\nlibrary(dplyr)\n\n# select Height and q1 from mydata\ndplyr::select(mydata, c(Height, q1))\n\n# A tibble: 21 × 2\n   Height    q1\n    &lt;dbl&gt; &lt;dbl&gt;\n 1    5.6     4\n 2    5.4     3\n 3    5.6     3\n 4    5.6     4\n 5    6       5\n 6    5.2     2\n 7    3.3     3\n 8    5.1     1\n 9    5.5     1\n10    5.8     4\n# ℹ 11 more rows\n\n\nWorkflow using the pipe %&gt;% operator\nAnother improvement of the tidyverse approach of R programming is to use the pipe %&gt;% operator. Basically what this syntax does is take the results from “A” and pipe it into –&gt; the next “B” function, e.g. A %&gt;% B so we can begin to “daisy-chain” a sequence of programming steps together into a logical workflow that is easy to “read” and follow.\nHere is a working example to show the same variable selection process we did above, but now we will be using the dplyr::select() function. The code below takes the mydata dataset and pipes %&gt;% it into the select() function. We were also able to drop using the c() function here.\n\n# start with mydata and then \n# select Height and q1 from mydata\nmydata %&gt;% dplyr::select(Height, q1)\n\n# A tibble: 21 × 2\n   Height    q1\n    &lt;dbl&gt; &lt;dbl&gt;\n 1    5.6     4\n 2    5.4     3\n 3    5.6     3\n 4    5.6     4\n 5    6       5\n 6    5.2     2\n 7    3.3     3\n 8    5.1     1\n 9    5.5     1\n10    5.8     4\n# ℹ 11 more rows\n\n\nWe could even add the base R head() function here. If we put each code step on a separate line, you can now see that we are [1] taking the mydata dataset “and then” [2] selecting 2 variables “and then” [3] looking at the top 6 rows of the dataset.\n\n# select Height and q1 from mydata\n# and show only the top 6 rows\nmydata %&gt;% \n  dplyr::select(Height, q1) %&gt;%\n  head()\n\n# A tibble: 6 × 2\n  Height    q1\n   &lt;dbl&gt; &lt;dbl&gt;\n1    5.6     4\n2    5.4     3\n3    5.6     3\n4    5.6     4\n5    6       5\n6    5.2     2\n\n\n\n\n\n\n\n\nTL;DR If %&gt;% is a pipe, then what is |&gt;??\n\n\n\nThe %&gt;% pipe operator is implemented within tidyverse from the magrittr package which is used by the tidyverse packages which started being used quite extensively by R programmers over the last decade.\nHowever, the rest of the R development community (which is much larger than just those who use the tidyverse suite) also recently added a new base R pipe operator |&gt; (since R version 4.1.0).\nLearn more in this tidyverse blog post from 2023\n\n\nSo, you do have the option to also use the base R |&gt; pipe operator.\n\n# select Height and q1 from mydata\n# and show only the top 6 rows\nmydata |&gt; \n  dplyr::select(Height, q1) |&gt;\n  head()\n\n# A tibble: 6 × 2\n  Height    q1\n   &lt;dbl&gt; &lt;dbl&gt;\n1    5.6     4\n2    5.4     3\n3    5.6     3\n4    5.6     4\n5    6       5\n6    5.2     2\n\n\nFor now, we will stay with the %&gt;% operator for consistency. But be aware that you will see both approaches on the Internet when “Googling” for answers.\nSelect variables with matching using starts_with()\nWhen using dplyr::select() to select variables, there are several “helper functions” that are useful for “selection”. You can see a list of these functions by running help(\"starts_with\", package = \"tidyselect\"). These “selection helper” functions are actually in the tidyselect package which is loaded with the dplyr package.\nLet’s use these functions to pull out all of the Likert-scaled “question” variables that start with the letter \"q\".\n\nmydata %&gt;%\n  dplyr::select(starts_with(\"q\"))\n\n# A tibble: 21 × 6\n      q1    q2    q3    q4    q5    q6\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     4    NA    NA     4     4     5\n 2     3     4     1    40     3     2\n 3     3     4     2     3     5     2\n 4     4     2     2     1     1     9\n 5     5     3     5     2     4     1\n 6     2     5     5     1     4     5\n 7     3    NA     4     3     9     2\n 8     1     4     1     3     1     4\n 9     1     1     5     5     1     2\n10     4    44     1     1     4     5\n# ℹ 11 more rows\n\n\n\nPick rows using dplyr::filter()\nIn addition to selecting columns or variables from your dataset, you can also pull out a subset of your data by “filtering” out only the rows you want.\nFor example, suppose we only want to look at the Age, WeightPRE for the Females in the dataset indicates by GenderCoded equal to 2.\nFor reference, take a look at the mydata codebook - and here is a screenshot as well:\n\n\nNotice that:\n\nI changed the order of the columns, which is OK,\nand to filter out and KEEP only the rows for females, I typed GenderCoded == 2 using two equal signs ==. R uses two == equal signs to perform a logical operation to ask does the variable GenderCoded equal the value of 2, with either a TRUE or FALSE result. Only the rows with a TRUE result are shown.\n\n\n\n\n\n\n\nBe careful not to mix up = and ==\n\n\n\nOdds are you will get errors at some point due to typos or other issues, but a common error is to use a single = equals sign when trying to perform a logic operation. Remember to use 2 equals signs == if you are trying to perform a TRUE/FALSE operation and use only 1 equals sign = when assigning a value to a function argument.\n\n\n\n# select columns from mydata\n# and then only show rows for females\nmydata %&gt;%\n  select(GenderCoded, Age, WeightPRE) %&gt;%\n  filter(GenderCoded == 2)\n\n# A tibble: 8 × 3\n  GenderCoded   Age WeightPRE\n        &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1           2    50       167\n2           2    48       165\n3           2    51       110\n4           2    46       167\n5           2    40       200\n6           2    99       180\n7           2    35       175\n8           2    36       190\n\n\nHere is an example of the error you will get if you use a single = sign instead of == two.\n\n# select columns from mydata\n# and then only show rows for females\nmydata %&gt;%\n  select(GenderCoded, Age, WeightPRE) %&gt;%\n  filter(GenderCoded = 2)\n\nError in `filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `GenderCoded == 2`?\nRun `rlang::last_trace()` to see where the error occurred.\n\nFilter rows using matching %in% operator\nAnother helpful operator in R is the %in% operator used for matching. Let’s suppose we wanted to pull out the rows for specific subject IDs - perhaps you want to review only these records.\nLet’s pull out the data for only IDs 14, 21 and 24. Rather than writing a complicated if-then-else set of code steps, we can search for these IDs and only the rows with these IDs will be kept.\n\nmydata %&gt;%\n  filter(SubjectID %in% c(14, 21, 24))\n\n# A tibble: 3 × 14\n  SubjectID   Age WeightPRE WeightPOST Height   SES GenderSTR GenderCoded    q1\n      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1        14    35       190        200    5.8     1 Male                1     4\n2        21    99       180        185    5.9     3 f                   2     2\n3        24    35       175        174    5.8     2 F                   2     5\n# ℹ 5 more variables: q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;\n\n\nSort/arrange rows using dplyr::arrange()\nHere is another helpful function from dplyr. Suppose we want to find the 5 oldest people in mydata and show their IDs.\nLet’s use the dplyr::arrange() function which will sort our data based on the variable we specify in increasing order (lowest to highest) by default. We will add the desc() function to sort decreasing from largest to smallest.\nLearn more by running help(arrange, package = \"dplyr\")\nNote: There was someone with age 99 in this made-up dataset.\n\n# take mydata\n# select SubjectID and Age\n# sort descending by Age\n# show the top 5 IDs and Ages\nmydata %&gt;%\n  select(SubjectID, Age) %&gt;%\n  arrange(desc(Age)) %&gt;%\n  head(n=5)\n\n# A tibble: 5 × 2\n  SubjectID   Age\n      &lt;dbl&gt; &lt;dbl&gt;\n1        21    99\n2        22    52\n3         9    51\n4        27    51\n5         2    50\n\n\nThe oldest people are subject IDs 21, 22, 9, 27 and 2 who are age 99, 52, 51, 51 and 50 years old respectively.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-create-and-modify-variables.",
    "href": "module132_DataWrangling.html#to-create-and-modify-variables.",
    "title": "1.3.2: Data Wrangling",
    "section": "4. To create and modify variables.",
    "text": "4. To create and modify variables.\nTo create and add new variables to the dataset, we can use either a base R approach or use the mutate() function from the dplyr package. Let’s take a look at both approaches. In the mydata dataset, we have Height in decimal feet and we have WeightPRE and WeightPOST in pounds.\nSo, let’s compute BMI (body mass index) as follows from Height (in inches) and Weight (in pounds):\n\\[BMI = \\left(\\frac{weight_{(lbs)}}{(height_{(inches)})^2}\\right) * 703\\]\nCreate New Variable - Base R Approach\nCreate a new variable using the $ selector operator. Then write out the mathematical equation. I also had to multiply the height in decimal feet * 12 to get inches.\n\n# Compute BMI for the PRE Weight\nmydata$bmiPRE &lt;- \n  (mydata$WeightPRE * 703) / (mydata$Height * 12)^2\n\n# look at result\nmydata$bmiPRE\n\n [1]  10.58585  27.95901  22.26142  33.62564  32.95312  29.78997  26.89777\n [8]  20.64644  26.95156  27.57341  29.21039  26.23996  25.24418  27.73176\n[15]  29.79702  25.39656  27.06041 166.10166  28.54938  30.98891        NA\n\n\nLook at the “Global Environment” or run the str() function to see if a new variable was added to mydata - which should now have 15 variables instead of only 14.\nYou can also list the variable names in the updated dataset.\n\n# look at updated data structure\nstr(mydata)\n\nspc_tbl_ [21 × 15] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ SubjectID  : num [1:21] 1 2 3 4 5 6 8 9 12 14 ...\n $ Age        : num [1:21] 45 50 35 44 32 48 50 51 46 35 ...\n $ WeightPRE  : num [1:21] 68 167 143 216 243 165 60 110 167 190 ...\n $ WeightPOST : num [1:21] 145 166 135 201 223 145 132 108 158 200 ...\n $ Height     : num [1:21] 5.6 5.4 5.6 5.6 6 5.2 3.3 5.1 5.5 5.8 ...\n $ SES        : num [1:21] 9 2 2 2 2 2 2 3 2 1 ...\n $ GenderSTR  : chr [1:21] \"m\" \"f\" NA \"m\" ...\n $ GenderCoded: num [1:21] 1 2 NA 1 1 2 1 2 2 1 ...\n $ q1         : num [1:21] 4 3 3 4 5 2 3 1 1 4 ...\n $ q2         : num [1:21] NA 4 4 2 3 5 NA 4 1 44 ...\n $ q3         : num [1:21] NA 1 2 2 5 5 4 1 5 1 ...\n $ q4         : num [1:21] 4 40 3 1 2 1 3 3 5 1 ...\n $ q5         : num [1:21] 4 3 5 1 4 4 9 1 1 4 ...\n $ q6         : num [1:21] 5 2 2 9 1 5 2 4 2 5 ...\n $ bmiPRE     : num [1:21] 10.6 28 22.3 33.6 33 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   SubjectID = col_double(),\n  ..   Age = col_double(),\n  ..   WeightPRE = col_double(),\n  ..   WeightPOST = col_double(),\n  ..   Height = col_double(),\n  ..   SES = col_double(),\n  ..   GenderSTR = col_character(),\n  ..   GenderCoded = col_double(),\n  ..   q1 = col_double(),\n  ..   q2 = col_double(),\n  ..   q3 = col_double(),\n  ..   q4 = col_double(),\n  ..   q5 = col_double(),\n  ..   q6 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# list the variable names in the\n# updated dataset\nnames(mydata)\n\n [1] \"SubjectID\"   \"Age\"         \"WeightPRE\"   \"WeightPOST\"  \"Height\"     \n [6] \"SES\"         \"GenderSTR\"   \"GenderCoded\" \"q1\"          \"q2\"         \n[11] \"q3\"          \"q4\"          \"q5\"          \"q6\"          \"bmiPRE\"     \n\n\n\nCreate New Variable - dplyr::mutate() Approach\nIn the dplyr package, you can create or modify variables using the mutate() function.\n\n# Compute BMI for the POST Weight\n# use the dplyr::mutate() function\nmydata &lt;- mydata %&gt;%\n  mutate(\n    bmiPOST = (WeightPOST * 703) / (Height * 12)^2\n    )\n\n# check updates\nstr(mydata)\n\ntibble [21 × 16] (S3: tbl_df/tbl/data.frame)\n $ SubjectID  : num [1:21] 1 2 3 4 5 6 8 9 12 14 ...\n $ Age        : num [1:21] 45 50 35 44 32 48 50 51 46 35 ...\n $ WeightPRE  : num [1:21] 68 167 143 216 243 165 60 110 167 190 ...\n $ WeightPOST : num [1:21] 145 166 135 201 223 145 132 108 158 200 ...\n $ Height     : num [1:21] 5.6 5.4 5.6 5.6 6 5.2 3.3 5.1 5.5 5.8 ...\n $ SES        : num [1:21] 9 2 2 2 2 2 2 3 2 1 ...\n $ GenderSTR  : chr [1:21] \"m\" \"f\" NA \"m\" ...\n $ GenderCoded: num [1:21] 1 2 NA 1 1 2 1 2 2 1 ...\n $ q1         : num [1:21] 4 3 3 4 5 2 3 1 1 4 ...\n $ q2         : num [1:21] NA 4 4 2 3 5 NA 4 1 44 ...\n $ q3         : num [1:21] NA 1 2 2 5 5 4 1 5 1 ...\n $ q4         : num [1:21] 4 40 3 1 2 1 3 3 5 1 ...\n $ q5         : num [1:21] 4 3 5 1 4 4 9 1 1 4 ...\n $ q6         : num [1:21] 5 2 2 9 1 5 2 4 2 5 ...\n $ bmiPRE     : num [1:21] 10.6 28 22.3 33.6 33 ...\n $ bmiPOST    : num [1:21] 22.6 27.8 21 31.3 30.2 ...\n\nnames(mydata)\n\n [1] \"SubjectID\"   \"Age\"         \"WeightPRE\"   \"WeightPOST\"  \"Height\"     \n [6] \"SES\"         \"GenderSTR\"   \"GenderCoded\" \"q1\"          \"q2\"         \n[11] \"q3\"          \"q4\"          \"q5\"          \"q6\"          \"bmiPRE\"     \n[16] \"bmiPOST\"    \n\n\n\nCreate New Variable - add labels to codes by creating a “factor” type variable\nAs you probably noticed in the views of the mydata dataset above, there was originally a variable where people were allowed to enter their gender using free text (the GenderSTR variable). There were entries like “f”, “F”, “female”, “male”, “Male” and other variations. So, another variable GenderCoded was included where 1=male and 2=female, but when we look at mydata$GenderCoded all we see are 1’s and 2’s and NAs.\n\nmydata$GenderCoded\n\n [1]  1  2 NA  1  1  2  1  2  2  1  1  2  2  1  1  2  1  1  2  1 NA\n\n\nIt would be nice if we could add some labels. One way to do this is to convert GenderCoded from being a simple “numeric” variable to a new object class called a “factor” which includes both numeric values and text labels.\nHere is the base R approach to create a new factor type variable. Learn more by looking at the help page for factor(), run help(factor, package = \"base\").\n\n# create a new factor with labels\nmydata$GenderCoded.f &lt;-\n  factor(mydata$GenderCoded,\n         levels = c(1, 2),\n         labels = c(\"Male\", \"Female\"))\n\n# look at new variable\nmydata$GenderCoded.f\n\n [1] Male   Female &lt;NA&gt;   Male   Male   Female Male   Female Female Male  \n[11] Male   Female Female Male   Male   Female Male   Male   Female Male  \n[21] &lt;NA&gt;  \nLevels: Male Female\n\n\nWe can check the type each variable using the class() function.\n\nclass(mydata$GenderCoded)\n\n[1] \"numeric\"\n\nclass(mydata$GenderCoded.f)\n\n[1] \"factor\"\n\n\nAnother quick way to see these class type differences is to use the table() function to get the frequencies of each distinct value. I’m also adding the useNA = \"ifany\" option to also get a count of any missing values. Learn more by running help(table, package = \"base\").\n\n# table of frequencies of GenderCoded - numeric class\ntable(mydata$GenderCoded, useNA = \"ifany\")\n\n\n   1    2 &lt;NA&gt; \n  11    8    2 \n\n# table of GenderCoded.f - factor class\ntable(mydata$GenderCoded.f, useNA = \"ifany\")\n\n\n  Male Female   &lt;NA&gt; \n    11      8      2",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#to-get-data-summary-and-descriptive-statistics.",
    "href": "module132_DataWrangling.html#to-get-data-summary-and-descriptive-statistics.",
    "title": "1.3.2: Data Wrangling",
    "section": "5. To get data summary and descriptive statistics.",
    "text": "5. To get data summary and descriptive statistics.\nGetting summary statistics\nsummary() function\nOne of the best functions that is part of base R is the summary() function. Let’s see what this gives us for the mydata dataset.\nAs you can see for all of the numeric class variables, the summary() function gives us the min, max, median, mean, 1st quartile and 3rd quartile and a count of the the number of missing NAs. So, you can see the mean Age is 44.8 and the median Age is 44.0.\nFor the character variable GenderSTR all we know is it has a length of 21.\nBut for the factor type variable GenderCoded.f we get the number of Males, Females and NAs.\n\nsummary(mydata)\n\n   SubjectID          Age          WeightPRE       WeightPOST   \n Min.   : 1.00   Min.   :24.00   Min.   : 60.0   Min.   : 98.0  \n 1st Qu.: 5.75   1st Qu.:35.75   1st Qu.:166.5   1st Qu.:142.5  \n Median :15.00   Median :44.00   Median :190.0   Median :177.0  \n Mean   :15.30   Mean   :44.80   Mean   :185.2   Mean   :172.2  \n 3rd Qu.:23.25   3rd Qu.:50.00   3rd Qu.:230.0   3rd Qu.:203.2  \n Max.   :32.00   Max.   :99.00   Max.   :260.0   Max.   :240.0  \n NA's   :1       NA's   :1       NA's   :1       NA's   :1      \n     Height           SES       GenderSTR          GenderCoded   \n Min.   :2.600   Min.   :1.0   Length:21          Min.   :1.000  \n 1st Qu.:5.475   1st Qu.:2.0   Class :character   1st Qu.:1.000  \n Median :5.750   Median :2.0   Mode  :character   Median :1.000  \n Mean   :5.550   Mean   :2.3                      Mean   :1.421  \n 3rd Qu.:6.125   3rd Qu.:2.0                      3rd Qu.:2.000  \n Max.   :6.500   Max.   :9.0                      Max.   :2.000  \n NA's   :1       NA's   :1                        NA's   :2      \n       q1              q2               q3             q4        \n Min.   : 1.00   Min.   : 1.000   Min.   :1.00   Min.   : 1.000  \n 1st Qu.: 1.75   1st Qu.: 2.000   1st Qu.:1.00   1st Qu.: 2.000  \n Median : 3.00   Median : 4.000   Median :3.00   Median : 3.000  \n Mean   : 3.35   Mean   : 5.526   Mean   :3.15   Mean   : 5.062  \n 3rd Qu.: 4.25   3rd Qu.: 4.500   3rd Qu.:4.25   3rd Qu.: 4.000  \n Max.   :11.00   Max.   :44.000   Max.   :9.00   Max.   :40.000  \n NA's   :1       NA's   :2        NA's   :1      NA's   :5       \n       q5               q6            bmiPRE          bmiPOST     \n Min.   : 1.000   Min.   :1.000   Min.   : 10.59   Min.   :12.99  \n 1st Qu.: 2.000   1st Qu.:2.000   1st Qu.: 26.03   1st Qu.:25.38  \n Median : 4.000   Median :4.000   Median : 27.65   Median :26.42  \n Mean   : 9.176   Mean   :3.706   Mean   : 33.78   Mean   :29.43  \n 3rd Qu.: 5.000   3rd Qu.:5.000   3rd Qu.: 29.79   3rd Qu.:28.71  \n Max.   :99.000   Max.   :9.000   Max.   :166.10   Max.   :70.77  \n NA's   :4        NA's   :4       NA's   :1        NA's   :1      \n GenderCoded.f\n Male  :11    \n Female: 8    \n NA's  : 2    \n              \n              \n              \n              \n\n\nSo, the summary() function is helpful, but you’ll notice we do not get the standard deviation. For some reason that was left out of the original summary() statistics function.\nThere are a few other descriptive statistics functions that can be useful. There is a describe() function in both the Hmisc package and the psych packages.\nHmisc::describe() function\nLet’s look at Hmisc::describe() for a couple of the variables.\nYou’ll notice that this still doesn’t give us the standard deviation, but we get the min, max, mean, median, as well as the .05 (5th percentile) and others, and the output includes a summary of the frequency of the distinct values.\n\nmydata %&gt;%\n  select(Age, GenderCoded.f, bmiPRE) %&gt;%\n  Hmisc::describe()\n\n. \n\n 3  Variables      21  Observations\n--------------------------------------------------------------------------------\nAge \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n      20        1       14    0.994     44.8       43    13.81    31.60 \n     .10      .25      .50      .75      .90      .95 \n   34.70    35.75    44.00    50.00    51.10    54.35 \n                                                                           \nValue        24   32   35   36   40   43   44   45   46   48   50   51   52\nFrequency     1    1    3    2    1    1    2    1    1    1    2    2    1\nProportion 0.05 0.05 0.15 0.10 0.05 0.05 0.10 0.05 0.05 0.05 0.10 0.10 0.05\n               \nValue        99\nFrequency     1\nProportion 0.05\n\nFor the frequency table, variable is rounded to the nearest 0\n--------------------------------------------------------------------------------\nGenderCoded.f \n       n  missing distinct \n      19        2        2 \n                        \nValue        Male Female\nFrequency      11      8\nProportion  0.579  0.421\n--------------------------------------------------------------------------------\nbmiPRE \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n      20        1       20        1    33.78    27.73    18.52    20.14 \n     .10      .25      .50      .75      .90      .95 \n   22.10    26.03    27.65    29.79    33.02    40.25 \n\n10.5858489229025 (1, 0.05), 20.6464394036482 (1, 0.05), 22.2614175878685 (1,\n0.05), 25.2441827061189 (1, 0.05), 25.3965599815035 (1, 0.05), 26.2399593896503\n(1, 0.05), 26.8977655341292 (1, 0.05), 26.9515610651974 (1, 0.05),\n27.0604126424232 (1, 0.05), 27.5734079799181 (1, 0.05), 27.7317554240631 (1,\n0.05), 27.9590096784027 (1, 0.05), 28.5493827160494 (1, 0.05), 29.2103855937103\n(1, 0.05), 29.7899716469428 (1, 0.05), 29.7970241970486 (1, 0.05),\n30.9889051649305 (1, 0.05), 32.953125 (1, 0.05), 33.6256377551021 (1, 0.05),\n166.101660092045 (1, 0.05)\n\nFor the frequency table, variable is rounded to the nearest 0\n--------------------------------------------------------------------------------\n\n\n\npsych::describe() function\nThe psych::describe() function only works on numeric data. So, let’s look at Age and bmiPRE. This function now gives us the standard deviation sd and even the mad which is the mean absolute deviation.\n\nmydata %&gt;%\n  select(Age, bmiPRE) %&gt;%\n  psych::describe()\n\n       vars  n  mean    sd median trimmed   mad   min   max  range skew\nAge       1 20 44.80 14.87  44.00   43.06 10.38 24.00  99.0  75.00 2.21\nbmiPRE    2 20 33.78 31.53  27.65   27.79  3.17 10.59 166.1 155.52 3.66\n       kurtosis   se\nAge        6.10 3.32\nbmiPRE    12.53 7.05\n\n\nBase R specific statistics functions\nThere are many built-in functions in base R for computing specific statistics like mean(), sd() for standard deviation, median(), min(), max() and quantile() to get specific percentiles.\nLet get some summary statistics for different variables in mydata.\n\n# get min, max for Age\nmin(mydata$Age)\n\n[1] NA\n\nmax(mydata$Age)\n\n[1] NA\n\n\nWAIT!? - why did I get NA? Since there is missing data in this dataset, we need to tell these R functions how to handle the missing data. We need to add na.rm=TRUE to remove the NAs and then compute the min() and max() for the non-missing values.\n\nmin(mydata$Age, na.rm = TRUE)\n\n[1] 24\n\nmax(mydata$Age, na.rm = TRUE)\n\n[1] 99\n\n\nIf we want, we could get the non-parametric statistics of median (which is the 50th percentile), 25th and 75th percentiles for the interquartile range. Let’s get these statistics for bmiPRE.\n\n# get median bmiPRE\n# and 25th and 75th percentiles for bmiPRE\nmedian(mydata$bmiPRE,\n       na.rm = TRUE)\n\n[1] 27.65258\n\nquantile(mydata$bmiPRE, \n         probs = 0.25,\n         na.rm = TRUE)\n\n     25% \n26.02911 \n\nquantile(mydata$bmiPRE, \n         probs = 0.75,\n         na.rm = TRUE)\n\n     75% \n29.79173 \n\n\nWe can also get the mean() and sd() for Height.\n\nmean(mydata$Height, na.rm = TRUE)\n\n[1] 5.55\n\nsd(mydata$Height, na.rm = TRUE)\n\n[1] 0.9795273\n\n\n\ndplyr::summarize() function\nThe dplyr package also has a summarize() function you can use to get specific statistics of your choosing. For example, let’s get the mean() and sd() for Age in one code step.\n\nmydata %&gt;%\n  dplyr::summarise(\n    mean_age = mean(Age, na.rm = TRUE),\n    sd_age = sd(Age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 2\n  mean_age sd_age\n     &lt;dbl&gt;  &lt;dbl&gt;\n1     44.8   14.9\n\n\nWe can do this same code again but add the dplyr::group_by() function to add a grouping variable to get the statistics by.\nNOTE: The dplyr::group_by() function must come BEFORE dplyr::summarise().\nLet’s get the summary stats (mean and sd) for Age by GenderCoded.f.\n\nmydata %&gt;%\n  dplyr::group_by(GenderCoded.f) %&gt;%\n  dplyr::summarise(\n    mean_age = mean(Age, na.rm = TRUE),\n    sd_age = sd(Age, na.rm = TRUE)\n  )\n\n# A tibble: 3 × 3\n  GenderCoded.f mean_age sd_age\n  &lt;fct&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n1 Male              41.5   8.77\n2 Female            50.6  20.5 \n3 &lt;NA&gt;              35    NA   \n\n\n\n\n\n\n\n\n\nEach code step may result in different object classes\n\n\n\nAs you work through a series of code steps in an analysis or computational workflow, each step may produce an output object with a different class.\n\n\nLet’s look at each step of the code above to produce a table of means and standard deviations of Age by GenderCoded.f.\nSTEP 1 - begin with the dataset\nWe start with the dataset mydata which is a “tibble” “data.frame” since we imported the data using one of the tidyverse packages: readr, readxl or haven all of which create a tbl_df class object.\n\n# Step 1\nclass(mydata)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nSTEP 2 - create a “grouped” data.frame\nNotice that as soon as we use the dplyr::group_by() function, the result is an updated type of “tibble”“data.frame” which is now a grouped_df class object. This object class is described at https://dplyr.tidyverse.org/articles/grouping.html.\nThe grouped_df is similar to:\n\napplying the SPLIT FILE command in the SPSS software\n\nor using the BY command in SAS to “work with grouped data”\n\n\n\n# save the output of step 2\nstep2 &lt;- mydata %&gt;%\n  dplyr::group_by(GenderCoded.f)\n\nclass(step2)\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nSTEP 3 - after the summarise step\nAfter STEP 3, another tbl_df is created.\n\nstep3 &lt;- mydata %&gt;%\n  dplyr::group_by(GenderCoded.f) %&gt;%\n  dplyr::summarise(\n    mean_age = mean(Age, na.rm = TRUE),\n    sd_age = sd(Age, na.rm = TRUE)\n  )\n\nclass(step3)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nSince this saved output object step3 is a tbl_df, we can use it like any other “data.frame” object. For example, we can pull out the mean_age column:\n\n# pull out the mean_age column using $\nstep3$mean_age\n\n[1] 41.45455 50.62500 35.00000\n\n# pull out the sd_age column using select()\nstep3 %&gt;%\n  select(sd_age)\n\n# A tibble: 3 × 1\n  sd_age\n   &lt;dbl&gt;\n1   8.77\n2  20.5 \n3  NA   \n\n\n\nMake summary tables\nCreating nicely formatted summary tables is an active area of development in the R community. So, I’m sure there are new functions and packages that I may not have shown here. But here are a few packages I use often for making tables of summary statistics. Most of these are designed to work within a Rmarkdown document.\narsenal package for tables\nThe arsenal package is useful for making tables - especially with Rmarkdown - to be explained further in a later session Module 1.3.6. Learn more at tableby() vignette.\nHere is a quick example of some summary statistics for Age, bmiPRE, and SES by GenderCoded.f using the tableby() function.\nFirst let’s add labels for SES and create a factor variable.\n\nmydata$SES.f &lt;- \n  factor(mydata$SES,\n         levels = c(1, 2, 3),\n         labels = c(\"low income\",\n                    \"average income\",\n                    \"high income\"))\n\nlibrary(arsenal)\ntab1 &lt;- tableby(GenderCoded.f ~ Age + bmiPRE +SES.f, \n                data = mydata)\nsummary(tab1)\n\n\n\n\n\n\n\n\n\n\nMale (N=11)\nFemale (N=8)\nTotal (N=19)\np value\n\n\n\nAge\n\n\n\n0.199\n\n\n   Mean (SD)\n41.455 (8.768)\n50.625 (20.493)\n45.316 (15.089)\n\n\n\n   Range\n24.000 - 52.000\n35.000 - 99.000\n24.000 - 99.000\n\n\n\nbmiPRE\n\n\n\n0.370\n\n\n   Mean (SD)\n40.230 (42.193)\n26.347 (2.785)\n34.384 (32.274)\n\n\n\n   Range\n10.586 - 166.102\n20.646 - 29.790\n10.586 - 166.102\n\n\n\nSES.f\n\n\n\n0.625\n\n\n   N-Miss\n1\n0\n1\n\n\n\n   low income\n2 (20.0%)\n2 (25.0%)\n4 (22.2%)\n\n\n\n   average income\n7 (70.0%)\n4 (50.0%)\n11 (61.1%)\n\n\n\n   high income\n1 (10.0%)\n2 (25.0%)\n3 (16.7%)\n\n\n\n\n\ngtsummary package for tables\nThe gtsummary package is also useful for making tables. We can even use it to make nicely formatted tables in the “Viewer” window pane or in Rmarkdown. Learn more at tbl_summary() vignette.\nHere is a quick example of some summary statistics for Age and bmiPRE by GenderCoded.f using the tbl_summary() function.\n\nlibrary(gtsummary)\n\nmydata %&gt;%\n  select(Age, bmiPRE, SES.f, GenderCoded.f) %&gt;%\n  tbl_summary(by = GenderCoded.f)\n\n\n\nTable 1\n\n\n\n\n\n\ntableone package for making summary tables\nThe output produced from tableone is simple text output.\n\nlibrary(tableone)\n\ntableone::CreateTableOne(\n  data = mydata,\n  vars = c(\"Age\", \"bmiPRE\", \"SES.f\"),\n  strata = \"GenderCoded.f\"\n)\n\n                    Stratified by GenderCoded.f\n                     Male          Female        p      test\n  n                     11             8                    \n  Age (mean (SD))    41.45 (8.77)  50.62 (20.49)  0.199     \n  bmiPRE (mean (SD)) 40.23 (42.19) 26.35 (2.79)   0.370     \n  SES.f (%)                                       0.625     \n     low income          2 (20.0)      2 (25.0)             \n     average income      7 (70.0)      4 (50.0)             \n     high income         1 (10.0)      2 (25.0)             \n\n\n\ngmodels package for R-x-C tables\nIf we want to look at a “cross-table” similar to output from SPSS or SAS, the gmodels package has the CrossTable() function that creates text-based tables similar to these other statistics software packages.\nLet’s get the frequencies and columns percentages for SES by gender. The first variable is the row variable, the second is the column variable.\n\nlibrary(gmodels)\n\nCrossTable(mydata$SES.f,          # row variable\n           mydata$GenderCoded.f,  # column variable\n           prop.t = FALSE,        # turn off percent of total\n           prop.r = FALSE,        # turn off percent of row\n           prop.c = TRUE,         # turn on percent of column\n           prop.chisq = FALSE,    # turn off percent for chisq test\n           format = \"SPSS\")       # format like SPSS\n\n\n   Cell Contents\n|-------------------------|\n|                   Count |\n|          Column Percent |\n|-------------------------|\n\nTotal Observations in Table:  18 \n\n               | mydata$GenderCoded.f \n  mydata$SES.f |     Male  |   Female  | Row Total | \n---------------|-----------|-----------|-----------|\n    low income |        2  |        2  |        4  | \n               |   20.000% |   25.000% |           | \n---------------|-----------|-----------|-----------|\naverage income |        7  |        4  |       11  | \n               |   70.000% |   50.000% |           | \n---------------|-----------|-----------|-----------|\n   high income |        1  |        2  |        3  | \n               |   10.000% |   25.000% |           | \n---------------|-----------|-----------|-----------|\n  Column Total |       10  |        8  |       18  | \n               |   55.556% |   44.444% |           | \n---------------|-----------|-----------|-----------|\n\n \n\n\nTable Inspiration\nMaking effective, nicely formatted tables from R and Rmarkdown has been an active area of development these past few years. In fact, I encourage you to check out the winners of the last few Table Contests:\n\n2024 Table Contest Winners\n2022 Table Contest Winners\n2021 Table Contest Winners\n\nOther Table Resources and Packages include:\n\nhttps://bookdown.org/yihui/rmarkdown-cookbook/table-other.html\nhttps://epirhandbook.com/en/new_pages/tables_descriptive.html\n\ngt package on CRAN and gt package website\n\n\nkableExtra package on CRAN and kableExtra package website\n\n\nflextable package on CRAN and flextable package website and flextable book\n\nhuxtable package on CRAN",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#exportingsaving-data",
    "href": "module132_DataWrangling.html#exportingsaving-data",
    "title": "1.3.2: Data Wrangling",
    "section": "6. Exporting/Saving Data",
    "text": "6. Exporting/Saving Data\nThroughout this lesson we have worked with the mydata dataset. We have made some changes and created new variables. Let’s save the updates to this little dataset for use in later modules.\nUsing the save() function\nSave mydata as *.Rdata native R binary format\nAs we move forward in our lesson modules, we will mostly be working with the “native” format for datafiles (and objects) which have the extension of *.RData or *.rda. These file formats are efficient in terms of saving memory and speed for faster loading of data.\n\n\n\n\n\n\nR data binary formats registered with Library of Congress\n\n\n\nThe “R Data Format Family (.rdata, .rda)” are registered with the Library of Congress under the “Sustainability of Digital Formats”. The description summary states:\n\n“The RData format (usually with extension .rdata or .rda) is a format designed for use with R, a system for statistical computation and related graphics, for storing a complete R workspace or selected”objects” from a workspace in a form that can be loaded back by R. The save function in R has options that result in significantly different variants of the format. This description is for the family of formats created by save and closely related functions. A workspace in R is a collection of typed “objects” and may include much more than the typical tabular data that might be considered a “dataset,” including, for example, results of intermediate calculations and scripts in the R programming language. A workspace may also contain several datasets, which are termed “data frames” in R.”\n\n\n\nLet’s save the mydata data.frame object as “mydata.RData”, using the save() function. See help(save, package = \"base\").\n\n# save the mydata dataset object\nsave(mydata,\n     file = \"mydata.RData\")\n\n\nSave All Objects in Global Environment as *.Rdata\nIt is worth noting that the code above specifically ONLY saves the mydata object. Assuming that your “Global Environment” was empty at the beginning of your computing session at the beginning of this Module 1.3.2, we have created 6 objects so far:\n\n\nforeignhistory - created above looking at the CRAN history for the foreign package\n\nhavenhistory - created above looking at the CRAN history for the haven package\n\nmydata - main dataset imported above\n\nstep2 - created to illustrate the %&gt;% stepwise programming workflow\n\nstep3 - created to illustrate the %&gt;% stepwise programming workflow\n\ntab1 - created above to make a table using the arsenal package\n\n\nSuppose we want to save ALL of these objects for a future computing session or if you’d like to share all of these objects with someone else on your team.\nWe can save the whole Global Environment or select objects in the environment also to a *.RData file to be read back into a future computing session.\nTo save all objects in the Global Environment, we can use save.image():\n\n# save all objects from module 1.3.2\nsave.image(file = \"module132.RData\")\n\n\nSave More than One Object in Global Environment as *.Rdata\nTo save one or more objects for future use - simply list the object names and then save them into an *.RData file.\n\n# save mydata and tab1\nsave(mydata, tab1,\n     file = \"mydata_tab1.RData\")\n\nReading Objects Saved as *.Rdata Back Into Session\nTo test and make sure these items were saved as we expect, let’s remove all objects from our Global Environment and load them back in.\n\n\n\n\n\n\nBe careful using rm(list= ls())\n\n\n\nThe use of the rm(list= ls()) should NOT be used unless you know you have saved everything up to this point. Once you remove all objects from your Global Environment, it cannot be undone. You can either rerun the R code to recreate these objects, or go through the steps described below to save and re-load your objects into your session.\n\n\n\n# remove all objects\nrm(list = ls())\n\n# check that global environment is empty\nls()\n\ncharacter(0)\n\n\nRead back in only the mydata file.\n\n# read in mydata\nload(file = \"mydata.RData\")\n\n# check objects in global environment\nls()\n\n[1] \"mydata\"\n\n\nI’ll remove all objects again for to illustrate the next use of load() function.\n\nrm(list = ls())\n\nRead back in both the mydata and tab1 objects\n\n# read in mydata_tab1\nload(file = \"mydata_tab1.RData\")\n\n# check objects in global environment\nls()\n\n[1] \"mydata\" \"tab1\"  \n\n\n\nrm(list = ls())\n\nRead back in all objects saved from Module 1.3.2.\n\n# read in module132.RData\nload(file = \"module132.RData\")\n\n# check objects in global environment\nls()\n\n[1] \"foreignhistory\" \"havenhistory\"   \"mydata\"         \"step2\"         \n[5] \"step3\"          \"tab1\"          \n\n\n\nrm(list = ls())\n\n\nSave/export data to other formats\nIn addition to use the built-in save() and save.image() functions, we can also export (or save) data objects from R into other formats like CSV and those for specific statistics software like SPSS (*.sav), SAS (*.XPT)and Stata (*.dta).\n\nExport/Write CSV and EXCEL\nIn the readr package, we can use write_csv() to save our updated data as a CSV file which can be read by other software like Excel.\nI’ll load the data back in and then save/export it as other formats.\n\n# read in mydata\nload(file = \"mydata.RData\")\n\n# write as CSV\nreadr::write_csv(mydata,\n                 file = \"mydata_updated.csv\")\n\n\nExport/Write for Other Software (SPSS, SAS, Stata)\nWe can also use the haven package to export/save the updated mydata dataset as a SPSS (*.sav), SAS (*.XPT) or Stata (*.dta) file format.\nCode to export to SPSS *.sav format\n\nhaven::write_sav(mydata,\n                 path = \"mydata_updated.sav\")\n\nRename variable “GenderCoded.f” and “SES.f” to “GenderCoded_f” and “SES_f” to export to SAS or Stata since the “*.f” won’t work in a variable name in these software.\n\n# rename GenderCoded.f and SES.f since the\n# xxx.f wont work for SAS or Stata\nnames(mydata)[names(mydata) == \"GenderCoded.f\"] &lt;-\n  \"GenderCoded_f\"\nnames(mydata)[names(mydata) == \"SES.f\"] &lt;-\n  \"SES_f\"\n\nCode to export to SAS using the “XPT” format\n\nhaven::write_xpt(mydata,\n                 path = \"mydata_updated.xpt\")\n\nCode to export to Stata *.dta format\n\nhaven::write_dta(mydata,\n                 path = \"mydata_updated.dta\")\n\nIf you have these other statistical software on your computer, try opening these new exported files into that software to confirm they worked.\n\n\n\n\n\n\nSome import/export work better than others\n\n\n\nBe aware that many of these import/export functions do work pretty well, but some features of functionality of native formats used by other software packages may not work fully. Read the documentation for each package and function to understand what the limitations may be. For example, importing and exporting SAS *.sas7bdat formatted files can be problematic.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#references",
    "href": "module132_DataWrangling.html#references",
    "title": "1.3.2: Data Wrangling",
    "section": "References",
    "text": "References\n\n\nBolker, Gregory R. Warnes andBen, Thomas Lumley, Randall C Johnson, and Randall C. Johnson. 2022. Gmodels: Various r Programming Tools for Model Fitting. https://CRAN.R-project.org/package=gmodels.\n\n\nCsárdi, Gábor, and Maëlle Salmon. 2025. Pkgsearch: Search and Query CRAN r Packages. https://github.com/r-hub/pkgsearch.\n\n\nHeinzen, Ethan, Jason Sinnwell, Elizabeth Atkinson, Tina Gunderson, and Gregory Dougherty. 2021. Arsenal: An Arsenal of r Functions for Large-Scale Statistical Summaries. https://github.com/mayoverse/arsenal.\n\n\nIannone, Richard. 2023. Fontawesome: Easily Work with Font Awesome Icons. https://github.com/rstudio/fontawesome.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSjoberg, Daniel D., Joseph Larmarange, Michael Curry, Jessica Lavery, Karissa Whiting, and Emily C. Zabor. 2024. Gtsummary: Presentation-Ready Data Summary and Analytic Result Tables. https://github.com/ddsjoberg/gtsummary.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery, and Joseph Larmarange. 2021. “Reproducible Summary Tables with the Gtsummary Package.” The R Journal 13: 570–80. https://doi.org/10.32614/RJ-2021-053.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nYoshida, Kazuki, and Alexander Bartel. 2022. Tableone: Create Table 1 to Describe Baseline Characteristics with or Without Propensity Score Weights. https://github.com/kaz-yos/tableone.",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module132_DataWrangling.html#other-helpful-resources",
    "href": "module132_DataWrangling.html#other-helpful-resources",
    "title": "1.3.2: Data Wrangling",
    "section": "Other Helpful Resources",
    "text": "Other Helpful Resources\nOther Helpful Resources",
    "crumbs": [
      "1.3.2: Data Wrangling"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html",
    "href": "module134_MissingWeight.html",
    "title": "1.3.4: Missing Data and Sampling Weights",
    "section": "",
    "text": "Module “1.3.4: Missing Data and Sampling Weights” will be posted prior to the In-Person Workshops in Summer 2025.",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#coming-summer-2025",
    "href": "module134_MissingWeight.html#coming-summer-2025",
    "title": "1.3.4: Missing Data and Sampling Weights",
    "section": "",
    "text": "Module “1.3.4: Missing Data and Sampling Weights” will be posted prior to the In-Person Workshops in Summer 2025.",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module134_MissingWeight.html#session-objectives",
    "href": "module134_MissingWeight.html#session-objectives",
    "title": "1.3.4: Missing Data and Sampling Weights",
    "section": "Session Objectives",
    "text": "Session Objectives\n\nIdentify and summarize missing data.\nLearn methods to handle missing data according to variable type.\nUse a survey sampling weight to generate more representative descriptive and inferential statistical values (brief intro)\nDiscuss potential bias when removing missing observations without careful examination.\n\nKey points:\n\nR packages that support missing data examination\nMean/median imputation for continuous variables\nWhat to do with missing observations for categorical variables\nWays to examine potential differences between complete and missing observations in association between certain independent and dependent variables\n\nWhat to do if such association significantly differs between complete and missing observations\n\n\nR packages for complex survey data (e.g., survey package)\n\nR codes to generate weighted descriptive statistics and contingency tables, as well as to develop weighted linear models",
    "crumbs": [
      "1.3.4: Missing Data and Sampling Weights"
    ]
  },
  {
    "objectID": "module136_ReproducibleResearch.html",
    "href": "module136_ReproducibleResearch.html",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "",
    "text": "Module “1.3.6: Putting Reproducible Research Principles Into Practice” will be posted prior to the In-Person Workshops in Summer 2025.",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  },
  {
    "objectID": "module136_ReproducibleResearch.html#coming-summer-2025",
    "href": "module136_ReproducibleResearch.html#coming-summer-2025",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "",
    "text": "Module “1.3.6: Putting Reproducible Research Principles Into Practice” will be posted prior to the In-Person Workshops in Summer 2025.",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  },
  {
    "objectID": "module136_ReproducibleResearch.html#session-objectives",
    "href": "module136_ReproducibleResearch.html#session-objectives",
    "title": "1.3.6: Putting Reproducible Research Principles Into Practice",
    "section": "Session Objectives",
    "text": "Session Objectives\n\nDiscuss reproducible research principles.\nApply reproducible research principles to data analysis using R Markdown.\n\nKey points to cover:\n\nReproducible research principles\nWhat is R Markdown\nHow to create a report using R Markdown\n\nCustomize the layout of presentations or reports\nInsert and create objects, such as tables, images, or videos, within a document",
    "crumbs": [
      "1.3.6: Reproducible Research Principles"
    ]
  }
]